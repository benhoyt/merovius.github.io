<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>Between a rock and a crazy place</title>
    <link>https://blog.merovius.de/</link>
    <description>RSS feed for Between a rock and a crazy place</description>
    <pubDate>Wed, 05 Sep 2018 04:00:00 +0000</pubDate>
    <item>
      <title>Scrapping contracts</title>
      <link>https://blog.merovius.de//2018/09/05/scrapping_contracts.html</link>
      <description><![CDATA[tl;dr: I describe a way to simplify the generics design. The ideas are not
particularly novel and have been expressed to various degrees by other people
as well. I hope to provide a more complete view of the design though.
]]></description>
      <pubDate>Wed, 05 Sep 2018 04:00:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/09/05/scrapping_contracts.html</guid>
      <content:encoded><![CDATA[**tl;dr: I describe a way to simplify the generics design. The ideas are not
particularly novel and have been expressed to various degrees by other people
as well. I hope to provide a more complete view of the design though.**

Recently a [Problem Overview](https://go.googlesource.com/proposal/+/master/design/go2draft-generics-overview.md)
and [Draft Design](https://go.googlesource.com/proposal/+/master/design/go2draft-contracts.md)
for generics in Go have dropped. Since then, predictably,
there has been a bunch of chatter on the intertubez about it. This is a
summary of my thoughts, so far, on the subject - after a bunch of discussions
on Twitter and Reddit.

Note: The design is called "Contracts", but I will refer to it as "the design
doc" here.  When I say "contracts", I will refer to the specific part of the
design to express constraints.

#### Contracts vs. Interfaces

First, there is a common observation of overlap between generics and interfaces.
To untangle that, we can say that when we use "generics", what we mean is
*constrained parametric polymorphism*. Go already allows polymorphism by using
interfaces. This desgn doc adds two things: One, a way to add *type-parameters* to
functions and types. And two, a syntax to constrain those type-parameters to a
subset that allows specific operations, via *contracts*.

The latter is where the overlap lies: Interfaces *already* allow you to
constrain arguments to types that allow certain operations. In a way, what
contracts add to this, is that those operations can not only be method calls,
but also allow (and constrain) builtin operators and functions to be used and
to allow or disallow certain composite types (though that mainly affects `map`).

Contracts allow that by the way they are specified: You write a function body
(including arguments, whose notational type becomes the type-variable of the
contract) containing all the statements/expressions you wish to be able
to do. When instantiating a generic type/function with a given set of
type-arguments, the compiler will try to substitute the corresponding
type-variable in the contract body and allow the instantiation, if that body
type-checks.

#### The cost of contracts

After talking a bit through some examples, I feel that contracts optimize for
the wrong thing. The analogy I came up with is vocabulary vs. grammar.

The contracts design is appealing to a good degree, because it uses familiar
*syntax*: You don't have to learn any new syntax or language to express your
contract. Just write natural Go code and have that express your constraints for
you. I call this the "grammar" of constraints: The structure that you use to
input them.

On the other hand, for the *user* of Go, the relevant question is what
constraints are possible to express and how to express them. They might be
interested in deduplicating values in their algorithm, which requires
equality-operations. Or they might want to do comparisons (e.g. `Max`), which
requires `>`. I call this the *vocabulary*: What is the correct way to express
the set of constraints that my algorithm needs?

The issue now, is that while the grammar of constraints might be obvious, it is
not always clear what the actual semantic constraints that generates *are*. A
simple example is map-keys. The design doc uses the contract

```
contract comparable (t T) {
   t == t
}
```

to specify types that are valid map-keyes. But to a beginner, it is not
immediately obvious, what comparisons have to do with maps. An alternative
would be

```
contract mapkey (t T) {
  var _ map[t]bool
}
```

But which is better? Similarly, these two contracts

```
contract mult (t T) {
  t = t * t
}

contract add (t T) {
  t = t + t
}
```

seem very similar, but they are, in theory at least, fundamentally different.
Not only because `add` allows `string`, while `mult` doesn't. But also, because
*technically* any type that supports `*` also supports `-` and `/`. And then there's

```
contract div (t T) {
  t = t % t
}
```

which creates another completely different set of types and allowed operators.

A third example is

```
contract stringlike (t T) {
  append([]byte(nil), t...)
}
```

This allows any type with underlying type `string` or `[]byte`, but nothing
else. And again, technically that would imply allowing index-operations and
`len`. But does the compiler understand that?

Lastly, it's not really clear how `len`, `cap`, `make` or `range` would work.
For example, all these contracts are superficially valid:

```
contract rangeable (t T) {
  for x := range t {
    fmt.Println(x)
  }
}

contract lengthed (t T) {
  var _ int = len(t)
}

contract capped (t T) {
  var _ int = cap(t)
}

contract makeable (t T) {
  t = make(T)
}

contract makeable2 (t T) {
  t = make(T, 0)
}
```

But in all these cases, they allow some subset of channel, map, slice and array
types, with vastly different interpretations of these operations, depending on
the kind of type used - to the degree, that code using them would usually be
nonsensical. Disallowing these, however, opens questions about the claim of
familiar Go syntax, as we now have to make decisions what sort of expressions
and statements we do or don't allow in a contract.

This is why I say contracts optimize for grammar, instead of vocabulary. The
programmer is interested in the vocabulary - what does the contract actually
*mean* and what contract should they use? But the vocabulary is obscured by the
grammar - because we use Go syntax, to understand a given contract we need to
know a bunch of things about what the compiler is and is not able to infer from
it.

This is why I don't really buy the argument of not wanting to learn a bunch of
new syntax or new identifiers for constraints: You *still* have to learn that
vocabulary, but you express it in an obscure and unnatural grammar. I hope to
show that we can introduce the power of generics while also using familiar
grammar and with minimal addition of vocabulary.

#### Scrapping contracts

Now, I'm not the first person to suggest this, but I think we should consider
scrapping contracts from the design. We can still retain type-parameters and we
can still have constraints, but we express them via interfaces instead. I
should point out, that - for now - I'm intentionally optimizing for simplicity
of the design, at the cost of some boilerplate and some loss of power. I will
later try and provide some alternatives to compensate for that in part. But
there is still likely going to remain a net cost in expressiveness. Personally,
I think that tradeoff is worth exploring.

The new design would retain type-parameters and most of their syntax. The
difference is that type-parameters are a full argument list. The type of an
argument has to be an interface type. It can be ellided, in which case it
defaults to the type of the following type-parameter. The last type-parameter
defaults to `interface{}`. As a bonus, this allows providing multiple sets of
constraints on one declaration:

```
func Map(type A, B) (s []A, f func(A) B) []B {
  var out []B
  for _, a := range s {
    out = f(a)
  }
  return out
}

func Stringify(type A fmt.Stringer) (s []A) []string {
  // Because of the signature of fmt.Stringer.String, we can infer all the
  // type-arguments here. Note that A does not *have* to be passed boxed in an
  // interface. A.String is still a valid method-expression for any fmt.Stringer.
  return Map(s, A.String)
}
```

We still want to be able to express multiple, interdependent parameters, which
we can, via parametric interfaces:

```
type Graph(type Node, Edge) interface {
  Nodes(Edge) []Node
  Edges(Node) []Edge
}

func ShortestPath(type Node, Edge) (g Graph(Node, Edge), from, to Node) []Edge {
  // …
}

// Undirected Graph as an adjacency list. This could be further parameterized,
// to allow for user-defined paylooads.
type AdjacencyList [][]int

func (g AdjacencyList) Nodes(edge [2]int) []int {
  return edge[:]
}

func (g AdjacencyList) Edges(node int) [][2]int {
  var out [][2]int
  for _, v := range g[node] {
    out = append(out, [2]int{node, v}
    if v != node {
      out = append(out, [2]int{v, node})
    }
  }
  return out
}

func main() {
  g := AdjacencyList{…}
  // Types could be infered here, as the names of methods are unique, so we can
  // look at the methods Nodes and Edges of AdjacencyList to infer the
  // type-arguments.
  path := ShortestPath(g, 0, len(g)-1)
  fmt.Println(path)
}
```

The last example is relevant to the difference in power between contracts and
interfaces: Usage of operators. We can still express the concept, but this is
where the increased boilerplate comes in:

```
func Max(type T)(a, b T, less func(T, T) bool) T {
  if less(a, b) {
    return b
  }
  return a
}

func main() {
  fmt.Println(Max(a, b int, func(a, b int) { return a < b }))
}
```

I will try to show some ways to get rid of that boilerplate later. For now,
let's just treat it as a necessary evil of this idea. Though it should be
mentioned, that while this is more *cumbersome*, it's still just as *typesafe*
as contracts (as opposed to, say, a reflect-based generic `Max`).

So, scrapping contracts leaves us with more boilerplate, but just the same set
of concepts we can express - though we do have to pass in any builtin
operations we want to perform as extra functions (or express them in an
interface). In exchange, we get

* Only one way to specify constraints.
* A simpler spec (we don't need to add a new concept, contracts, to the
  language) and a saved (pseudo-)keyword.
* A simpler compiler: We don't need to add a solver to deduce constraints from
  a given contract. The constraint-checker already exists.
* Still a well-known, though less powerfull, language to express constraints,
  with interfaces.
* Simple syntax (same as normal arglists) for having multiple sets of
  constraints in one declaration.
* Trivially good error messages. Types passed in need only be checked for
  consistency and interface satisfaction - the latter is already implemented,
  including good error messages.

#### Getting rid of boilerplate

I see two main ways to get rid of boilerplate: Adding methods to builtin types,
or what I call pseudo-interfaces.

##### Methods on builtin types

An obvious idea is to not use operators in generic code, but instead use
method-call syntax. That is, we'd do something akin to

```
func Max(type T Ordered) (a, b T) T {
  if a.Less(b) {
    return b
  }
  return a
}
```

To actually reduce the boilerplate, we'd predefine methods for all the
operators on the builtin types. That would allow us to call `Max` with `int`,
for example.

Unfortunately, I can see a bunch of roadblocks to make this work. Methods are
not promoted to derived types, so you couldn't use `Max` with e.g.
`time.Duration`, which has *underlying* type `int64`, but is not the same type.
We'd probably want those methods to be "special" in that they automatically get
promoted to any type whose underlying type is predeclared. That introduces
compatibility issues of clashing Method/Field names.

At the end, to express that `Less` has to take the same argument as the
receiver type, `Ordered` might look something like this:

```go
type Ordered(T) interface {
  Less(T) bool
}

func Max(type T Ordered(T)) (a, b T) T {
  if a.Less(b) {
    return b
  }
  return a
}

// In the universe block:

// Implements Ordered(int).
func (a int) Less(b int) bool {
  retun a < b
}
```

Though it's not clear, whether a parameter like `T Ordered(T)` should be
allowed. And this would technically allow to implement `Ordered(int)` on a
custom type. While that probably won't be very useful (the majority of usecases
will require `T Ordered(T)`), it's not excluded.

##### Pseudo-interfaces

Unfortunately I didn't have a lot of time the last couple of days, so I got
beat to the punch on this. Matt Sherman [described the idea first](https://clipperhouse.com/go-generics-typeclasses/)
and called the concept "typeclasses". I will stick with pseudo-interface,
because it fits better in the general concept of this description.

The idea is to introduce a set of types into the language that can be used like
interfaces (including embedding), but instead of providing methods, provide
operators. There is a limited set of base types that need to be provided:

```
pseudo-interface | Allowed operators
-----------------+-------------------
comparable       | ==, !=
ordered          | <, <= > >=
boolean          | ||, &&, !
bitwise          | ^, %, &, &^, <<, >>
arith            | +, -, *, /
concat           | +
complex          | real(z), imag(z)
nilable          | v == nil
```

and a set of derived pseudo-interfaces:

```
pseudo-interface | definition
-----------------+-----------------------------------------------------
num              | interface { comparable; ordered; arith }
integral         | interface { num; bitwise }
stringy          | interface { comparable; ordered; concat; len() int }
iface            | interface { comparable; nilable }
```

The pseudo-interfaces would be declared in the universe block, as predeclared
identifiers. This makes them backwards-compatible (as opposed to methods on
builtin types), because any existing identifier would just shadow these (akin
to how you can have a variable with name `string`).

Bitshift-operators currently are restricted when used with constants
overflowing the width of an integral type. For generic code, this restriction
would be lifted (as the size is not statically known) and instead the behavior
is equivalent to if the right operand is an uint variable with the given
value.

This would allow us to write

```
func Max(type T ordered) (a, b T) T {
  if a < b {
    return b
  }
  return a
}
```

Notably, the list of pseudo-interfaces doesn't include anything related to
channel-, slice- or map-operations (or other composite types). The idea is to
instead use a type literal directly:

```
type Keys(type K, V) (m map[K]V) []K {
  var out []K
  for k := range m {
    out = append(out, k)
  }
  return out
}
```

As every type supporting, e.g. `map` operations, need to have underlying type
`map[K]V`, it's thus assignable to that type and can be passed to `Keys` as is.
That is, this is completely legal:

```
func main() {
  type MyMap map[string]int
  var m = MyMap{
    "foo": 23,
    "bar": 42,
  }
  fmt.Println(Keys(m))
}
```

This also solves another problem with contracts: The ambiguity of `len`, `cap`
and `range`. As the actual kind of the value is not only known during
compilation of the generic function, but even obvious from the code, there is
no question about the intended semantics.

Should Go ever grow operator overloading via operator methods, the
pseudo-interfaces could be changed into actual interfaces, containing the
necessary methods. Of course, that implies that operator overloading would
retain the properties of existing operators, e.g. that having `==` implies
having `!=`, or having `-` implying having `+`. Personally, I consider that a
good thing - it limits the abuse of operator overloading for nonsensical
operations (say, `<<` for writing to an `io.Writer`).

I'm not trying to advocate for operator overloading, but think it's worth
mentioning that this design leaves the door open to that.

##### But performance

A possible criticism of either of these approaches is, that operators have
better performance than dynamic dispatch to a method. I believe (vigorous
handwaving ahead) that this is no different in the existing contracts proposal.
If generic code is compiled generically, it still needs to employ some means
of dynamic dispatch for operators. If, on the other hand, it's compiled
instantiated, then the compiler would also be able to devirtualize the
interfaces - and then inline the method definition.

#### Conclusion

I've previously said that I'm "meh" on the design doc, which is the strongest
form of endorsement a generics proposal could ever get from me. After some
discussion, I'm more and more convinced that while contracts *seem*
conceptually simple, they create a plethora of implementation- and usage
questions. I'm not sure, the supposed advantage of contracts, of a well-known
syntax, holds up to scrutiny when it comes to mapping that to the actually
derived constraints or writing contracts. There are also many open questions in
regards to contracts, a bunch of them related to the ambiguity of
Go-expressions. As a result, I'm starting to feel more negative towards them
- they *look* like an elegant idea, but in practice, they have a lot of weird
corners.

This design is similar (AIUI) to the [type functions](https://go.googlesource.com/proposal/+/master/design/15292/2010-06-type-functions.md)
proposal, so I assume there are good reasons the Go team does not want this.
The difference is mainly the absence of operator methods in favor of
pseudo-interfaces or explicit method calls. This design also handwaves a
couple of important implementation questions - the justification for that is
that these questions (e.g. type inference and code generation) should be able
to be taken from the design doc with minimal changes. It's entirely
possible that I am overlooking something, though.
]]></content:encoded>
      <dc:date>2018-09-05T04:00:00+00:00</dc:date>
    </item>
    <item>
      <title>Why doesn't Go have variance in its type system?</title>
      <link>https://blog.merovius.de//2018/06/03/why-doesnt-go-have-variance-in.html</link>
      <description><![CDATA[tl;dr: I explain what co-, contra- and invariance are and what the
implications for Go&#39;s type system would be. In particular, why it&#39;s impossible
to have variance in slices.
]]></description>
      <pubDate>Sun, 03 Jun 2018 23:20:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/06/03/why-doesnt-go-have-variance-in.html</guid>
      <content:encoded><![CDATA[**tl;dr: I explain what co-, contra- and invariance are and what the
implications for Go's type system would be. In particular, why it's impossible
to have variance in slices.**

A question that comes up relatively often with Go newcomers is "why can't I
pass e.g. an `[]int` to a `func([]interface{})`"? In this post I want to
explore this question and its implications for Go. But the concept of
variance (which this is about) is also useful in other languages.

Variance describes what happens to subtype relationships, when they are
used in composite types. In this context, "A is a subtype of B" means
that a value of type A can always be used, where a value of type B is required.
Go doesn't have explicit subtype relationships - the closest it has is
[assignability](https://golang.org/ref/spec#Assignability) which mostly
determines whether types can be used interchangeably. Probably the most
important case of this is given by interfaces: If a type T (whether its a
concrete type, or itself an interface) implements an interface I, then T can be
viewed as a subtype of I. In that sense,
[`*bytes.Buffer`](https://godoc.org/bytes#Buffer) is a subtype of
[io.ReadWriter](https://godoc.org/io#ReadWriter), which is a subtype of
[io.Reader](https://godoc.org/io#Reader). And every type is a subtype of
`interface{}`.

The easiest way to understand what variance means, is to look at function
types. Let's assume, we have a type and a subtype - for example, let's look at
`*bytes.Buffer` as a subtype of `io.Reader`. Say, we have a `func()
*bytes.Buffer`. We could also use this like a `func() io.Reader` - we just
reinterpret the return value as an `io.Reader`. The reverse is not true: We
can't treat a `func() io.Reader` as a `func() *bytes.Buffer`, because not every
`io.Reader` is a `*bytes.Buffer`. So, function return values could *preserve*
the direction of subtyping relationships: If A is a subtype of B, `func() A`
could be a subtype of `func() B`. This is called *covariance*.

```go
func F() io.Reader {
	return new(bytes.Buffer)
}

func G() *bytes.Buffer {
	return new(bytes.Buffer)
}

func Use(f func() io.Reader) {
	useReader(f())
}

func main() {
	Use(F) // Works

	Use(G) // Doesn't work right now; but *could* be made equivalent to…
	Use(func() io.Reader { return G() })
}
```

On the other hand, say we have a `func(*bytes.Buffer)`. Now we can't use that
as a `func(io.Reader)`: You can't call it with an `io.Reader`. But we *can* do
the reverse. If we have a `*bytes.Buffer`, we can call a `func(io.Reader)` with
it. Thus, function arguments *reverse* the subtype relationship: If A is a
subtype of B, then `func(B)` could be a subtype of `func(A)`. This is called
*contravariance*.

```go
func F(r io.Reader) {
	useReader(r)
}

func G(r *bytes.Buffer) {
	useReader(r)
}

func Use(f func(*bytes.Buffer)) {
	b := new(bytes.Buffer)
	f(b)
}

func main() {
	Use(F) // Doesn't work right now; but *could* be made equivalent to…
	Use(func(r *bytes.Buffer) { F(r) })

	Use(G) // Works
}
```

So, `func` is contravariant for arguments and covariant for return values. Of
course, we can combine the two: If A and C are subtypes of B and D
respectively, we can make `func(B) C` a subtype of `func(A) D`, by converting
like this:

```go
// *os.PathError implements error

func F(r io.Reader) *os.PathError {
	// ...
}

func Use(f func(*bytes.Buffer) error) {
	b := new(bytes.Buffer)
	err := f(b)
	useError(err)
}

func main() {
	Use(F) // Could be made to be equivalent to
	Use(func(r *bytes.Buffer) error { return F(r) })
}
```

However, `func(A) C` and `func(B) D` are incompatible. Neither can be a subtype
of the other:

```go
func F(r *bytes.Buffer) *os.PathError {
	// ...
}

func UseF(f func(io.Reader) error) {
	b := strings.NewReader("foobar")
	err := f(b)
	useError(err)
}

func G(r io.Reader) error {
	// ...
}

func UseG(f func(*bytes.Buffer) *os.PathErorr) {
	b := new(bytes.Buffer)
	err := f()
	usePathError(err)
}

func main() {
	UseF(F) // Can't work, because:
	UseF(func(r io.Reader) error {
		return F(r) // type-error: io.Reader is not *bytes.Buffer
	})

	UseG(G) // Can't work, because:
	UseG(func(r *bytes.Buffer) *os.PathError {
		return G(r) // type-error: error is not *os.PathError
	})
}
```

So in this case, there just *is* not relationship between the composite types.
This is called *invariance*.

---

Now, we can get back to our opening question: Why can't you use `[]int` as
`[]interface{}`? This really is the question "Why are slice-types invariant"?.
The questioner assumes that because `int` is a subtype of `interface{}`, we
should also make `[]int` a subtype of `[]interface{}`. However, we can now see
a simple problem with that. Slices support (among other things) two fundamental
operations, that we can roughly translate into function calls:

```go
as := make([]A, 10)
a := as[0] 		// func Get(as []A, i int) A
as[1] = a  		// func Set(as []A, i int, a A)
```

This shows a clear problem: The type A appears *both* as an argument *and*
as a return type. So it appears both covariantly and contravariantly. So while
with functions there is a relatively clear-cut answer to how variance might
work, it just doesn't make a lot of sense for slices. Reading from it would
require covariance but writing to it would require contravariance. In other
words: If you'd make `[]int` a subtype of `[]interface{}` you'd need to explain
how this code would work:

```go
func G() {
	v := []int{1,2,3}
	F(v)
	fmt.Println(v)
}

func F(v []interface{}) {
	// string is a subtype of interface{}, so this should be valid
	v[0] = "Oops"
}
```

Channels give another interesting perspective here. The bidirectional channel
type has the same issue as slices: Receiving requires covariance, whereas
sending requires contravariance. But you can restrict the directionality of a
channel and only allow send- or receive-operations respectively. So while `chan
A` and `chan B` would not be related, we could make `<-chan A` a subtype of
`<-chan B`. And `chan<- B` a subtype of `chan<- A`.

In that sense, [read-only types](https://github.com/golang/go/issues/22876)
have the potential to at least theoretically allow variance for slices. While
`[]int` still wouldn't be a subtype of `[]interface{}`, we could make `ro
[]int` a subtype of `ro []interface{}` (borrowing the syntax from the
proposal).

---

Lastly, I want to emphasize that all of these are just the *theoretical* issues
with adding variance to Go's type system. I consider them harder, but even if
we *could* solve them we would still run into practical issues. The most
pressing of which is that subtypes have different memory representations:

```go
var (
	// super pseudo-code to illustrate
	x *bytes.Buffer // unsafe.Pointer
	y io.ReadWriter // struct{ itable *itab; value unsafe.Pointer }
					// where itable has two entries
	z io.Reader		// struct{ itable *itab; value unsafe.Pointer }
					// where itable has one entry
)
```

So even though you might think that all interfaces have the same memory
representation, they actually don't, because the method tables have a different
assumed layout. So in code like this

```go
func Do(f func() io.Reader) {
	r := f()
	r.Read(buf)
}

func F() io.Reader {
	return new(bytes.Buffer)
}

func G() io.ReadWriter {
	return new(bytes.Buffer)
}

func H() *bytes.Buffer {
	return new(bytes.Buffer)
}

func main() {
	// All of F, G, H should be subtypes of func() io.Reader
	Do(F)
	Do(G)
	Do(H)
}
```

there still needs to be a place where the return value of `H` is wrapped into
an `io.Reader` and there needs to be a place where the itable of the return
value of `G` is transformed into the correct format expected for an
`io.Reader`. This isn't a *huge* problem for `func`: The compiler can
generate the appropriate wrappers at the call site in `main`.
There is a performance overhead, but only code that actually uses this form of
subtyping needs to pay it. However, it becomes significant problem for slices.

For slices, we must either a) convert the `[]int` into an `[]interface{}` when
passing it, meaning an allocation and complete copy. Or b) delay the conversion
between `int` and `interface{}` until the access, which would mean that every
slice access now has to go through an indirect function call - just *in case*
anyone would ever pass us a subtype of what we are expecting. Both options
seem prohibitively expensive for Go's goals.
]]></content:encoded>
      <dc:date>2018-06-03T23:20:00+00:00</dc:date>
    </item>
    <item>
      <title>Persistent datastructures with Go</title>
      <link>https://blog.merovius.de//2018/02/25/persistent_datastructures_with_go.html</link>
      <description><![CDATA[I&#39;ve recently taken a liking to persistent datastructures.
These are datastructures where instead of mutating data in-place, you are
creating a new version of the datastructures, that shares most of its state
with the previous version. Not all datastructures can be implemented
efficiently like this, but those that do get a couple of immediate benefits -
keeping old versions around allows you to get cheap snapshotting and copying.
It is trivial to pass a copy to a different thread and you don&#39;t have to worry
about concurrent writes, as neither actually mutates any shared state.
]]></description>
      <pubDate>Sun, 25 Feb 2018 17:30:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/02/25/persistent_datastructures_with_go.html</guid>
      <content:encoded><![CDATA[I've recently taken a liking to [persistent datastructures](https://en.wikipedia.org/wiki/Persistent_data_structure).
These are datastructures where instead of mutating data in-place, you are
creating a new version of the datastructures, that shares most of its state
with the previous version. Not all datastructures can be implemented
efficiently like this, but those that do get a couple of immediate benefits -
keeping old versions around allows you to get cheap snapshotting and copying.
It is trivial to pass a copy to a different thread and you don't have to worry
about concurrent writes, as neither actually mutates any shared state.

Persistent datastructures are popular in functional programming languages, but
I also found the idea a useful tool to model datastructures in Go. Go's
interfaces provide a nice way to model them and make them easy to reason about.
In this post, I will try to illustrate this with a couple of examples.

There are four key ideas I'd like you to walk away with:

* Modeling datastructures as persistent (*if possible*) makes them easier to
  reason about.
* When you want to use sum types, try to think of the common properties you are
  trying to abstract over instead - put those in an interface.
* Separate out the required from the provided interface. Make the former an
  interface type, provide the latter as functions or a wrapper.
* Doing these allows you to add more efficient implementations later, when you
  discover they are necessary.

#### Linked lists

This is more of an illustrative example, to demonstrate the techniques, than
actually useful. But one of the simplest datastructures existing are linked
lists: A list of nodes, where each node has a value and possibly a next node
(unless we are at the end of the List). In functional languages, you'd use a
sum type to express this:

```haskell
type List a = Node a (List a) -- either it's a node with a value and the rest of the list
            | End             -- or it's the end of the list
```

Go infamously does not have sum types, but we can use interfaces to instead.
The classical way would be something like

```go
type List interface {
  // We use an unexported marker-method. As nothing outside the current package
  // can implement this unexported method, we get control over all
  // implementations of List and can thus de-facto close the set of possible
  // types.
  list()
}

type Node struct {
  Value int
  Next List
}

func (Node) list() {}

type End struct {}

func (End) list() {}

func Value(l List) (v int, ok bool) {
  switch l := l.(type) {
  case Node:
    return l.Value, true
  case End:
    return 0, false
  default:
    // This should never happen. Someone violated our sum-type assumption.
    panic(fmt.Errorf("unknown type %T", l))
  }
}
```

This works, but it is not really idiomatic Go code. It is error-prone and easy
to misuse, leading to potential panics. But there is a different way to model
this using interfaces, closer to how they are intended. Instead of expressing
what a list is

> A list *is* either a value and a next element, or the end of the list

we say what we want a list to be able to *do*:

> A list has a current element and may have a tail

```go
type List interface {
  // Value returns the current value of the list
  Value() int
  // Next returns the tail of the list, or nil, if this is the last node.
  Next() List
}

type node struct {
  value int
  next  List
}

func (n node) Value() int {
  return n.value
}

func (n node) Next() List {
  return n.next
}

func New(v int) List {
  return node{v, nil}
}

func Prepend(l List, v int) List {
  return node{v, l}
}
```

This is a far more elegant abstraction. The empty list is represented by the
`nil` interface. We have only one implementation of that interface, for the
nodes. We offer exported functions to create new lists - potentially from
existing ones.

Note that the methods actually have `node` as a receiver, not `*node`, as we
often tend to do with structs. This fact makes this implementation a
*persistent* linked list. None of the methods can modify the list. So after
creation, the linked list will stay forever immutable. Even if you type-assert
to get to the underlying data, that would only provide you with a *copy* of the
data - the original would stay unmodified. The memory layout, however, is the
same - the value gets put on the heap and you are only passing pointers to it
around.

The beauty of this way to think about linked lists, is that it allows us to
amend it after the fact. For example, say we notice that our program is slow,
due to excessive cache-misses (as linked lists are not contiguous in memory).
We can easily add a function, that packs a list:

```go
type packed []int

func (p packed) Value() int {
  return p[0]
}

func (p packed) Next() List {
  if len(p) == 0 {
    return nil
  }
  return p[1:]
}

func Pack(l List) List {
  if l == nil {
    return nil
  }
  var p packed
  for ; l != nil; l = l.Next() {
    p = append(p, l.Value())
  }
  return p
}
```

The cool thing about this is that we can mix and match the two: For example,
we could prepend new elements and once the list gets too long, pack it and
continue to prepend to the packed list. And since `List` is an interface, users
can implement it themselves and use it with our existing implementation. So,
for example, a user could build us a list that calculates fibonacci numbers:

```go
type fib [2]int

func (l fib) Value() int {
  return l[0]
}

func (l fib) Next() List {
  return fib{l[1], l[0]+l[1]}
}
```

and then use that with functions that take a `List`. Or they could have a
lazily evaluated list:

```go
type lazy struct {
  o sync.Once
  f func() (int, List)
  v int
  next List
}

func (l *lazy) Value() int {
  l.o.Do(func() { l.v, l.next = l.f() })
  return l.v
}

func (l *lazy) Next() List {
  l.o.Do(func() { l.v, l.next = l.f() })
  return l.next
}
```

Note that in this case the methods need to be on a pointer-receiver. This
(technically) leaves the realm of persistent data-structures. While they
motivated our interface-based abstraction and helped us come up with a safe
implementation, we are not actually *bound* to them. If we later decide, that
for performance reasons we want to add a mutable implementation, we can do so
(of course, we still have to make sure that we maintain the safety of the
original). And we can intermix the two, allowing us to only apply this
optimization to part of our data structure.

I find this a pretty helpful way to think about datastructures.

#### Associative lists

Building on linked lists, we can build a map based on [Association Lists](https://en.wikipedia.org/wiki/Association_list).
It's a similar idea as before:

```go
type Map interface {
  Value(k interface{}) interface{}
  Set(k, v interface{}) Map
}

type empty struct{}

func (empty) Value(_ interface{}) interface{} {
  return nil
}

func (empty) Set(k, v interface{}) Map {
  return pair{k, v, empty{}}
}

func Make() Map {
  return empty{}
}

type pair struct {
  k, v interface{}
  parent Map
}

func (p pair) Value(k interface{}) interface{} {
  if k == p.k {
    return p.v
  }
  return p.parent.Value(k)
}

func (p pair) Set(k, v interface{}) Map {
  return pair{k, v, p}
}
```

This time, we don't represent an empty map as `nil`, but add a separate
implementation of the interface for an empty map.  That makes the
implementation of `Value` cleaner, as it doesn't have to check the parent map
for `nil` -- but it requires users to call `Make`.

There is a problem with our `Map`, though: We cannot iterate over it. The
interface does not give us access to any parent maps. We could use
type-assertion, but that would preclude users from implementing their own. What
if we added a method to the interface to support iteration?

```go
type Map interface {
  Value(k interface{}) interface{}

  // Iterate calls f with all key-value pairs in the map.
  Iterate(f func(k, v interface{}))
}

func (empty) Iterate(func(k, v interface{})) {
}

func (p pair) Iterate(f func(k, v interface{})) {
  f(p.k, p.v)
  p.parent.Iterate(f)
}
```

Unfortunately, this still doesn't really work though: If we write multiple
times to the same key, `Iterate` as implemented would call `f` with all
key-value-pairs. This is likely not what we want.

The heart of the issue here, is the difference between the *required* interface
and the *provided* interface. We can also see that with `Set`. Both of the
implementations of that method look essentially the same and neither actually
depends on the used type. We could instead provide `Set` as a function:

```go
func Set(m Map, k, v interface{}) Map {
  return pair{k,v,m}
}
```

The lesson is, that some operations need support from the implementation, while
other operations can be implemented without it. The provided interface is the
set of operations we provide to the user, whereas the required interface is the
set of operations that we rely on. We can split the two and get something like this:

```go
// Interface is the set of operations required to implement a persistent map.
type Interface interface {
  Value(k interface{}) interface{}
  Iterate(func(k, v interface{}))
}

type Map struct {
  Interface
}

func (m Map) Iterate(f func(k, v interface{})) {
  seen := make(map[interface{}]bool)
  m.Interface.Iterate(func(k, v interface{}) {
    if !seen[k] {
      f(k, v)
    }
  })
}

func (m Map) Set(k, v interface{}) Map {
  return Map{pair{k, v, m.Interface}}
}
```

Using this, we could again implement a packed variant of `Map`:

```go
type packed map[interface{}]interface{}

func (p packed) Value(k interface{}) interface{} {
  return p[k]
}

func (p packed) Iterate(f func(k, v interface{})) {
  for k, v := range p {
    f(k, v)
  }
}

func Pack(m Map) Map {
  p := make(packed)
  m.Iterate(func(k,v interface{}) {
    p[k] = v
  })
  return m
}
```

#### Ropes

A [Rope](https://en.wikipedia.org/wiki/Rope_(data_structure)) is a data
structure to store a string in a way that is efficiently editable. They are
often used in editors, as it is too slow to copy the complete content on every
insert operation. Editors also benefit from implementing them as persistent data
structures, as that makes it very easy to implement multi-level undo: Just have
a stack (or ringbuffer) of Ropes, representing the states the file was in after
each edit. Given that they all share most of their structure, this is very
efficient. Implementing ropes is what really bought me into the patterns
I'm presenting here. Let's see, how we could represent them.

<a href="https://en.wikipedia.org/wiki/Rope_(data_structure)#/media/File:Vector_Rope_example.svg">
  <img src="/assets/Vector_Rope_example.svg" alt='Rope representing the string "Hello_my_name_is_Simon"'>
</a>

A Rope is a binary tree with strings as leafs. The represented string
is what you get when you do a depth-first traversal and concatenate all the
leafs. Every node in the tree also has a *weight*, which corresponds to the
length of the string for leafs and the length of the left subtree for inner
nodes. This allows easy recursive lookup of the `i`th character: If `i` is less
than the weight of a node, we look into the left subtree, otherwise into the
right. Let's represent this:

```go
type Base interface {
  Index(i int) byte
  Length() int
}

type leaf string

func (l leaf) Index(i int) byte {
  return l[i]
}

func (l leaf) Length() int {
  return len(l)
}

type node struct {
  left, right Base
}

func (n node) Index(i int) byte {
  if w := n.left.Length(); i >= w {
    // The string represented by the right child starts at position w,
    // so we subtract it when recursing to the right
    return n.right.Index(i-w)
  }
  return n.left.Index(i)
}

func (n node) Length() int {
  return n.left.Length() + n.right.Length()
}

type Rope struct {
  Base
}

func New(s string) Rope {
  return Rope{leaf(s)}
}

func (r Rope) Append(r2 Rope) Rope {
  return Rope{node{r.Base, r2.Base}}
}
```

Note, how we did not actually add a `Weight`-method to our interface: Given
that it's only used by the traversal on inner nodes, we can just directly
calculate it from its definition as the length of the left child tree. In
practice, we might want to pre-calculate `Length` on creation, though, as it
currently is a costly recursive operation.

The next operation we'd have to support, is splitting a Rope at an index. We
can't implement that with our current interface though, we need to add it:

```go
type Base interface {
  Index(i int) byte
  Length() int
  Split(i int) (left, right Base)
}

func (l leaf) Split(i int) (Base, Base) {
  return l[:i], l[i:]
}

func (n node) Split(i int) (Base, Base) {
  if w := n.left.Length(); i >= w {
    left, right := n.right.Split(i-w)
    return node{n.left, left}, right
  }
  left, right := n.left.Split(i)
  return left, node{n.right, right}
}

func (r Rope) Split(i int) (Rope, Rope) {
  // Note that we return the wrapping struct, as opposed to Base.
  // This is so users work with the provided interface, not the required one.
  left, right := r.Split(i)
  return Rope{left}, Rope{right}
}
```

I think this code is remarkably readable and easy to understand - and that is
mostly due to the fact that we are reusing subtrees whenever we can. What's
more, given these operations we can implement the remaining three from the
wikipedia article easily:

```go
func (r Rope) Insert(r2 Rope, i int) Rope {
  left, right := r.Split(i)
  return left.Append(r2).Append(right)
}

func (r Rope) Delete(i, j int) Rope {
  left, right := r.Split(j)
  left, _ = left.Split(i)
  return left.Append(right)
}

func (r Rope) Slice(i, j int) Rope {
  r, _ = r.Split(j)
  _, r = r.Split(i)
  return r
}
```

This provides us with a fully functioning Rope implementation. It doesn't
support everything we'd need to write an editor, but it's a good start that was
quick to write. It is also reasonably simple to extend with more functionality.
For example, you could imagine having an implementation that can rebalance
itself, when operations start taking too long. Or adding traversal, or
random-access unicode support that is still backed by compact UTF-8. And I
found it reasonably simple (though it required some usage of unsafe) to write
an implementation of `Base` that used an `mmap`ed file (thus you'd only need to
keep the actual edits in RAM, the rest would be read directly from disk with
the OS managing caching for you).

#### Closing remarks

None of these ideas are revolutionary (especially to functional programmers).
But I find that considering if a datastructure I need can be implemented as a
persistent/immutable one helps me to come up with clear abstractions that work
well. And I also believe that Go's interfaces provide a good way to express
these abstractions - because they allow you to start with a simple, immutable
implementation and then compose it with mutable ones - if and only if there are
clear efficiency benefits. Lastly, I think there is an interesting idea here of
how to substitute sum-types by interfaces - not in a direct manner, but instead
by thinking about the common behavior you want to provide over the sum.

I hope you find that this inspires you to think differently about these problems too.
]]></content:encoded>
      <dc:date>2018-02-25T17:30:00+00:00</dc:date>
    </item>
    <item>
      <title>What even is error handling?</title>
      <link>https://blog.merovius.de//2018/01/21/what_even_is_error_handling.html</link>
      <description><![CDATA[tl;dr: Error handling shouldn&#39;t be about how to best propagate an error
value, but how to make it destroy it (or make it irrelevant). To encourage
myself to do that, I started removing errors from function returns wherever I
found it at all feasible
]]></description>
      <pubDate>Sun, 21 Jan 2018 23:40:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/01/21/what_even_is_error_handling.html</guid>
      <content:encoded><![CDATA[**tl;dr: Error handling shouldn't be about how to best propagate an error
value, but how to make it destroy it (or make it irrelevant). To encourage
myself to do that, I started removing errors from function returns wherever I
found it at all feasible**

Error handling in Go is a contentious and often criticized issue. There is no
shortage on articles criticizing the approach taken, no shortage on articles
giving advice on how to deal with it (or defending it) and also no shortage on
proposals on how to improve it.

During these discussion, I always feel there is something missing. The
proposals for improvement usually deal with syntactical issues, how to avoid
boilerplate. Then there is the other school of thought - where it's not about
syntax, but about how to best pass errors around. Dave Chaney wrote [an often
quoted blog post on the
subject](https://dave.cheney.net/2016/04/27/dont-just-check-errors-handle-them-gracefully),
where he lists all the ways error information can be mapped into the Go type
system, why he considers them flawed and what he suggests instead.
This school of thought regularly comes up with helper packages, to make
wrapping or annotating errors easier.
[pkg/errors](https://github.com/pkg/errors) is very popular (and is grown out
of the approach of above blog post) but [upspin's
incarnation](https://godoc.org/github.com/upspin/upspin/errors#Error) also
gathered some attention.

I am dissatisfied with both schools of thought. Overall, neither seems to
explicitly address, what to me is the underlying question: What *is* error
handling? In this post, I'm trying to describe how I interpret the term and
why, to me, the existing approaches and discussions mostly miss the mark. Note,
that I don't claim this understanding to be universal - just how *I* would put
into words my understanding of the topic.

---

Let's start with a maybe weird question: Why is the entry point into the
program `func main()` and not `func main() error`? Personally, I start most of
my programs writing

```go
func main() {
  if err := run(); err != nil {
    log.Fatal(err)
  }
}

func run() error {
  // …
}
```

This allows me to use `defer`, pass on errors and all that good stuff. So, why
doesn't the language just do that for me?

We can find part of the answer in [this old golang-nuts thread](https://groups.google.com/d/topic/golang-nuts/6xl02B_MxdA/discussion).
It is about return codes, instead of an `error`, but the principle is the
same. And the best answer - in my opinion - is this:

> I think the returned status is OS-specific, and so Go the language should not
> define its type (Maybe some OS can only report 8-bit result while some other
> OS support arbitrary string as program status, there is considerable
> differences between that; there might even be environment that don't support
> returning status code or the concept of status code simply doesn't exist)
>
> I imagine some Plan 9 users might be disagree with the signature of
> `os.Exit()`.

So, in essence: Not all implementations would necessarily be able to assign a
reasonable meaning to a return code (or error) from `main`. For example, an
embedded device likely couldn't really do anything with it. It thus seems
preferable to not couple the language to this decision which only *really* makes
semantic sense on a limited subset of implementations. Instead, we provide
mechanisms in the standard library to exit the program or take any other
reasonable action and then let the developer decide, under what circumstances
they want to exit the program and with what code. Being coupled to a decision
in the standard library is better than being coupled in the language itself.
And a developer who targets a platform where an exit code doesn't make sense,
can take a different action instead.

Of course, this leaves the programmer with a problem: What to do with errors?
We could write it to stderr, but `fmt.Fprintf` *also* returns an error, so what
to do with that one? Above I used `log.Fatal`, which does *not* return an error.
What happens if the underlying `io.Writer` fails to write, though? What
does `log` do with the resulting error? The answer is, of course: It ignores
any errors.

The point is, that passing on the error is not a solution. *Eventually* every
program will return to `main` (or `os.Exit` or panic) and the buck stops there.
It needs to get *handled* and the signature of `main` enforces that the only
way to do that is via side-effects - and if they fail, you just have to deal
with that one too.

---

Let's continue with a similar question, that has a similar answer, that
occasionally comes up: Why doesn't `ServeHTTP` return an `error`? Sooner or
later, people face the question of what to do with errors in their HTTP
Handlers. For example, what if you are writing out a JSON object and
`Marshal` fails? In fact, a lot of HTTP frameworks out there will define their
own handler-type, which differs from `http.Handler` in exactly that way. But if
everyone wants to return an `error` from their handler, why doesn't the
interface just add that error return itself? Was that just an oversight?

I'm strongly arguing that no, this was not an oversight, but the correct design
decision. Because the HTTP Server package *can not handle any errors*. An HTTP
server is supposed to stay running, every request demands a response. If
`ServeHTTP` would return an `error`, the server would have to do *something*
with it, but what to do is highly application-specific. You might respond that
it should serve a 500 error code, but in 99% of cases, that is the wrong thing
to do. Instead you should serve a more specific error code, so the client
knows (for example) whether to retry or if the response is cacheable.
`http.Server` could also just ignore the error and instead drop the request on
the floor, but that's even worse. Or it could propagate it up the stack. But as
we determined, eventually it would have to reach `main` and the buck stops
there. You probably don't want your server to come down, every time a request
contains an invalid parameter.

So, given that a) every request needs an answer and b) the right answer is
highly application-specific, the translation from errors into status codes
*has* to happen in application code. And just like `main` enforces you to
handle any errors via side-effects by not allowing you to return an `error`, so
does `http` force you to handle any errors via writing a response by not
allowing you to return an `error`.[¹](#footnote1)<a id="footnote1_back"></a>

So, what are you supposed to do, when `json.Marshal` fails? Well, that depends
on our application. Increment a metric. Log the error. panic. Write out a 500.
Ignore it and write a 200. Commit to the uncomfortable knowledge, that
sometimes, you can't just pass the decision on what to do with an error to
someone else.

---

These two examples distill, I think, pretty well, what I view as error
*handling*: An error is handled, when you destroy the error value. In that
parlance, `log.Error` handles any errors of the underlying writer by not
returning them. Every program needs to handle any error in *some* way, because
`main` can't return anything and the values need to go *somewhere*. Any HTTP
handler needs to actually *handle* errors, by translating them into HTTP
responses.

And in that parlance, packages like `pkg/errors` have little, really, to do with
error *handling* - instead, they provides you with a strategy for the case where
you are *not* handling your errors. In the same vein, proposals that address
the repetitive checking of errors via extra syntax do not really simplify their
handling at all - they just move it around a bit. I would term that *error
propagation*, instead - no doubt important, but keep in mind, that an error
that was *handled*, doesn't need to be propagated at all. So to me, a good
approach to error handling would be characterized by mostly obviating the need
for convenient error propagation mechanisms.

And to me, at least, it seems that we talk too little about how to handle
errors, in the end.

---

Does Go encourage explicit error handling? This is the phrasing very often used
to justify the repetitive nature, but I tend to disagree. Compare, for example,
Go's approach to checked exceptions in Java: There, errors are propagated via
exceptions. Every exception that could be thrown (theoretically) must be
annotated in the method signature. Any exception that you handle, has to be
mentioned in a try-catch-statement. And the compiler will refuse to compile a
program which does not explicitly mention how exceptions are handled. This, to
me, seems like the pinnacle of *explicit* error handling. Rust, too, requires
this - it introduces a `?` operator to signify propagating an error, but that,
still, is an explicit annotation. And apart from that, you can't use the return
value of a function that might propagate an error, without explicitly handling
that error first.

In Go, on the other hand, it is not only perfectly acceptable to ignore errors
when it makes sense (for example, I will always ignore errors created from
writing to a [`*bytes.Buffer`](https://godoc.org/bytes#Buffer.Write)), it is
actually often the only sensible thing to do. It is fundamentally not only
okay, but 99% of times *correct* to just completely ignore the error returned
by `fmt.Println`. And while it makes sense to check the error returned from
`json.Marshal` in your HTTP handler against `*json.MarshalError` (to
panic/log/complain loudly, because your code is buggy), any other errors
*should 99% of the time just be ignored*. And that's fine.

I believe that to say Go encourages explicit error handling, it would need some
mechanism of checked exceptions, Result types, or a requirement to pass an
[errcheck](https://github.com/kisielk/errcheck) like analysis in the compiler.

I think it would be closer to say, that Go encourages *local* error handling.
That is, the code that handles an error, is close to the code that produced it.
Exceptions encourages the two to be separated: There are usually several
or many lines of code in a single `try`-block, all of which share one
`catch`-block and it is hard to tell which of the lines produced it. And very
often, the actual error location is several stack frames deep. You could
contrast this with Go, where the error return is immediately obvious from the
code and if you have a line of error handling, it is usually immediately
attached to the function call that produced it.

However, that still seems to come short, in my view. After all, there is
nothing to force you to do that. And in fact, one of the most often [cited
articles about Go error handling](https://blog.golang.org/errors-are-values) is
often interpreted to encourage exactly that. Plus, a lot of people end up
writing `return err` far too often, simply propagating the error to be
*handled* elsewhere. And the proliferation of error-wrapping libraries happens
in the same vein: What their proponents phrase as "adding context to the error
value", I interpret as "adding back some of the information as a crutch, that
you removed when passing the error to non-local handling code". Sadly, far too
often, the error then ends up not being handled at all, as everyone just takes
advantage of that crutch. This leaves the end-user with an error message that is
essentially a poorly formatted, non-contiguous stacktrace.

Personally, I'd characterize Go's approach like this: In Go, error handling is
simply first-class code. By forcing you to use exactly the same control-flow
mechanisms and treat errors like any other data, Go encourages you to code your
error handling. Often that means a bunch of control flow to catch and recover
from any errors where they occur. But that's not "clutter", just as it is not
"clutter" to write `if n < 1 { return 1 }` when writing a Fibonacci function
(to choose a trivial example). It is just code. And yes, sometimes that code
might also store the error away or propagate it out-of-band to reduce
repetition *where it makes sense* - like in above blog post. But focussing on
the "happy path" is a bit of a distraction: Your *users* will definitely be
more happy about those parts of the control flow that make the errors disappear
or transform them into clean, actionable advise on how to solve the problem.

So, in my reading, the title of the Go blog post puts the emphasis in slightly
the wrong place - and often, people take the wrong message from it, in my
opinion. Not "errors are values", but "error handling is code".

---

So, what *would* be my advise for handling errors? To be honest, I don't know
yet - and I'm probably in no place to lecture anyone anyway.

Personally, I've been trying for the last couple of months to take a page out
of `http.Handler`s playbook and try, as much as possible, to completely avoid
returning an error. Instead of thinking "I should return an error here, in case
I ever do any operation that fails", I instead think "is there *any way at
all* I can get away with not returning an error here?". It doesn't always work
and sometimes you *do* have to pass errors around or wrap them. But I am
forcing myself to think very hard about handling my errors and it encourages a
programming-style of isolating failing components. The constraint of not being
able to return an error tends to make you creative in how to handle it.

---

<a id="footnote1"></a>[1] You might be tempted to suggest, that you could
define an `HTTPError`, containing the necessary info. Indeed, that's what the
[official Go blog](https://blog.golang.org/error-handling-and-go#TOC_3.) does,
so it can't be bad? And indeed, that *is* what they do, but note that they do
*not* actually return an `error` in the end - they return an `appError`, which
contains the necessary information. Exactly *because* they don't know how to
deal with general errors. So they translate any errors into a domain specific
type that carries the response. So, that is *not* the same as returning an
`error`.

I think *this* particular pattern is fine, though, personally, I don't
really see the point. Anything that builds an `appError` needs to provide
the complete response anyway, so you might as well just write it out
directly. YMMV. [⬆](#footnote1_back)
]]></content:encoded>
      <dc:date>2018-01-21T23:40:00+00:00</dc:date>
    </item>
    <item>
      <title>Generating entropy without imports in Go</title>
      <link>https://blog.merovius.de//2018/01/15/generating_entropy_without_imports_in_go.html</link>
      <description><![CDATA[tl;dr: I come up with a couple of useless, but entertaining ways to generate entropy without relying on any packages.
]]></description>
      <pubDate>Mon, 15 Jan 2018 01:04:30 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/01/15/generating_entropy_without_imports_in_go.html</guid>
      <content:encoded><![CDATA[**tl;dr: I come up with a couple of useless, but entertaining ways to generate entropy without relying on any packages.**

This post is inspired by a [comment on reddit](https://www.reddit.com/r/golang/comments/7qb74r/can_golang_package_source_with_no_imports_be/dso7xsc/),
saying

> […]given the constraints of no imports and the function signature:
>
> `func F(map[string]string) map[string]string { ... }`
>
> F must use a deterministic algorithm, since it is a deterministic algorithm
> it can be represented in a finite state machine.

Now, the point of this comment was to talk about how to then compile such a
function into a deterministic finite state machine, but it got me thinking
about a somewhat different question. If we disallow any imports and assume a
standard (gc) Go implementation - how many ways can we find to create a
non-deterministic function?

So, the challenge I set to myself was: Write a function `func() string` that a)
can not refer to any qualified identifier (i.e. no imports) and b) is
non-deterministic, that is, produces different outputs on each run. To start me
off, I did add a couple of helpers, to accumulate entropy, generate random
numbers from it and to format strings as hex, without any imports:

```go
type rand uint32

func (r *rand) mix(v uint32) {
	*r = ((*r << 5) + *r) + rand(v)
}

func (r *rand) rand() uint32 {
	mx := rand(int32(*r)>>31) & 0xa8888eef
	*r = *r<<1 ^ mx
	return uint32(*r)
}

func hex(v uint32) string {
	var b []byte
	for v != 0 {
		if x := byte(v & 0xf); x < 10 {
			b = append(b, '0'+x)
		} else {
			b = append(b, 'a'+x-10)
		}
		v >>= 4
	}
	return string(b)
}
```

Obviously, these could be inlined, but separating them allows us to reuse them
for our different functions. Then I set about the actual task at hand.

##### Method 1: Map iteration

In Go, the iteration order of maps is [not specified](https://golang.org/ref/spec#For_range):

> The iteration order over maps is not specified and is not guaranteed to be
> the same from one iteration to the next.

But `gc`, the canonical Go implementation, actively
[randomizes](https://golang.org/doc/go1.3#map) the map iteration order to
prevent programs from depending on it. We can use this, to receive some of
entropy from the runtime, by creating a map and iterating over it:

```go
func MapIteration() string {
  var r rand

  m := make(map[uint32]bool)
  for i := uint32(0); i < 100; i++ {
    m[i] = true
  }
  for i := 0; i < 1000; i++ {
    for k := range m {
      r.mix(k)
      break // the rest of the loop is deterministic
    }
  }
  return hex(r.rand())
}

```

We first create a map with a bunch of keys. We then iterate over it a bunch of
times; each map iteration gives us a different start index, which we mix into
our entropy pool.

##### Method 2: Select

Go actually defines [a way](https://golang.org/ref/spec#Select_statements) in
which the runtime is giving us access to entropy directly:

> If one or more of the communications can proceed, a single one that can
> proceed is chosen via a uniform pseudo-random selection.

So the spec guarantees that if we have multiple possible communications in a
select, the case *has* to be chosen non-deterministically. We can, again,
extract that non-determinism:

```go
func Select() string {
	var r rand

	ch := make(chan bool)
	close(ch)
	for i := 0; i < 1000; i++ {
		select {
		case <-ch:
			r.mix(1)
		case <-ch:
			r.mix(2)
		}
	}
	return hex(r.rand())
}
```

We create a channel and immediately close it. We then create a select-statement
with two cases and depending on which was taken, we mix a different value into
our entropy pool. The channel is closed, to guarantee that communication can
always proceed. This way, we extract one bit of entropy per iteration.

Note, that there is no racing or concurrency involved here: This is simple,
single-threaded Go code. The randomness comes directly from the runtime. Thus,
this should work in any compliant Go implementation. The [playground](https://play.golang.org/),
however, is not compliant with the spec in this regard, strictly speaking. It
is deliberately deterministic.

##### Method 3: Race condition

This method exploits the fact, that on a multi-core machine at least, the Go
scheduler is non-deterministic. So, if we let two goroutines race to write a
value to a channel, we can extract some entropy from which one wins this race:

```go
func RaceCondition() string {
	var r rand

	for i := 0; i < 1000; i++ {
		ch := make(chan uint32, 2)
		start := make(chan bool)
		go func() {
			<-start
			ch <- 1
		}()
		go func() {
			<-start
			ch <- 2
		}()
		close(start)
		r.mix(<-ch)
	}

	return hex(r.rand())
}
```

The `start` channel is there to make sure that both goroutines become runnable
concurrently. Otherwise, the first goroutine would be relatively likely to
write the value before the second is even spawned.

##### Method 4: Allocation/data races

Another thought I had, was to try to extract some entropy from the allocator or
GC. The basic idea is, that the address of an allocated value might be
non-deterministic - in particular, if we allocate a lot. We can then try use
that as entropy.

However, I could not make this work very well, for the simple reason that Go
does not allow you to actually do anything with pointers - except dereferencing
and comparing them for equality. So while you might get non-deterministic
values, those values can't be used to actually generate random numbers.

I thought I might be able to somehow get a string or integer representation of
some pointer without any imports. One way I considered was inducing a
runtime-panic and recovering that, in the hope that the error string would
contain a stacktrace or offending values. However, none of the error strings
created by the runtime actually seem to contain any values that could be used
here.

I also tried a workaround to interpret the pointer as an integer, by exploiting
[race conditions](https://research.swtch.com/gorace) to do unsafe operations:

```go
func DataRace() string {
	var r rand

	var data *uintptr
	var addr *uintptr

	var i, j, k interface{}
	i = (*uintptr)(nil)
	j = &data

	done := false
	go func() {
		for !done {
			k = i
			k = j
		}
	}()
	for {
		if p, ok := k.(*uintptr); ok && p != nil {
			addr = p
			done = true
			break
		}
	}

	data = new(uintptr)
	r.mix(uint32(*addr))
	return hex(r.rand())
}
```

It turns out, however, that at least this particular instance of a data race
has been fixed since Russ Cox wrote that blog post. In Go 1.9, this code just
loops endlessly. I tried it in Go 1.5, though, and it works there - but we
don't get a whole lot of entropy (addresses are not *that* random). With other
methods, we could re-run the code to collect more entropy, but in this case,
I believe the escape analysis gets into our way by stack-allocating the
pointer, so it will be the same one on each run.

I like this method, because it uses several obscure steps to work, but on the
other hand, it's the least reliable and it requires an old Go version.

##### Your Methods?

These are all the methods I could think of; but I'm sure I missed a couple. If
you can think of any, feel free to let me know on
[Twitter](https://twitter.com/TheMerovius),
[reddit](https://www.reddit.com/r/golang/comments/7qfvzu/generating_entropy_without_imports_in_go/)
or [hackernews](https://news.ycombinator.com/item?id=16147475) :) I also posted
the code in a
[gist](https://gist.github.com/Merovius/283ff12a1186d001815485fca1094968), so
you can download and run it yourself, but keep in mind, that the last method
busy-loops in newer Go versions.
]]></content:encoded>
      <dc:date>2018-01-15T01:04:30+00:00</dc:date>
    </item>
    <item>
      <title>Monads are just monoids in the category of endofunctors</title>
      <link>https://blog.merovius.de//2018/01/08/monads-are-just-monoids.html</link>
      <description><![CDATA[tl;dr: I explain the mathematical background of a joke-explanation of monads. Contains lots of math and a hasty introduction to category theory.
]]></description>
      <pubDate>Mon, 08 Jan 2018 00:30:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/01/08/monads-are-just-monoids.html</guid>
      <content:encoded><![CDATA[**tl;dr: I explain the mathematical background of a joke-explanation of monads. Contains lots of math and a hasty introduction to category theory.**

There is a running gag in the programming community, that newcomers will often
be confused by the concept of monads (which is how sequential computations are
modeled in purely functional languages) and getting the explanation "it is
simple, really: Monads are just monoids in the category of endofunctors". This
is not meant as an actual explanation, but rather to poke a bit of fun at the
habit of functional programmers to give quite abstract and theoretical
explanations at times, that are not all that helpful.

However, given my background in mathematics, I decided that I wanted to
actually approach Haskell from this point of view: I am interested in how it
uses math to model programming and also to, after several years of doing mostly
engineering focused programming work, flex my math muscles again
- as there is quite a bit of interesting math behind these concepts.

The quote is from a pretty popular [book about category
theory](http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf) and is, in full:

> All told, a monad in \\(X\\) is just a monoid in the category of endofunctors
> of \\(X\\), with product \\(\times\\) replaced by composition of endofunctors
> and unit set by the identity endofunctor.

This, of course, is an explanation of the *mathematical* concept of monads,
not meant for programmers. Most explanations of the quote that I found either
assumed quite a bit of knowledge in Haskell or took a lot of liberties with the
mathematical concepts (and relied a lot on "squinting") or both. This write up
is my attempt, to walk through all the concepts needed to explain monads as a
mathematical concept and how it relates to Haskell - with as little squinting
as possible.

Of course, there are a couple of disclaimers, I should start with:

1. This is not the best way to understand what monads are, if you are actually
   interested in using them to program. In fact, it is literally the worst way.
   I would recommend [this intro](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf),
   which takes a much more practical approach.
2. This is not the best way to understand how category theory works, if you are
   actually interested in learning mathematics. In fact, it is literally the
   worst way. I would recommend [the book the quote is from](http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf),
   it's quite good (but assumes a math audience).
3. I haven't done mathematics in years. I also don't know much Haskell either.
   So I might be getting a bunch of stuff wrong to varying degrees. I'm sure I
   will hear all about it :)
4. Even if I would *understand* everything correctly, there are still a lot of
   details, mostly of technical nature, I had to omit, to keep this "short".
   Not that it is short.

Originally, I intended this to be the ultimate explanation, which would teach
Haskellers category theory, mathematicians Haskell and people who know neither
both. Unsurprisingly, this is not what this is, at all. It ended up mostly a
write up to assure myself that I understood the path myself. If anything, you
can treat this as a kind of "reading companion": If you want to understand this
topic of the intersection between category theory and functional programming,
this post can lead you through the correct terms to search for and give you a
good idea what to focus on, in the respective Wikipedia articles.

With all that out of the way, let's begin.

##### Categories

In mathematics, a category is (roughly) a collection of objects and a
collection of arrows between them. There is not a lot of meaning behind these,
but it will probably help you to think of objects as sets and arrows as
mappings.  Every arrow goes from an object (the *domain*) to an object (the
*codomain*) and we write an arrow as \\(f:X\to Y\\), where \\(f\\) is the name
of the arrow, \\(X\\) is the domain and \\(Y\\) is the codomain. Just like with
mappings, there can be many arrows between any given pair of objects - or there
may be none.

We do need *some* restrictions: First, we require a specific *identity* arrow
\\(\mathrm{id}:X\to X\\) attached to every object \\(X\\), which has \\(X\\) as
both domain and codomain. Secondly, we require (some) arrows to be
*composable*. That is if we have two arrows \\(f:X\to Y,g:Y\to Z\\) - so,
whenever the domain of \\(g\\) is the codomain of \\(f\\) - there should also
be a composed arrow[¹](#footnote1)<a id="footnote1_back"></a>
\\(g\circ f: X\to Z\\), that shares the domain with \\(f\\) and the codomain with
\\(g\\).

Furthermore, the identity arrows must act as a *unit* for composition, that is,
for every arrow \\(f\\) we require \\(\mathrm{id}\circ f = f = f
\circ\mathrm{id}\\). We also require composition to be *associative*, that is
\\((f\circ g)\circ h = f\circ(g\circ h)\\) (whenever all compositions exist)[²](#footnote2)<a id="footnote2_back"></a>.

When we talk about a category, we often draw diagrams like this:

<div>
\[
\require{AMScd}

\begin{CD}
X       @>{f}>> Y       \\
@V{g}VV         @VV{p}V \\
Z       @>>{q}> W       \\
\end{CD}
\]
</div>

They show some of the objects and arrows from the category in a compact way.
This particular diagram indicates that there are four objects and four arrows
involved, with obvious domains and codomains. We only draw a subset of the
objects and arrows, that is interesting for the point we are trying to make -
for example, above diagram could also contain, of course, identity arrows and
compositions \\(p\circ f\\) and \\(q\circ g\\)), but we didn't draw them. In
a square like this, we can take two paths from \\(X\\) to \\(W\\). If these
paths are identical (that is, \\(p\circ f = q\circ g\\), we say that the
square *commutes*. A *commutative* diagram is a diagram, in which any square
commutes, that is, it does not matter which path we take from any object to
another. Most of the time, when we draw a diagram, we intend it to be
commutative.

So, to summarize, to define a mathematical category, we need to:

1. Specify what our objects are
2. Specify what our arrows are, where each arrow starts and ends at a certain
   object
3. This collection of arrows need to include an arrow \\(\mathrm{id}\_X\\) for
   every object \\(X\\), which starts and ends at \\(X\\)
4. And we need to be able to glue together arrows \\(f:X\to Y\\) and \\(g:Y\to
   Z\\) to an arrow \\(g\circ f: X\to Z\\)

In Haskell, we work on the category **Hask**, which consists of:

1. The objects are *types*: `Int` is an object, `String` is an object but also
   `Int | String`, `String -> Int` and any other complicated type you can think
   of.
2. The arrows are *functions*: `f :: a -> b` is a function taking an `a` as an
   input and returning a `b` and is represented by an arrow `f`, which has `a`
   as its domain and `b` as its codomain.  So, for example, `length :: String
   -> Int` would start at the type `String` and end at `Int`.
3. Haskell has a function `id :: a -> a` which gives us the identity arrow
   for any type `a`.
4. We can compose functions with the operator `(.) :: (b -> c) -> (a -> b) ->
   (a -> c)`. Note, that this follows the swapped notation of \\(\circ\\), where
   the input type of the left function is the output type of the right function.

In general, category theory is concerned with the *relationship between*
categories, whereas in functional programming, we usually only deal with this
one category. This turns out to be both a blessing and a curse: It means that
our object of study is much simpler, but it also means, that it is sometimes
hard to see how to apply the general concepts to the limited environment of
functional programming.

##### Monoids

Understanding categories puts us in the position to understand *monoids*. A
monoid is the generalized structure underlying concepts like the natural
numbers: We can *add* two natural numbers, but we can't (in general) *subtract*
them, as there are no negative numbers. We also have the number \\(0\\), which,
when added to any number, does nothing - it acts as a *unit* for addition. And
we also observe, that addition is *associative*, that is, when doing a bunch of
additions, the order we do them in doesn't matter.

The same properties also apply to other constructs. For example, if we take all
maps from a given set to itself, they can be composed and that composition is
associative and there is a unit element (the identity map).

This provides us with the following elements to define a monoid:

1. A set \\(M\\)
2. An operation \\(\star\colon M\times M\to M\\), which "adds" together two elements to
   make a new one
3. We need a special unit element \\(u\in M\\), which acts neutrally when added to
   any other element, that is \\(m\star u=m=u\star m\\)
4. The operation needs to be associative, that is we always require
   \\(m\star(n\star k)=(m\star n)\star k\\)

There is another way to frame this, which is closer in line with category theory.
If we take \\(1 := \\{0\\}\\) to be a 1-element set, we can see that the
elements of \\(M\\) are in a one-to-one correspondence to functions \\(1\to M\\):
Every such function chooses an element of \\(M\\) (the image of \\(0\\)) and
every element \\(m\in M\\) fixes such a function, by using \\(f(0) := m\\).
Thus, instead of saying "we need a special element of \\(M\\)", we can also
choose a special *function* \\(\eta: 1\to M\\). And instead of talking about an
"operation", we can talk about a function \\(\mu: M\times M\to M\\). Which
means, we can define a monoid via a commutative diagram like so:

<div>
\[
\begin{CD}
1 \\
@V{\eta}VV \\
M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\times M \\
@V{\mu}VV \\
M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\times 1 @>{\mathrm{id}\times\eta}>> M\times M @<{\eta\times\mathrm{id}}<< 1\times M \\
@V{\pi_1}VV                           @V{\mu}VV                             @V{\pi_2}VV \\
M         @>{\mathrm{id}}>>           M         @<{\mathrm{id}}<<           M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\times M\times M @>{\mu\times\mathrm{id}}>> M\times M \\
@V{\mathrm{id}\times\mu}VV                   @V{\mu}VV \\
M\times M         @>{\mu}>>                  M \\
\end{CD}

\]
</div>

\\(\pi\_1\\) and \\(\pi\_2\\) here, are the functions that project to the first
or second component of a cross product respectively (that is \\(\pi\_1(a, b) :=
a, \pi\_2(a, b) := b\\)) and e.g. \\(\mathrm{id}\times\eta\\) is the map that
applies \\(\mathrm{id}\\) to the first component of a cross-product and
\\(\eta\\) to the second: \\(\mathrm{id}\times\eta(m, 0) = (m, \eta(0))\\).

There are four sub-diagrams here:

1. The first diagram just says, that we need an arrow \\(\eta:1\to M\\). This
   chooses a unit element for us.
2. Likewise, the second diagram just says, that we need an arrow
   \\(\mu:M\times M\to M\\). This is the operation.
3. The third diagram tells us that the chosen by \\(\eta\\) should be a unit
   for \\(\mu\\). The commutativity of the left square tells us, that it should
   be right-neutral, that is
   \\[ \forall m\in M: m = \pi\_1(m, 0) = \mu(\mathrm{id}\times\eta(m, 0)) = \mu(m, \eta(0)) \\]
   and the commutativity of the right square tells us, that it should be left-neutral, that is
   \\[ \forall m\in M: m = \pi\_2(0,m) = \mu(\eta\times\mathrm{id}(0, m)) = \mu(\eta(0), m) \\]

Thus, the first diagram is saying that the element chosen by \\(\eta\\) should
act like a unit. For example, the left square says

\\[\pi\_1(m,0) = \mu((\mathrm{id}\times\eta)(m,0)) = \mu(m,\eta(0))\\]

Now, writing \\(\mu(m,n) = m\star n\\) and \\(\eta(0) = u\\), this is equivalent to saying \\(m = u\star m\\).

The second diagram is saying that \\(\mu\\) should be associative: The top arrow
combines the first two elements, the left arrow combines the second two. The right and
bottom arrows then combine the result with the remaining element respectively,
so commutativity of that square means the familiar \\(m\star (n\star k) = (m\star n)\star k\\).

Haskell has the concept of a monoid too. While it's not really relevant to the
discussion, it might be enlightening to see, how it's modeled. A monoid in
Haskell is a type-class with two (required) methods:

```haskell
class Monoid a where
  mempty :: a
  mappend :: a -> a -> a
```

Now, this gives us the operation (`mappend`) and the unit (`a`), but where are
the requirements of associativity and the unit acting neutrally? The Haskell
type system is unable to codify these requirements, so they are instead given
as a "law", that is, any implementation of a monoid is supposed to have these
properties, to be manually checked by the programmer:

* `mappend mempty x = x` (the unit is left-neutral)
* `mappend x mempty = x` (the unit is right-neutral)
* `mappend x (mappend y z) = mappend (mappend x y) z` (the operation is associative)

##### Functors

I mentioned that category theory investigates the relationship between
categories - but so far, everything we've seen only happens inside a single
category. Functors are, how we relate categories to each other. Given two
categories \\(\mathcal{B}\\) and \\(\mathcal{C}\\), a *functor*
\\(F:\mathcal{B}\to \mathcal{C}\\) assigns to every object \\(X\\) of
\\(\mathcal{B}\\), an object \\(F(X)\\) of \\(\mathcal{C}\\). It also assigns
to every arrow \\(f:X\to Y\\) in \\(\mathcal{B}\\) a corresponding arrow
\\(F(f): F(X)\to F(Y)\\) in \\(\mathcal{C}\\)[³](#footnote3)<a
id="footnote3_back"></a>. So, a functor transfers arrows from one category
to another, preserving domain and codomain. To actually preserve the
structure, we also need it to preserve the extra requirements of a category,
identities and composition. So we need, in total:

1. An object map, \\(F:O\_\mathcal{B} \to O\_\mathcal{C}\\)
2. An arrow map, \\(F:A\_\mathcal{B}\to A\_\mathcal{C}\\), which preserves
   start and end object, that is the image of an arrow \\(X\to Y\\) starts at
   \\(F(X)\\) and ends at \\(F(Y)\\)
3. The arrow map has to preserve identities, that is \\(F(\mathrm{id}\_X) =
   \mathrm{id}\_{F(X)}\\)
4. The arrow map has to preserve composition, that is \\(F(g\circ f) =
   F(g)\circ F(f)\\).

A trivial example of a functor is the *identity functor* (which we will call
\\(I\\)), which assigns each object to itself and each arrow to itself - that
is, it doesn't change the category at all.

A simple example is the construction of the *free monoid*, which maps from the
category of sets to the category of monoids. The Free monoid \\(S^\*\\) on a
set \\(S\\) is the set of all finite length strings of elements of \\(S\\),
with concatenation as the operation and the empty string as the unit. Our
object map then assigns to each set \\(S\\) its free monoid \\(S^\*\\). And our
arrow map assigns to each function \\(f:S\to T\\) the function \\(f^\*:S^\*\to
T^\*\\), that applies \\(f\\) to each element of the input string.

There is an interesting side note here: Mathematicians love to abstract.
Categories arose from the observation, that in many branches of mathematics we
are researching some class of objects with some associated structure and those
maps between them, that preserve this structure. It turns out that category
theory is a branch of mathematics that is researching the objects of
categories, with some associated structure (identity arrows and composition)
and maps (functors) between them, that preserve that structure. So it seems
obvious that we should be able to view categories *as objects of a category*,
with functors as arrows. Functors can be composed (in the obvious way) and
every category has an identity functor, that just maps every object and arrow
to itself.

Now, in Haskell, Functors are again a type class:

```haskell
class Functor f where
  fmap :: (a -> b) -> (f a -> f b)
```

This looks like our arrow map: It assigns to each function `g :: a -> b` a
function `fmap g :: f a -> f b`. The object map is implicit: When we write `f a`,
we are referring to a new type, that depends on `a` - so we "map" `a` to `f a`
[⁴](#footnote4)<a id="footnote4_back"></a>.

Again, there are additional requirements the type system of Haskell can not
capture. So we provide them as laws the programmer has to check manually:

* `fmap id  ==  id` (preserves identities)
* `fmap (f . g)  ==  fmap f . fmap g` (preserves composition)

There is one thing to note here: As mentioned, in Haskell we only really deal
with one category, the category of types. That means that a functor always maps
from the category of types to *itself*. In mathematics, we call such a functor,
that maps a category to itself, an *endofunctor*. So we can tell, that in
Haskell, every functor is automatically an endofunctor.

##### Natural transformations

We now understand categories and we understand functors. We also understand,
that we can look at something like the category of categories. But the
definition of a monad given to us talks about the *category of endofunctors*.
So we seem to have to step up yet another level in the abstraction hierarchy
and somehow build this category. As objects, we'd like to have endofunctors -
and arrows will be *natural transformations*, which take one functor to
another, while preserving its internal structure (the mapping of arrows). If
that sounds complicated and abstract, that's because it is.

We need two functors \\(F,G:\mathcal{B}\to \mathcal{C}\\) of the same "kind"
(that is, mapping to and from the same categories). A natural transformation
\\(\eta:F\dot\to G\\) assigns an arrow[⁵](#footnote5)<a
id="footnote5_back"></a> \\(\eta\_X: F(X)\to G(X)\\) (called a *component* of
\\(\eta\\)) to every object in \\(\mathcal{B}\\). So a component \\(\eta\_X\\)
describes, how we can translate the action of \\(F\\) on \\(X\\) into the
action of \\(G\\) on \\(X\\) - i.e. how to translate their object maps. We also
have to talk about the translation of the arrow maps. For that, we observe that
for any arrow \\(f:X\to Y\\) in \\(\mathcal{B}\\), we get four new arrows in
\\(\mathcal{C}\\):

<div>
\[
\begin{CD}
X       \\
@V{f}VV \\
Y       \\
\end{CD}

\hspace{1em}

\begin{CD}
F(X)        @>{\eta_X}>> G(X)       \\
@V{F(f)}VV               @VV{G(f)}V \\
F(Y)        @>>{\eta_Y}> G(Y)       \\
\end{CD}
\]
</div>

For a natural transformation, we require the resulting square to commute.

So, to recap: To create a natural transformation, we need

1. Two functors \\(F,G:\mathcal{B}\to\mathcal{C}\\)
2. For every object \\(X\\) in \\(\mathcal{B}\\), an arrow \\(\eta\_X: F(X)\to
   G(X)\\)
3. The components need to be compatible with the arrow maps of the functors:
   \\(\eta\_Y\circ F(f) = G(f)\circ \eta\_X\\).

In Haskell, we can define a natural transformation like so:

```haskell
class (Functor f, Functor g) => Transformation f g where
    eta :: f a -> g a
```

`f` and `g` are functors and a natural transformation from `f` to `g` provides
a map `f a -> g a` for every type `a`. Again, the requirement of compatibility
with the actions of the functors is not expressible as a type signature, but we
can require it as a law:

* `eta (fmap fn a) = fmap fn (eta a)`

##### Monads

This, finally, puts us in the position to define monads. Let's look at our quote above:

> All told, a monad in \\(X\\) is just a monoid in the category of endofunctors
> of \\(X\\), with product \\(\times\\) replaced by composition of endofunctors
> and unit set by the identity endofunctor.

It should be clear, how we can *compose* endofunctors. But it is important,
that this is a different view of these things than if we'd look at the category
of categories - there, objects are categories and functors are arrows, while
here, objects are *functors* and arrows are natural transformations. That
shows, how composition of functors can take the role of the cross-product of
sets: In a set-category, the cross product makes a new set out of two other
set. In the category of endofunctors, composition makes a new endofunctor out
of two other endofunctors.

When we defined monoids diagrammatically, we also needed a cross product of
mappings, that is, given a map \\(f:X\_1\to Y\_1\\) and a map \\(g:X\_2\to
Y\_2\\), we needed the map \\(f\times g: X\_1\times X\_2\to Y\_1\times
Y\_2\\), which operated on the individual constituents. If we want to replace
the cross product with composition of endofunctors, we need an equivalent for
natural transformations. That is, given two natural transformations
\\(\eta:F\to G\\) and \\(\epsilon:J\to K\\), we want to construct a natural
transformation \\(\eta\epsilon:J\circ F\to K\circ G\\). This diagram
illustrates how we get there (working on components):

<div>
\[
\begin{CD}
F(X)    @>{\eta_X}>>    G(X)                  @.                            \\
@V{J}VV                 @VV{J}V               @.                            \\
J(F(X)) @>{J(\eta_X)}>> J(G(X))               @>{\epsilon_{G(X)}}>> K(G(X)) \\
\end{CD}
\]
</div>

As we can see, we can build an arrow \\(\epsilon\_{G(X)}\circ J(\eta\_X):
J(F(X)) \to K(G(X))\\), which we can use as the components of our natural
transformation \\(\eta\epsilon:J\circ F\to K\circ G\\). This construction is
called the *horizontal composition* of natural transformations. We should
verify that this is indeed a natural transformation - for now, let's just
accept that it follows from the naturality of \\(\eta\\) and \\(\epsilon\\).

Lastly, there is an obvious natural transformation taking a functor to itself;
each component being just the identity arrow. We call that natural
transformation \\(\iota\\), staying with the convention of using Greek letters
for natural transformations.

With this, we can redraw the diagram we used to define monoids above, the
replacements indicated by the quotes:

<div>
\[
\begin{CD}
I \\
@V{\eta}VV \\
M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\circ M \\
@V{\mu}VV \\
M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\circ I @>{\iota\ \eta}>> M\circ M  @<{\eta\ \iota}<< I\circ M \\
@VVV                       @V{\mu}VV                   @VVV \\
M        @>{\iota}>>       M         @<{\iota}<<       M \\
\end{CD}

\hspace{1em}

\begin{CD}
M\circ M\circ M @>{\mu\ \iota}>> M\circ M  \\
@V{\iota\ \mu}VV                 @V{\mu}VV \\
M\circ M        @>{\mu}>>      M         \\
\end{CD}
\]
</div>

The vertical arrows in the middle diagram now simply apply the composition of
functors, using that the identity functor is a unit.

These diagrams encode these conditions on our natural transformations[⁶](#footnote6)<a id="footnote6_back"></a>:

* \\(\mu\circ\eta\iota = \mu = \iota\eta\circ\mu\\), that is \\(\eta\\) serves as a unit
* \\(\mu\circ\mu\iota = \mu\circ\iota\mu\\), that is \\(\mu\\) is associative

To recap, a monad, in category theory, is

* An endofunctor \\(M\\)
* A natural transformation \\(\eta: I\to M\\), which serves as an identity for
  horizontal composition.
* A natural transformation \\(\mu: M\circ M\to M\\), which is associative in
  respect to horizontal composition.

Now, let's see, how this maps to Haskell monads.

First, what is the identity functor in Haskell? As we pointed out above, the
object function of functors is implicit, when we write `f a` instead of `a`. As
such, the identity functor is simply `a` - i.e. we map any type to itself.
`fmap` of that functor would thus also just be the identity
`fmap :: (a -> a) -> (a -> a)`.

So, what would our natural transformation \\(\eta\\) look like? As we said, a
natural transformation between two functors is just a map `f a -> g a`. So (if
we call our endofunctor `m`) the identity transformation of our monoid is
`eta :: a -> m a`
mapping the identity functor to `m`. We also need our monoidal operation, which
should map `m` applied twice to `m`:
`mu :: m (m a) -> m a`.

Now, Haskellers write `return` instead of `eta` and write `join` instead of
`mu`, giving us the type class[⁷](#footnote7)<a id="footnote7_back"></a>

```haskell
class (Functor m) => Monad where
  return :: a -> m a
  join :: m (m a) -> m a
```

As a last note, it is worth pointing out that you usually won't implement
`join`, but instead a different function, called "monadic bind":

```haskell
(>>=) :: m a -> (a -> m b) -> m b
```

The reason is, that this more closely maps to what monads are actually *used*
for in functional programming. But we can move between `join` and `>>=` via

```haskell
(>>=) :: m a -> (a -> m b) -> m b
v >>= f = join ((fmap f) v)

join :: m (m a) -> m a
join v = v >>= id
```

##### Conclusion

This certainly was a bit of a long ride. It took me *much* longer than
anticipated both to understand all the steps necessary and to write them down.
I hope you found it helpful and I hope I didn't make too many, too glaring
mistakes. If so (either), feel free to let me know on
[Twitter](https://twitter.com/TheMerovius),
[reddit](https://www.reddit.com/r/haskell/comments/7oudxd/monads_are_just_monoids_in_the_category_of/)
or [Hacker News](https://news.ycombinator.com/item?id=16093508) - but please
remember to be kind :)

*I want to thank [Tim Adler](https://twitter.com/r4dler) and
[mxf+](https://twitter.com/9b17fe) for proof-reading this absurdly long post
and for making many helpful suggestions for improvements*

---

<a id="footnote1"></a>[1] It is often confusing to people, that the way the
arrows point in the notation and the order they are written seems to contradict
each other: When writing \\(f:X\to Y\\) and \\(g:Y\to Z\\) you might reasonably
expect their composite to work like \\(f\circ g: X\to Z\\), that is, you glue
together the arrows in the order you are writing them.

The fact that we are not doing that is a completely justified criticism,
that is due to a historical accident - we write function application from
right to left, that is we write \\(f(x)\\), for applying \\(f\\) to \\(x\\).
Accordingly, we write \\(g(f(x))\\), when applying \\(g\\) to the result of
applying \\(f\\) to \\(x\\). And we chose to have the composite-notation be
consistent with *that*, instead of the arrow-notation.

I chose to just eat the unfortunate confusion, as it turns out Haskell is
doing exactly the same thing, so swapping things around would just increase
the confusion.

Sorry. [⬆](#footnote1_back)

<a id="footnote2"></a>[2] Keep in mind that this is a different notion from the
ones for monoids, which we come to a bit later: While the formulas seem the
same and the identities look like a unit, the difference is that only certain
arrows can be composed, not all. And that there are many identity arrows, not
just one. However, if we would have only *one* object, it would have to be the
domain and codomain of every arrow and there would be exactly one identity
arrow. In that case, the notions *would* be the same and indeed, "a category
with exactly one object" is yet another way to define monoids.
[⬆](#footnote2_back)

<a id="footnote3"></a>[3] It is customary, to use the same name for the object
and arrow map, even though that may seem confusing. A slight justification of
that would be, that the object map is already given by the arrow map anyway: If
\\(F\\) is the arrow map, we can define the object map as \\(X\mapsto
\mathrm{dom}(F(\mathrm{id}\_X))\\). So, given that they are always occurring
together and you can make one from the other, we tend to just drop the
distinction and save some symbols.

What was that? Oh, you thought Mathematicians where precise? Ha!
[⬆](#footnote3_back)

<a id="footnote4"></a>[4] It is important to note, that this is not really a
*function*. Functions operate on values of a given type. But here, we are
operating on *types* and Haskell has no concept of a "type of types" built in
that a function could operate on. There are constructs operating on types to
construct new types, like `data`, `type`, `newtype` or even `deriving`. But
they are special syntactical constructs that exist outside of the realm of
functions.

This is one of the things that was tripping me up for a while: I was trying to
figure out, how I would map types to other types in Haskell or even talk about
the object map. But the most useful answer is "you don't". [⬆](#footnote4_back)

<a id="footnote5"></a>[5] An important note here, is that the \\(\eta\_X\\) are
*arrows*. Where the object map of a functor is just a general association which
could look anything we like, the components of a natural transformation need to
preserve the internal structure of the category we are working in.
[⬆](#footnote5_back)

<a id="footnote6"></a>[6] You will often see these conditions written
differently, namely written e.g. \\(\mu M\\) instead of \\(\mu\iota\\). You can
treat that as a notational shorthand, it really means the same thing.
[⬆](#footnote6_back)

<a id="footnote7"></a>[7] There is a technicality here, that Haskell also has
an intermediate between functor and monad called "applicative". As I understand
it, this does not have a clear category theoretical analogue. I'm not sure why
it exits, but I believe it has been added into the hierarchy after the fact.
[⬆](#footnote7_back)

]]></content:encoded>
      <dc:date>2018-01-08T00:30:00+00:00</dc:date>
    </item>
    <item>
      <title>My case for veganism</title>
      <link>https://blog.merovius.de//2018/01/02/my-case-for-veganism.html</link>
      <description><![CDATA[I&#39;m going to try to make an argument for being vegan, but, to be clear, it is
not very likely to convince you to change your eating habits. It is not
designed to - it is only supposed to change the way you think about it. I
mention all of that, so you are aware that I don&#39;t care what your conclusions
are here. If you are reading this, you should do so out of a genuine interest
of my motives and for the purpose of self-reflection - not to pick a fight with
that vegan dude and really show him he&#39;s wrong. I will not debate the content
of this article with you. So, with that out of the way, here is a thought
experiment:
]]></description>
      <pubDate>Tue, 02 Jan 2018 00:23:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2018/01/02/my-case-for-veganism.html</guid>
      <content:encoded><![CDATA[I'm going to try to make an argument for being vegan, but, to be clear, it is
not very likely to convince you to change your eating habits. It is not
designed to - it is only supposed to change the way you think about it. I
mention all of that, so you are aware that *I don't care* what your conclusions
are here. If you are reading this, you should do so out of a genuine interest
of my motives and for the purpose of self-reflection - not to pick a fight with
that vegan dude and really show him he's wrong. I will not debate the content
of this article with you. So, with that out of the way, here is a thought
experiment:

> Say, we would live in a Star Trek like post-scarcity society. Energy is all
> but abundant and we figured out replicator-technology, that can instantly
> create anything we like out of it. You get offer the choice between two
> meals, one is a delicious steak dinner (or whatever), made in a traditional
> manner. The second is the same thing, but from a replicator. Both are
> indistinguishable, they taste the same, they have the same nutritional and
> chemical composition, they cost the same. *They only differ in how they're
> made.*

You might be trying to rules-lawyer this. You might be trying to make up an
argument, for why the replicator-steak would *have* to be worse. Or that the
cow would already be dead, so it wouldn't matter. But that is obviously not the
point of this thought experiment (and remember, you don't have to convince
anyone of being right, here). The point is, that I strongly believe that the
vast majority of people would agree, that *all things being equal*, choosing
the meal that no animal suffered for is *the correct choice*. And if you truly
believe that it isn't, if you can honestly say to yourself that it doesn't
matter: You won't gain anything from the rest of this article. You are relieved
and might now just as well stop reading.

The point I am trying to make, is that you probably already know all the
reasons you *should* be vegan. It's very likely that you already have an
intuitive understanding of all the reasons in the "pro veganism" column of your
pro/contra list. And it really shouldn't be necessary to convince you it's a
good idea, in general.

Why then, do so few people actually *choose* to be vegan, if they are fully
aware of all the reasons to do so? The obvious answer is: Because not all
things *are* being equal. There is a "contra veganism" column and it's filled
with many good reasons *not* to. What reasons those are, is deeply individual.
It might be due to health. Due to an appeal to nature. Convenience. Money.
Availability. Taste. Or maybe just priorities: Other things seem more important
and deserving of your energy. *And that's okay*. We all have to make hundreds
of decisions every day and weigh these kinds of questions. And sometimes we do
things that we shouldn't and we usually have good reasons to. And sometimes we
compromise and don't go *all the way*, but just do the best we feel able to and
that's fine too. Nobody has to be perfect all the time.

The reason, I'm writing this article anyway, is that there is a fundamental
difference between the two questions "Why are you vegan?" and "Why are you not
*not* vegan?". When you ask me why I am vegan, you are making the conversation
inherently about my values and you will usually end up attacking them - not
because you disagree with them (you likely are not), but just because that's
the natural progression of this question. And to be absolutely clear: I don't
owe you a justification for my value system. I'm sorry if that sounds harsh,
but the topic is mostly
[really annoying](/2017/10/20/a-day-in-the-life-of-an-omnivore.html)
to me (as hard as that may be to believe at this point).

A more sensible question, though, is to ask how to best *mitigate the
contra*-column. If we agree that, fundamentally, the points in the pro-column
are valid and shared reasons, we can proceed into the much more productive
conversation about how much weight the downsides really have and how you might
be able to reduce at least some of their impact. And, again to be clear: The
outcome of that might very well be, that your reasons are completely valid,
rational and that, applied to your personal situation, veganism wouldn't be a
good choice. (And to be also clear: I might not be in the mood to have *this*
conversation either. But it's much preferable).

So, what I wish people to take away from this is

1. Stop asking why you should be vegan (or why I am), you more than likely
   already know. If you are really interested in making an informed choice,
   bring up your concerns instead, but also accept if I don't want to talk
   about them at that particular time - it's a lot of emotional labor, to give
   the same explanations repeatedly. It might not seem like a big deal to me,
   to ask these questions, but I've repeated most of my answers literally
   hundreds of times at this point and might prefer to just enjoy my food.
2. Stop treating veganism as a *preference* and start treating it as a *moral
   choice*. There is a qualitative difference between someone who does not like
   Italian food and a vegan. This is interesting when choosing what or where to
   eat as a group: This is hard enough as it is and I at least usually try very
   hard to accommodate everyone and not be a burden. And I absolutely do not
   expect to be treated like I'm better for that. But if it actually would come
   down to a choice between a vegetarian restaurant or a steakhouse, just
   because you really like meat: Yes, I do absolutely expect us to avoid the
   steakhouse. (To be clear: In my experience, it rarely *actually* comes down
   to only those two choices. And there are good reasons to avoid vegetarian
   restaurants that are not based on preference which should be given ample
   weight too - e.g. someone I know has Coeliac disease, fructose intolerance
   and lactose intolerance and tends to have a very hard time eating non-meat
   things. In my experience, though, people who have needs and not just
   preferences tend to ironically be more open to compromise anyway, so it is
   less often a problem with them).
3. Maybe think about your reasons for not being vegan and evaluate them
   seriously. To be clear, this is a stretch-goal and not the actual point of
   this article.

And if you want to, you can watch someone who does eat meat say *essentially*
the same things here:

<div class="video-container">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/uwKrtNr76BM" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

Thanks for reading, don't @ me. ;)

---

#### Reasons I'm not *not* vegan

Now, the main point of this post is dedicated to the general question of "how I
think about the topic and how I believe you should think about it too". But I
also want it to serve as a reference of my personal, specific thoughts driving
my decision - so if you don't know me well or are not interested in my personal
reasons, this would be a good place to close the tab and do something else.

I'm still writing this, because I hope this can be the last thing I ever have
to write about this (yeah, lol). Because again, I don't actually *like*
discussing it, as unbelievable as that may seem. So here is, as a reference,
why *I* am vegan (and I might add to/change this list over time, when my
viewpoints evolve). Why, after ten-ish years of thinking "I *should* be vegan,
but…", I decided to make the switch - or at least give it a serious try. So,
this is a list of reasons I gave to myself to justify not going vegan and why
they stopped being convincing *to me*. Your mileage may vary.

##### Meat/Cheese/Eggs/Bailey's is awesome and I can't imagine giving it up.

For most of my life I didn't think about vegetarianism or veganism at all.
Eating meat was the default, so that's what I did. When I did start to think
about it, I convinced myself that I couldn't give up meat, because most of
my favorite foods where meat-based. However, a lot of people in my
peer-group around that time (university) where vegetarian or vegan, so I
naturally got into contact with a lot of good food that *wasn't* naturally
meat-based. So I started eating less and less meat - and the less meat I
ate, the more I realized I didn't really *miss* it that much, given how much
good alternatives there are. Eventually I decided to become a "flexitarian",
which very quickly (in ~1-2 months) became "vegetarian", when I realized
that it didn't actually bother me to not eat meat at all - and making that
commitment meant less decisions, so it made things easier. With
Cheese/Eggs/Bailey's, I basically went through exactly the same transition,
six or seven years later: "I can't imagine giving them up" - "Actually,
there are really good alternatives" - "Let's just try reducing it and see
how far I get" - "Meh, might as well just commit completely".

So, to me, giving up animal products just turned out much easier, than
expected, when I actually tried. And I'm not saying I don't *miss* them,
every once in a while, I will still look longingly at a steak or think
fondly of my scrambled eggs. But for the most part, the alternatives are
just as great (or at times better), so it isn't *as* big a sacrifice as
expected.

##### Being vegetarian/vegan is unhealthy.

There is a bunch of research about this and for a while (especially before
actually looking into the details) the health implications of veganism
(vegetarianism not so much) did concern me. But, it turns out, this topic is
pretty complicated. Nutrition research is *very hard* - and that manifests
in the fact that for most of it, the statistical significance is usually low
and the effect sizes usually small. Now, I'm not *denying*, that there are
health downsides to a vegan diet. But even *with* the general mess that
nutritional research is, it doesn't seem very controversial that if you are
concerned for your health, there are much more important factors to
consider. If weighed against the health benefits of sleeping more, doing
more sports, not sitting all day, stop recreational drug use, taking
extensive vacations… (neither of which I seem to be willing to do, even
though they would be easy enough), the relatively minor health effects of
eating animal products (contrasted with a somewhat balanced vegan diet plus
supplementation) just did not seem to be a relevant driving force for that
decision and more of a rationalization.

That being said, from what I can gather so far, there is a general consensus
that if a) you pay attention to having a somewhat balanced diet and b) are
willing to supplement the nutrients you *can't* actually get, the health
impact of veganism is pretty low, if any. Personally, I am supplementing
Vitamins B12 and D right now, which has very low quality of life impact - so
I don't consider that a significant downside. I also pay a little bit more
attention to what I'm eating, which I consider a good thing.

If it turns out that I can not sustain a vegan diet, I will reconsider it,
but for now, I don't see any realistic danger of that happening.

##### It is cumbersome to know whether or not something is vegan.

This is mostly true. As a vegetarian, this mostly revolved around looking
for gelatin in the ingredients of sweets and asking in a restaurant whether
"Lasagna" is made with meat or not. Being a vegan does involve a *lot* of
scanning ingredients-lists of basically every product I buy. Though I'm
positively surprised how many vendors are recently starting to choose to get
their products [certified](https://www.v-label.eu/) - and not only brands
you would expect to focus on that, but also, increasingly, all kinds of
mainstream products.

That being said, there *is* an availability issue (especially around "may
contain traces of…", which is basically saying "if you are allergic, we
can't rule out cross-contamination of other things from the same factory").
I tend to be pragmatic about this: If I have the choice, I will buy the
certifiably vegan option, otherwise I'm also fine with traces of animal
products, personally. If I don't know, I will go with my best guess and how
I feel in the moment.

This is definitely the most true and heavy argument still on the contra-side
for me, but being kind of pragmatic about it helps alleviate most of the
pain.

##### It's hypocritical to draw the line at X and not at Y.

You can always be more rigorous and there are a lot of line-drawing
questions that come up when thinking about vegetarianism/veganism. For the
record, a lot of that is just plain nonsense, but there are some
legitimate questions to be asked around whether or not insects count, for
example, or certain shellfish, whether you would eat meat if it would be
thrown away otherwise or would eat an egg, if the Hen laying it was living a
happy, free life. In the end, the vast majority of things you can eat will
involve *some* harm to the environment or animals and you won't always know,
so where do you draw the line?

Personally, I decided that *definite* harm is worse than *potential* harm
and *more* harm is worse than *less* harm. "It is hypocritical to not eat
meat/cheese/eggs but still kill a wasp entering your apartment" is about as
convincing an argument to me as "it is hypocritical to eat meat/cheese/eggs
but not also eat dogs/jellyfish/human". The world isn't black-and-white and
it's fine to choose a gray spot in the middle that makes you comfortable.

##### Eating out becomes a PITA.

Yes. Going out and eating in a group is a PITA. Honestly, there are no two
ways about it. I do have to actively make sure that a chosen food place has
options for me and more often than not that does involve making special
requests and/or making do with less great meals.

In general, this still works reasonably well. The cafeteria at work has
*great* vegan options most of the time, Zurich has an amazing choice of
great restaurants for vegans to offer, most other restaurants can accommodate
too and even if not, I'm fine just eating a little thing and then later eat
some more at home.

The main problem is working around the social issues associated with it -
dealing with people who are unwilling to be accommodating, having to
justify/discuss my choice or just exposing something about my person I might
prefer to keep private. Basically, I wrote
[a whole thing about this](/2017/10/20/a-day-in-the-life-of-an-omnivore.html).

But this is simply one of those downsides I chose to accept. Nobody said
going vegan wouldn't come with sacrifices.

##### Being vegan is expensive

I am not sure this is true *in general*. I am relatively sure, that being
vegetarian at least actually ended up *saving* me money as a student. But I
can't be completely certain, as the change also came with other changes in
circumstances. My vegan diet is *probably* more expensive than my vegetarian
one, mainly because it includes a lot more processed substitute products
("faux meat" and various plant milks, which are at least in Switzerland
still significantly more expensive than the cow-based variants), but again,
I didn't actually run any numbers.

I'm pretty sure it's possible to have an affordable vegan diet, especially
if limiting processed substitute products and not eating out so often.
Luckily, this isn't really a concern for me, right now, though. Food and
Groceries is a *relatively* small proportion of my monthly expenses and as
such, the impact this has on me is pretty limited either way.

I convinced myself, that if I can afford spending money on all kinds of
luxury items and electronic gadgets, I can probably afford spending a little
more on food.
]]></content:encoded>
      <dc:date>2018-01-02T00:23:00+00:00</dc:date>
    </item>
    <item>
      <title>A day in the life of an Omnivore</title>
      <link>https://blog.merovius.de//2017/10/20/a-day-in-the-life-of-an-omnivore.html</link>
      <description><![CDATA[You get an E-Mail. It&#39;s an invite to a team dinner. As you have only recently
joined this team, it&#39;s going to be your first. How exciting! You look forward
to get to know your new coworkers better and socialize outside the office. They
are nice and friendly people and you are sure it&#39;s going to be a great evening.
However, you also have a distinct feeling of anxiety and dread in you. Because
you know, the first dinner with new people also means you are going to have
the conversation again. You know it will come up, whether you want to or not.
Because you had it a thousand times - because you are an Omnivore.
]]></description>
      <pubDate>Fri, 20 Oct 2017 23:45:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/10/20/a-day-in-the-life-of-an-omnivore.html</guid>
      <content:encoded><![CDATA[You get an E-Mail. It's an invite to a team dinner. As you have only recently
joined this team, it's going to be your first. How exciting! You look forward
to get to know your new coworkers better and socialize outside the office. They
are nice and friendly people and you are sure it's going to be a great evening.
However, you also have a distinct feeling of anxiety and dread in you. Because
you know, the first dinner with new people also means you are going to have
the conversation again. You know it will come up, whether you want to or not.
Because you had it a thousand times - because you are an Omnivore.

You quickly google the place your manager suggested. "Green's Organic
Foodery".  They don't have a menu on their site, but the name alone makes
clear, that meat probably isn't their specialty, exactly. You consider asking
for a change of restaurant, but quickly decide that you don't want to get a
reputation as a killjoy who forces their habits on everyone just yet. You
figure they are going to have *something* with meat on their menu. And if
not, you can always just grab a burger on your way home or throw a steak into
a pan when you get back.  You copy the event to your calendar and continue
working.

At six, everyone gathers to go to dinner together. It's not far, so you decide
to just walk. On the way you get to talk to some of your team mates. You talk
about Skiing, your home countries, your previous companies. You are having fun -
they seem to be easy-going people, you are getting along well. It's going to be
an enjoyable night.

You arrive at the restaurant and get seated. The waiter arrives with the menu
and picks up your orders for drinks. When they leave, everyone starts flipping
through the menu. “You've got to try their Tofu stir-fry. It's amazing”, Kevin
tells you. You nod and smile and turn your attention to the booklet in your
hand. You quickly take in the symbols decorating some of the items. “G” - safe
to assume, these are the gluten-free options. There's also an “O” on a bunch of
them. Also familiar, but ambiguous - could be either “Omnivores" or “Ovo-lacto”
(containing at least dairy products or eggs), you've seen both usages. There is
no legend to help disambiguate and quickly flipping through the rest of the
menu, you find no other symbols. Ovo-lacto, then. You are going to have to
guess from the names and short descriptions alone, whether they also contain any
meat. They have lasagna, marked with an “O”. Of course that's probably just the
cheese but they *might* make it with actual minced beef.

The waiter returns and takes your orders. “The lasagna - what kind is it?”, you
ask. You are trying to avoid the O-word as long as you can. “It's Lasagna alla
Bolognese, house style”. Uh-oh. House style? “Is it made from cattle, or based
on gluten proteins?” (you don't care how awkward you have to phrase your
question, you are *not* saying the magic trigger words!) “Uhm… I'm not sure. I
can ask in the kitchen, if you'd like?” “That would be great, thanks”. They
leave. Jen, from across the table, smiles at you - are you imagining it, or
does it look slightly awkward? You know the next question. “Are you an
Omnivore?” “I eat meat, yes”, you say, smiling politely. Frick. Just *one*
meal, is all you wanted. But it had to come up at some point anyway, so fine.
“Wow. I'm Ovo-lacto myself. But I couldn't always eat meat, I think. Is it hard?”
You notice that your respective seat neighbors have started to listen too.
Ovo-lactos aren't a rarity anymore (Onmivores a bit more so, but not *that*
much), but the topic still seems interesting enough to catch attention. You've
seen it before. What feels like a hundred thousand times. In fact, you have
said exactly the same thing Jen just said, just a year or so ago. Before you
just decided to go Omnivore.

“Not really”, you start. “I found it not much harder than when I went
Ovo-lacto. You have to get used to it, of course, and pay a little more
attention, but usually you find something. Just like when you start eating
cheese and eggs.” At that moment, the waiter returns. “I spoke to the chef and
we can make the lasagna with beef, if you like”. “Yes please”, you hand the
menu back to them with a smile. “I considered going Ovo-lacto”, Mike continues
the conversation from the seat next to Jen, “but for now I just try to have
some milk every once in a while. Like, in my coffee or cereal. It's not that I
don't *like* it, there are really great dairy products. For example, this place
in the city center makes this *amazing* yogurt. But having it every day just
seems very hard“. “Sure”, you simply say. You know they mean well and you don't
want to offend them by not being interested; but you also heard these exact
literal words at least two dozen times. And always with ample evidence in the
room that it's not actually *that* hard.

“I don't really see the point”, Roy interjects from the end of the table. You
make a mental check-mark. “Shouldn't people just eat what they want? I don't
get why suddenly we all have to like meat”. It doesn't matter that no one
suggested that. “I mean, I do think it's cool if people eat meat”, someone
whose name you don't remember adds, “I sometimes think I should eat more eggs
myself. But it's just so annoying that you get these Omnivores who try to
lecture you about how unhealthy it is to not eat meat or that humans are
naturally predisposed to digest meat. I mean, you seem really relaxed about it”,
they quickly add as assurance in your direction, “but you can't deny that there
are also these Omni-nazis”. You sip on your water, mentally counting backwards
from 3. “You know how you find an Omnivore at a party?”, Kevin asks jokingly
from your right. “Don't worry, they will tell you”, Rick delivers the punchline
for him. How original.

”I totally get Omnivores. If they want to eat meat, that's great. What I don't
understand is this weird trend of fake-salad. Like, people get a salad, but
then they put french dressing on it, or bacon bits. I mean, if you want a
salad, why not just simply have a salad?”. You know the stupidly obvious answer
of course and you haven't talked in a while, so you decide to steer the
conversation into a more pleasant direction. “It's simple, really. You like
salad, right?” “Yeah, of course“ “So, if you like salad, but decide that you
also want to eat dairy or meat - doesn't it make sense to get as close to a
pure salad as you can? While still staying with your conviction to eat meat?
It's a tradeoff, sure, but isn't it better than *no* salad at all?” There's a
brief pause. You can tell that they haven't considered that before. No one has.
Which you find baffling. Every single time. “Hm. I guess I haven't thought
about it like that before. From that point of view it does kind of make sense.
Anyway, I still prefer the real deal”. “That's fine”, you say, smiling “I will
continue to eat my salad with french dressing”.

Your food arrives and the conversation continues for a bit, with the usual
follow-up questions - do you eat pork too, or just beef? What about dogs? Would
you consider eating monkey meat? Or Human? You explain that you don't worry
about the exact line, that you are not dogmatic about it and usually just
decide based on convenience and what seems obvious (and in luckily, these
questions don't usually need an answer in practice anyway). Someone brings up
how some of what's labeled as milk actually is made from almonds, because it's
cheaper, so you can't even be sure you actually get dairy. But slowly, person
by person, the topic shifts back to work, hobbies and family. “How's the
lasagna?”, Jen asks. “Great”, you reply with a smile, because it is.

On your way home, you take stock. Overall, the evening went pretty well. You
got along great with most of your coworkers and had long, fun conversations.
The food ended up delicious, even if you wish they had just properly labeled
their menu. You probably are going to have to nudge your team on future
occasions, so you go out to Omnivore-friendlier places. But you are also pretty
sure they are open to it. Who knows, you might even get them to go to a steak
house at some point. You know you are inevitably going to have the conversation
again, at some point - whether it will come up at another meal with your team
or with a new person, who you eat with for the first time. This time, at least,
it went reasonably well.

---

*This post is a work of fiction. ;) Names, characters, places and incidents
either are products of the author's imagination or are used fictitiously. Any
resemblance to actual events or locales or persons, living or dead, is entirely
coincidental.*

*Also, if we had "the conversation" before, you should know I still love you and
don't judge you :) It's just that I had it a thousand times :)*
]]></content:encoded>
      <dc:date>2017-10-20T23:45:00+00:00</dc:date>
    </item>
    <item>
      <title>Diminishing returns of static typing</title>
      <link>https://blog.merovius.de//2017/09/12/diminishing-returns-of-static-typing.html</link>
      <description><![CDATA[I often get into discussions with people, where the matter of strictness and
expressiveness of a static type system comes up. The most common one, by far,
is Go&#39;s lack of generics and the resulting necessity to use interface{} in
container types (the container-subpackages are
obvious cases, but also context). When I express
my view, that the lack of static type-safety for containers isn&#39;t a problem, I
am treated with condescending reactions ranging from disbelief to patronizing.
]]></description>
      <pubDate>Tue, 12 Sep 2017 11:05:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/09/12/diminishing-returns-of-static-typing.html</guid>
      <content:encoded><![CDATA[I often get into discussions with people, where the matter of strictness and
expressiveness of a static type system comes up. The most common one, by far,
is Go's lack of generics and the resulting necessity to use `interface{}` in
container types (the [container-subpackages](https://godoc.org/container) are
obvious cases, but also [context](https://godoc.org/context)). When I express
my view, that the lack of static type-safety for containers isn't a problem, I
am treated with condescending reactions ranging from disbelief to patronizing.

I also often take the *other* side of the argument. This happens commonly, when
talking to proponents of dynamically typed languages. In particular I got into
debates of whether Python would be suitable for a certain use-case. When the
lack of static type-safety is brought up, the proponents of Python defend it by
pointing out that it now features optional type hints. Which they say make it
possible, to reap the benefits of static typing even in a conventionally
dynamically typed language.

This is an attempt to write my thoughts on both of these (though they are not
in any way novel or creative) down more thoroughly. Discussions usually don't
provide the space for that. They are also often charged and parties are more
interested in “winning the argument”, than finding consensus.

---

I don't think it's particularly controversial, that static typing in general
has advantages, even though actual data about those seems to be [surprisingly
hard to come by](https://danluu.com/empirical-pl/). *I* certainly believe that,
it is why I use Go in the first place. There is a difference of opinion though,
in how large and important those benefits are and how much of the behavior of a
program must be statically checked to reap those benefits.

To understand this, we should first make explicit *what* the benefits of static
type checking are. The most commonly mentioned one is to catch bugs as early in
the development process as possible. If a piece of code I write already
contains a rigorous proof of correctness in the form of types, just writing it
down and compiling it gives me assurance that it will work as intended in all
circumstances. At the other end of the spectrum, in a fully dynamic language I
will need to write tests exercising all of my code to find bugs. Running tests
takes time. Writing *good* tests that actually cover all intended behavior is
hard. And as it's in general impossible to cover *all* possible execution
paths, there will always be the possibility of a rare edge-case that we didn't
think of testing to trigger a bug in production.

So, we can think of static typing as increasing the proportion of bug-free
lines of code deployed to production. This is of course a simplification. In
practice, we would still catch a lot of the bugs via more rigorous testing,
QA, canarying and other practices. To a degree we can still subsume these in
this simplification though. If we catch a buggy line of code in QA or the
canary phase, we are going to roll it back. So in a sense, the proportion of
code we wrote that makes it as bug-free into production will still go down.
Thus:

<img class="small" src="/assets/static_typing_v_good_code.png">

This is usually the understanding, that the “more static typing is always
better” argument is based on. Checking more behavior at compile time means less
bugs in production means more satisfied customers and less being woken up at
night by your pager. Everybody's happy.

Why then is it, that we don't all code in Idris, Agda or a similarly strict
language? Sure, the graph above is suggestively drawn to taper off, but it's
still monotonically increasing. You'd think that this implies more is better.
The answer, of course, is that static typing has a cost and that there is no
free lunch.

The costs of static typing again come in many forms. It requires more upfront
investment in thinking about the correct types. It increases compile times and
thus the change-compile-test-repeat cycle. It makes for a steeper learning
curve. And more often than we like to admit, the error messages a compiler will
give us will decline in usefulness as the power of a type system increases.
Again, we can oversimplify and subsume these effects in saying that it reduces
our speed:

<img class="small" src="/assets/static_typing_v_speed.png">

This is what we mean when we talk about dynamically typed languages being good
for rapid prototyping. In the end, however, what we are usually interested in,
is what I'd like to call *velocity*: The speed with which we can deploy new
features to our users. We can model that as the speed with which we can roll
out bug-free code.  Graphically, that is expressed as the product of the
previous two graphs:

<img class="small" src="/assets/static_typing_v_velocity.png">

In practice, the product of these two functions will have a maximum, a sweet
spot of maximum velocity. Designing a type system for a programming language
is, at least in part, about finding that sweet spot[¹](#footnote1)<a
id="footnote1_back"></a>.

Now if we are to accept all of this, that opens up a different question: If we
are indeed searching for that sweet spot, how do we explain the vast
differences in strength of type systems that we use in practice? The answer of
course is simple (and I'm sure many of you have already typed it up in an angry
response). The curves I drew above are completely made up. Given how hard it is
to do empirical research in this space and to actually quantify the measures I
used here, it stands to reason that their shape is very much up for
interpretation.

A Python developer might very reasonably believe that optional type-annotations
are more than enough to achieve most if not all the advantages of static
typing. While a Haskell developer might be much better adapted to static typing
and not be slowed down by it as much (or even at all). As a result, the
perceived sweet spot can vary widely:

<img src="/assets/static_typing_pythonista_v_haskeller.png">

What's more, the importance of these factors might vary a lot too. If you are
writing avionics code or are programming the control unit for a space craft,
you probably want to be pretty darn sure that the code you are deploying is
correct. On the other hand, if you are a Silicon Valley startup in your
growth-phase, user acquisition will be of a very high priority and you get
users by deploying features quicker than your competitors. We can model that,
by weighing the factors differently:

<img src="/assets/static_typing_startup_v_nasa.png">

Your use case will determine the sweet spot you are looking for and thus the
language you will choose. But a language is also designed with a set of use
cases in mind and will set its own sweet spot according to that.

I think when we talk about how strict a type system should be, we need to
acknowledge these subjective factors. And it is fine to believe that your
perception of one of those curves or how they should be weighted is closer to
a hypothetical objective reality than another persons. But you should make that
belief explicit and provide a justification of *why* your perception is more
realistic. Don't just assume that other people view them the same way and then
be confused that they do not come to the same conclusions as you.

---

Back to Go's type system. In my opinion, Go manages to hit a good sweet spot
(that is, its design agrees with my personal preferences on this). To me it
seems that Go reaps probably upwards of 90% of the benefits you can get from
static typing while still being not too impeding. And while I definitely agree
static typing is beneficial, the *marginal* benefit of making user-defined
containers type-safe simply seems pretty low (even if it's positive). In the
end, it would probably be less than 1% of Go code that would get this additional
type-checking and it is probably pretty obvious code. And meanwhile, I perceive
generics as a language feature pretty costly. So I find it hard to justify a
large perceived cost with a small perceived benefit.

Now, that is not to say I'm not open to be convinced. Just that simply saying
“but more type-safety!” is only looking at one side of the equation and isn't
enough. You need to acknowledge that there is no free lunch and that this is a
tradeoff. You need to accept that your perceptions of how big the benefit of
adding static typing is, how much it costs and how important it is are all
subjective. If you want to convince me that my perception of their benefit is
wrong, the best way would be to provide specific instances of bugs or
production crashes caused by a type-assertion on an `interface{}` taken out of
a container. Or a refactoring you couldn't make because of the lack of
type-safety with a specific container. Ideally, this takes the form of an
[experience report](https://github.com/golang/go/wiki/ExperienceReports), which
I consider an excellent way to talk about engineered tradeoffs.

Of course you can continue to roll your eyes whenever someone questions your
perception of the value-curve of static typing. Or pretend that when I say the
*marginal* benefit of type-safe containers is small, I am implying that the
*total* benefit of static typing is small. It's an effective debate-tactic, if
your goal is to shut up your opposition. But not if your goal is to convince
them and build consensus.

---

<a id="footnote1"></a>[1] There is a generous and broad exception for research
languages here. If the point of your design is to explore the possibility space
of type-systems, matters of practicality can of course often be ignored. [⬆](#footnote1_back)
]]></content:encoded>
      <dc:date>2017-09-12T11:05:00+00:00</dc:date>
    </item>
    <item>
      <title>Gendered Marbles</title>
      <link>https://blog.merovius.de//2017/09/05/gendered-marbles.html</link>
      <description><![CDATA[tl;dr: &quot;Some marbles, apparently, have a gender. And they seem to be overwhelmingly male.&quot;
]]></description>
      <pubDate>Tue, 05 Sep 2017 23:22:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/09/05/gendered-marbles.html</guid>
      <content:encoded><![CDATA[**tl;dr: "Some marbles, apparently, have a gender. And they seem to be overwhelmingly male."**

A couple of days ago [The MarbleLympics 2017](https://www.youtube.com/playlist?list=PL-RXmnw758NH0bj5KIbpiDrLygvT7k9un)
popped into my twitter stream. In case you are unaware (I certainly was): It's
a series of videos where a bunch of marbles participate in a made-up Olympics.
They are split into teams that then participate in a series of "competitions"
in a variety of different events. The whole event is professionally filmed, cut
and overlaid both with fake noises from spectators and a well-made, engaging
sports commentary. It is *really* fun to watch. I don't know why, but I find it
*way* more captivating than watching actual sports. I thoroughly recommend it.

Around event 8 (high jump) though, I suddenly noticed that the commentator
would *occasionally* not only personify but actually gender marbles.  For most
of the commentary he just refers to the teams as a whole with a generic "they".
But every once in a while - and especially often during the high-jump event -
he would use a singular gendered pronoun. Also, that only really occurred to me
when he referred to one of the marbles as "she".

This instantly became one of those things that after noticing it, I
couldn't *unnotice* it. It's not so much that it *matters*. But from then on, I
couldn't stop listening up every time a singular pronoun was used.

Well, you know where this is going. Fully aware of how much of a waste of my
time this is, I sat down and counted. More specifically, I downloaded the
closed captions of all the videos and grepped through them for pronouns. I
*did* double-check all findings and here is what I found: By my count, 20
distinct marbles are referred to by singular pronouns (yes. I noted their names
to filter duplicates. Also I kind of hoped to find a genderfluid marble to be
honest). Here is an alphabetized list of gendered marbles:

* Aqua (Oceanics) - Male
* Argent (Quicksilvers) - Male
* Clementin (O'Rangers) - Male
* Cocoa (Chocolatiers) - Male (in two events)
* Cosmo (Team Galactic) - Male
* Imar (Primaries) - Male
* Jump (Jungle Jumpers) - Male
* Leap (Jungle Jumpers) - Male
* Mandarin (O'Rangers) - Male (in two events)
* Mary (Team Primary) - Female
* Mercurial (Quicksilvers) - Male
* Mimo (Team Momo) - Male
* Momo Momo (Team Momo) - Male (in three events)
* Pinky Winky (Pinkies) - Male
* Rapidly (Savage Speeders) - Male
* Taffy (Jawbreakers) - Male
* Wespy (Midnight Wisps) - Male
* Whizzy (Savage Speeders) - Male
* Yellah (Mellow Yellow) - Male
* Yellup (Mellow Yellow) - Male

As you can see, the overwhelming majority of gendered marbles are men. There is
exactly one exception: Mary. From what I can tell, that's because it's the
only name that has clear gender associations. All the other names probably
could go either way. And marbles obviously *have* no gender. They are as
non-gendered an object as you could imagine. And yet there seems to be a
default assumption that athletic marbles would be men.

Obviously this doesn't matter. Obviously you can't discriminate marbles.
You can't misgender them or hurt their feelings. Obviously the commentator
didn't sit down and made a list of all the marbles and assigned 95% of them a
male gender - it's clearly just an ad-hoc subconscious assignment. And to be
absolutely clear: I do not try to fault the makers of these videos *at all*.
They did nothing wrong. It's a ludicrous expectation for them to sit down and
make sure that they assign balanced genders to their marbles.

But I do find it an interesting observation. I do think it reflects an
implicit, unconscious bias in a striking way. I also think it illustrates
nicely that gender bias in language isn't exclusive to languages like German,
where all nouns are gendered (take note, German friends). Of course none of
this is news. This kind of unconscious gender bias in language is
well-researched and documented. It's just that once you know about it, you
can't stop noticing the evidence for it popping up *everywhere*. Even with
marbles.

And all of that being said: Yes, I am also aware that all of this is slightly
ridiculous.

---

PS: In case the team behind the MarbleLympics are reading this: Really, thank
you for the videos :) They are great.
]]></content:encoded>
      <dc:date>2017-09-05T23:22:00+00:00</dc:date>
    </item>
    <item>
      <title>Why context.Value matters and how to improve it</title>
      <link>https://blog.merovius.de//2017/08/14/why-context-value-matters-and-how-to-improve-it.html</link>
      <description><![CDATA[tl;dr: I think context.Value solves the important use case of writing
stateless - and thus scalable - abstractions. I believe
dynamic scoping
could provide the same benefits while solving most of the criticism of the
current implementation. I thus try to steer the discussion away from the
concrete implementation and towards the underlying problem.
]]></description>
      <pubDate>Mon, 14 Aug 2017 00:17:25 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/08/14/why-context-value-matters-and-how-to-improve-it.html</guid>
      <content:encoded><![CDATA[**tl;dr: I think context.Value solves the important use case of writing
stateless - and thus scalable - abstractions. I believe
[dynamic scoping](https://en.wikipedia.org/wiki/Scope_(computer_science)#Dynamic_scoping)
could provide the same benefits while solving most of the criticism of the
current implementation. I thus try to steer the discussion away from the
concrete implementation and towards the underlying problem.**

*This blog post is relatively long. I encourage you to skip sections you find boring*

---

Lately [this
blogpost](https://faiface.github.io/post/context-should-go-away-go2/) has been
discussed in several Go forums. It brings up several good arguments against the
[context-package](http://godoc.org/context):

* It requires every intermediate functions to include a `context.Context` even
  if they themselves do not use it. This introduces clutter into APIs and
  requires extensive plumbing.  Additionally, `ctx context.Context` "stutters".
* `context.Value` is not statically type-safe, requiring type-assertions.
* It does not allow you to express critical dependencies on context-contents
  statically.
* It's susceptible to name collisions due to requiring a global namespace.
* It's a map implemented as a linked list and thus inefficient.

However, I don't think the post is doing a good enough job to discuss the
problems context was designed to *solve*. It explicitly focuses on cancellation.
`Context.Value` is discarded by simply stating that

> […] designing your APIs without ctx.Value in mind at all makes it always
> possible to come up with alternatives.

I think this is not doing this question justice. To have a reasoned argument
about context.Value there need to be consideration for both sides involved.
No matter what your opinion on the current API is: The fact that seasoned,
intelligent engineers felt the need - after significant thought - for
`Context.Value` should already imply that the question deserves more attention.

I'm going to try to describe my view on what kind of problems the context
package tries to address, what alternatives currently exist and why I find them
insufficient and I'm trying to describe an alternative design for a future
evolution of the language. It would solve the same problems while avoiding some
of the learned downsides of the context package. It is not meant as a specific
proposal for Go 2 (I consider that way premature at this point) but just to
show that a balanced view can show up alternatives in the design space and make
it easier to consider all options.

---

The problem context sets out to solve is one of abstracting a problem into
independently executing units handled by different parts of a system. And how
to scope data to one of these units in this scenario. It's hard to clearly
define the abstraction I am talking about. So I'm instead going to give some
examples.

* When you build a scalable web service you will probably have a stateless
  frontend server that does things like authentication, verification and
  parsing for you. This allows you to scale up the external interface
  effortlessly and thus also gracefully fall back if the load increases past
  what the backends can handle. By treating requests as independent from each
  other you can load-balance them freely between your frontends.
* [Microservices](https://en.wikipedia.org/wiki/Microservices) split a large
  application into small individual pieces that each process individual
  requests, each potentially branching out into more requests to other
  services. The requests will usually be independent, making it easy to scale
  individual microservices up and down based on demand, to load-balance between
  instances and to solve problems in [transparent proxies](https://istio.io/).
* [Functions as a Service](https://en.wikipedia.org/wiki/Serverless_computing)
  goes one step further: You write single stateless functions that transform
  data and the platform will make them scale and execute efficiently.
* Even [CSP](https://en.wikipedia.org/wiki/Communicating_sequential_processes),
  the concurrency model built into Go, can be viewed through that lens. The
  programmer expresses her problem as individually executing "processes" and
  the runtime will execute them efficiently.
* [Functional Programming](https://en.wikipedia.org/wiki/Functional_programming)
  as a paradigm calls this "purity". The concept that a functions result may
  only depend on its input parameters means not much more than the absence of
  shared state and independent execution.
* The design of a [Request Oriented Collector](https://docs.google.com/document/d/1gCsFxXamW8RRvOe5hECz98Ftk-tcRRJcDFANj2VwCB0/edit)
  for Go plays exactly into the same assumptions and ideas.

The idea in all these cases is to increase scaling (whether distributed among
machines, between threads or just in code) by reducing shared state while
maintaining shared usage of resources.

Go takes a measured approach to this. It doesn't go as far as some functional
programming languages to forbid or discourage mutable state. It allows sharing
memory between threads and synchronizing with mutexes instead of relying purely
on channels. But it also definitely tries to be a (if not *the*) language to
write modern, scalable services in. As such, it *needs* to be a good language
to write this kind of stateless services. It needs to be able to make
*requests* the level of isolation instead of the process. At least to a degree.

*(Side note: This seems to play into the statement of the author of above
article, who claims that context is mainly useful for server authors. I
disagree though. The general abstraction happens on many levels. E.g. a click
in a GUI counts just as much as a "request" for this abstraction as an HTTP
request)*

This brings with it the requirement of being able to store some data on a
request-level. A simple example for this would be authentication in an [RPC
framework](https://grpc.io/). Different requests will have different
capabilities. If a request originates from an administrator it should have
higher privileges than if it originates from an unauthenticated user. This is
fundamentally *request scoped* data. Not process, service or application
scoped. And the RPC framework should treat this data as opaque. It is
application specific not only how that data looks en détail but also *what
kinds* of data it requires.

Just like an HTTP proxy or framework should not need to know about request
parameters or headers it doesn't consume, an RPC framework shouldn't know about
request scoped data the application needs.

---

Let's try to look at specific ways this problem is (or could be) solved without
involving context. As an example, let's look at the problem of writing an HTTP
middleware. We want to be able to wrap an
[http.Handler](http://godoc.org/net/http#Handler) (or a variation thereof) in a
way that allows the wrapper to attach data to a request.

To get static type-safety we could try to add some type to our handlers. We
could have a type containing all the data we want to keep request scoped and
pass that through our handlers:

```go
type Data struct {
	Username string
	Log *log.Logger
	// …
}

func HandleA(d Data, res http.ResponseWriter, req *http.Request) {
	// …
	d.Username = "admin"
	HandleB(d, req, res)
	// …
}

func HandleB(d Data, res http.ResponseWriter, req *http.Request) {
	// …
}
```

However, this would prevent us from writing reusable Middleware. Any such
middleware would need to make it possible to wrap `HandleA`. But as it's
supposed to be reusable, it can't know the type of the Data parameter. You
could make the `Data` parameter an `interface{}` and require type-assertion.
But that wouldn't allow the middleware to inject its own data. You might think
that interface type-assertions could solve this, but they have [their own set
of problems](https://blog.merovius.de/2017/07/30/the-trouble-with-optional-interfaces.html).
In the end, this approach won't bring you actual additional type safety.

We could store our state keyed by requests. For example, an authentication
middleware could do

```go
type Authenticator struct {
	mu sync.Mutex
	users map[*http.Request]string
	wrapped http.Handler
}

func (a *Authenticator) ServeHTTP(res http.ResponseWriter, req *http.Request) {
	// …
	a.mu.Lock()
	a.users[req] = "admin"
	a.mu.Unlock()
	defer func() {
		a.mu.Lock()
		delete(a.users, req)
		a.mu.Unlock()
	}()
	a.wrapped.ServeHTTP(res, req)
}

func (a *Authenticator) Username(req *http.Request) string {
	a.mu.Lock()
	defer a.mu.Unlock()
	return a.users[req]
}
```

This has *some* advantages over context:

* It is more type-safe.
* While we still can't express a requirement on an authenticated user
  statically, we *can* express a requirement on an `Authenticator`
* It's not susceptible to name-collisions anymore.

However, we bought this with shared mutable state and the associated lock
contention. It can also break in subtle ways, if one of the intermediate
handlers decides to create a new Request - as
[http.StripPrefix](https://github.com/golang/go/blob/816deacc70f48d14638104e284b3b75d5b1e8036/src/net/http/server.go#L1946)
is going to do soon.

Lastly, we might consider to store this data in the
[\*http.Request](http://godoc.org/net/http#Request) itself, for example by
adding it as a stringified [URL parameter](http://godoc.org/net/url#URL.RawQuery).
This too has several downsides, though. In fact it checks almost every single
item from our list of downsides of `context.Context`. The exception is being a
linked list. But even that advantage we buy with a lack of thread safety. If
that request is passed to a handler in a different goroutine we get into
trouble.

*(Side note: All of this also gives us a good idea of why the context package
is implemented as a linked list. It allows all the data stored in it to be
read-only and thus inherently thread-safe. There will never be any
lock-contention around the shared state saved in a context.Context, because
there will never be any need for locks)*

So we see that it is really hard (if not impossible) to solve this problem of
having data attached to requests in independently executing handlers while also
doing significantly better than with `context.Value`. Whether you believe this
a problem worth solving or not is debatable. But *if* you want to get this kind
of scalable abstraction you will have to rely on *something* like
`context.Value`.

---

No matter whether you are now convinced of the usefulness of `context.Value` or
still doubtful: The disadvantages can clearly not be ignored in either case.
But we can try to find a way to improve on it. To eliminate some of the
disadvantages while still keeping its useful attributes.

One way to do that (in Go 2) would be to introduce [dynamically scoped](https://en.wikipedia.org/wiki/Scope_(computer_science)#Dynamic_scoping)
variables. Semantically, each dynamically scoped variable represents a separate
stack. Every time you change its value the new one is pushed to the stack.  It
is pop'ed off again after your function returns. For example:

```go
// Let's make up syntax! Only a tiny bit, though.
dyn x = 23

func Foo() {
	fmt.Println("Foo:", x)
}

func Bar() {
	fmt.Println("Bar:", x)
	x = 42
	fmt.Println("Bar:", x)
	Baz()
	fmt.Println("Bar:", x)
}

func Baz() {
	fmt.Println("Baz:", x)
	x = 1337
	fmt.Println("Baz:", x)
}

func main() {
	fmt.Println("main:", x)
	Foo()
	Bar()
	Baz()
	fmt.Println("main:", x)
}

// Output:
main: 23
Foo: 23
Bar: 23
Bar: 42
Baz: 42
Baz: 1337
Bar: 42
Baz: 23
Baz: 1337
main: 23
```

There are several notes about what I would imagine the semantics to be here.

* I would only allow `dyn`-declarations at package scope. Given that there is
  no way to refer to a local identifier of a different function, that seems
  logical.
* A newly spawned goroutine would inherit the dynamic values of its parent
  function. If we implement them (like `context.Context`) via linked lists, the
  shared data will be read-only. The head-pointer would need to be stored in
  some kind of goroutine-local storage. Thus, writes only ever modify this
  local storage (and the global heap), so wouldn't need to be synchronized
  specifically.
* The dynamic scoping would be independent of the package the variable is
  declared in. That is, if `foo.A` modifies a dynamic `bar.X`, then that
  modification is visible to all subsequent callees of `foo.A`, whether they
  are in `bar` or not.
* Dynamically scoped variables would likely not be addressable. Otherwise we'd
  loose concurrency safety and the clear "down-stack" semantics of dynamic
  scoping. It would still be possible to declare `dyn x *int` though and thus
  get mutable state to pass on.
* The compiler would allocate the necessary storage for the stacks, initialized
  to their initializers and emit the necessary instructions to push and pop
  values on writes and returns. To account for panics and early returns, a
  mechanism like `defer` would be needed.
* There is some confusing overlap with package-scoped variables in this design.
  Most notably, from seeing `foo.X = Y` you wouldn't be able to tell whether
  `foo.X` is dynamically scoped or not. Personally, I would address that by
  removing package-scoped variables from the language. They could still be
  emulated by declaring a dynamically-scoped pointer and never modifying it.
  Its pointee is then a shared variable. But most usages of package-scoped
  variables would probably just use dynamically scoped variables.

It is instructive to compare this design against the list of disadvantages
identified for `context`.

* API clutter would be removed, as request-scoped data would now be
  part of the language without needing explicit passing.
* Dynamically scoped variables are statically type-safe. Every `dyn`
  declaration has an unambiguous type.
* It would still not be possible to express critical dependencies on
  dynamically scoped variables but they also couldn't be *absent*. At worst
  they'll have their zero value.
* Name collision is eliminated. Identifiers are, just like variable names,
  properly scoped.
* While a naive implementation would still use linked lists, they wouldn't be
  inefficient. Every `dyn` declaration gets its own list and only the
  head-pointer ever needs to be operated on.
* The design is still "magic" to a degree. But that "magic" is problem-inherent
  (at least if I understand the criticism correctly). The magic is exactly the
  possibility to pass values transparently through API boundaries.

Lastly, I'd like to mention cancellation. While the author of above post
dedicates most of it to cancellation, I have so far mostly ignored it.  That's
because I believe cancellation to be trivially implementable on top of a good
`context.Value` implementation. For example:

```go
// $GOROOT/src/done
package done

// C is closed when the current execution context (e.g. request) should be
// cancelled.
dyn C <-chan struct{}

// CancelFunc returns a channel that gets closed, when C is closed or cancel is
// called.
func CancelFunc() (c <-chan struct, cancel func()) {
	// Note: We can't modify C here, because it is dynamically scoped, which is
	// why we return a new channel that the caller should store.
	ch := make(chan struct)

	var o sync.Once
	cancel = func() { o.Do(close(ch)) }
	if C != nil {
		go func() {
			<-C
			cancel()
		}()
	}
	return ch, cancel
}

// $GOPATH/example.com/foo
package foo

func Foo() {
	var cancel func()
	done.C, cancel = done.CancelFunc()
	defer cancel()
	// Do things
}
```

This cancellation mechanism would now be usable from any library that wants it
without needing any explicit support in its API. This would also make it easy
to add cancellation capabilities retroactively.

---

Whether you *like* this design or not, it demonstrates that we shouldn't rush
to calling for the removal of `context`. Removing it is only one possible
solution to its downsides.

If the removal of `context.Context` actually comes up, the question we should
ask is "do we want a canonical way to manage request-scoped values and at what
cost".  Only then should we ask what the best implementation of this would be
or whether to remove the current one.
]]></content:encoded>
      <dc:date>2017-08-14T00:17:25+00:00</dc:date>
    </item>
    <item>
      <title>What I want from a logging API</title>
      <link>https://blog.merovius.de//2017/08/06/what-i-want-from-a-logging-api.html</link>
      <description><![CDATA[This is intended as an Experience
Report about logging in
Go. There are many like it but this one is mine.
]]></description>
      <pubDate>Sun, 06 Aug 2017 20:08:56 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/08/06/what-i-want-from-a-logging-api.html</guid>
      <content:encoded><![CDATA[*This is intended as an [Experience
Report](https://github.com/golang/go/wiki/ExperienceReports) about logging in
Go. There are many like it but this one is mine.*

I have been trying for a while now to find (or build) a logging API in Go that
fills my needs. There are several things that make this hard to get "right"
though. This is my attempt to describe them coherently in one place.

When I say "logging", I mean informational text messages for human consumption
used to debug a specific problem. There is an idea currently gaining traction
in the Go community called "structured logging".
[logrus](https://github.com/sirupsen/logrus) is a popular package that
implements this idea. If you haven't heard of it, you might want to skim its
README. And while I definitely agree that log-messages should
contain some structural information that is useful for later filtering (like
the current time or a request ID), I believe the idea as often advocated is
somewhat misguided and conflates different use cases that are better addressed
otherwise. For example, if you are tempted to add a structured field to your
log containing an HTTP response code to alert on too many errors, you probably
want to use [metrics and
timeseries](https://landing.google.com/sre/book/chapters/practical-alerting.html)
instead. If you want to follow a field through a variety of systems, you
probably want to [annotate a
trace](https://research.google.com/pubs/pub36356.html). If you want analytics
like calculating daily active users or what used user-agents are used how
often, you probably want what I like to call [request
annotations](https://research.google.com/pubs/pub36632.html), as these are
properties of a request, not of a log-line. If you exclude all these use cases,
there isn't a lot left for structured logging to address.

The logs I am talking about is to give a user or the operator of a software
more insight into what is going on under the covers. The default assumption is,
that they are not looked at until something goes wrong: Be it a test failing,
an alerting system notifying of an issue or a bug report being investigated or
a CLI not doing what the user expected. As such it is important that they are
verbose to a certain degree. As an operator, I don't want to find out that I
can't troubleshoot a problem because someone did not log a critical piece of
information. An API that requires (or encourages) me to only log structured
data will ultimately only discourage me from logging at all. In the end, some
form of `log.Debugf("Error reading foo: %v", err)` is the perfect API for my use
case. Any structured information needed to make this call practically useful
should be part of the setup phase of whatever `log` is.

The next somewhat contentious question is whether or not the API should support
log levels (and if so, which). My personal short answer is "yes and the log
levels should be Error, Info and Debug". I could try and justify these specific
choices but I don't think that really helps; chalk it up to personal
preference if you like. I believe having *some* variation on the
verbosity of logs is very important. A CLI should be quiet by default but be
able to tell the user more specifically where things went wrong on request. A
service should be debuggable in depth, but unconditionally logging verbosely
would have in unacceptable latency impact in production and too heavy storage
costs. There need to be *some* logs by default though, to get quick insights
during an emergency or in retrospect. So, those three levels seem fine to me.

Lastly what I need from a logging API, is the possibility to set up verbosity
and log sinks both horizontally *and* vertically. What I mean by that is that
software is usually build in layers. They could be individual microservices,
Go packages or types. Requests will then traverse these layers vertically,
possibly branching out and interleaved to various degrees.

![Request forest](/assets/request_forest.svg)

Depending on what and how I am debugging, it makes sense to increase the log
verbosity of a particular layer (say I narrowed down the problem to shared
state in a particular handler and want to see what happens to that state during
multiple requests) or for a particular request (say, I narrowed down a problem
to "requests which have header FOO set to BAR" and want to follow one of them
to get a detailed view of what it does). Same with logging sinks, for example,
a request initiated by a test should get logged to its `*testing.T` with
maximum verbosity, so that I get a detailed context about it if and only if the
test fails to immediately start debugging. These settings should be possible
during runtime without a restart. If I am debugging a production issue, I
don't want to change a command line flag and restart the service.

Let's try to implement such an API.

We can first narrow down the design space a bit, because we want to use
`testing.T` as a logging sink. A `T` has several methods that would suit our
needs well, most notably [Logf](http://godoc.org/testing#T.Logf). This suggest
an interface for logging sinks that looks somewhat like this:

```go
type Logger interface {
	Logf(format string, v ...interface{})
}

type simpleLogger struct {
	w io.Writer
}

func (l simpleLogger) Logf(format string, v ...interface{}) {
	fmt.Fprintf(l.w, format, v...)
}

func NewLogger(w io.Writer) Logger {
	return simpleLogger{w}
}
```

This has the additional advantage, that we can add easily implement a
Discard-sink, that has minimal overhead (not even the allocations of
formatting the message):

```go
type Discard struct{}

func (Discard) Logf(format string, v ...interface{}) {}
```

The next step is to get leveled logging. The easiest way to achieve this is
probably

```go
type Logs struct {
	Debug Logger
	Info Logger
	Error Logger
}

func DiscardAll() Logs {
	return Logs{
		Debug: Discard{},
		Info: Discard{},
		Error: Discard{},
	}
}
```

By putting a struct like this (or its constituent fields) as members of a
handler, type or package, we can get the horizontal configurability we are
interested in.

To get vertical configurability we can use
[context.Value](http://godoc.org/context#Context.Value) - as much as it's
frowned upon by some, it is the canonical way to get request-scoped
behavior/data in Go. So, let's add this to our API:

```go
type ctxLogs struct{}

func WithLogs(ctx context.Context, l Logs) context.Context {
	return context.WithValue(ctx, ctxLogs{}, l)
}

func GetLogs(ctx context.Context, def Logs) Logs {
	// If no Logs are in the context, we default to its zero-value,
	// by using the ,ok version of a type-assertion and throwing away
	// the ok.
	l, _ := ctx.Value(ctxLogs{}).(Logs)
	if l.Debug == nil {
		l.Debug = def.Debug
	}
	if l.Info == nil {
		l.Info = def.Info
	}
	if l.Error == nil {
		l.Error = def.Error
	}
	return l
}
```

So far, this is a sane, simple and easy to use logging API. For example:

```go
type App struct {
	L log.Logs
}

func (a *App) ServeHTTP(res http.ResponseWriter, req *http.Request) {
	l := log.GetLogs(req.Context(), a.L)
	l.Debug.Logf("%s %s", req.Method, req.URL.Path)
	// ...
}
```

The issue with this API, however, is that it is completely inflexible, if we
want to preserve useful information like the file and line number of the
caller. Say, I want to implement the equivalent of
[io.MultiWriter](http://godoc.org/io#MultiWriter). For example, I want to write
logs both to `os.Stderr` and to a file and to a network log service.

I might try to implement that via

```go
func MultiLogger(ls ...Logger) Logger {
	return multiLog{ls}
}

type multiLog struct {
	loggers []Logger
}

func (m *multiLog) Logf(format string, v ...interface{}) {
	for _, l := range m.loggers {
		m.Logf(format, v...)
	}
}
```

However, now the caller of `Logf` of the individual loggers will be the line in
`(*multiLog).Logf`, *not* the line of its caller. Thus, caller information will
be useless. There are two APIs currently existing in the stdlib to work around this:

1. [(testing.T).Helper](https://tip.golang.org/pkg/testing/#T.Helper) (from
   Go 1.9) lets you mark a frame as a test-helper. When the caller-information
   is then added to the log-output, all frames marked as a helper is skipped.
   So, theoretically, we could add a `Helper` method to our Logger interface
   and require that to be called in each wrapper. However, `Helper` *itself*
   uses the same caller-information. So all wrappers must call the `Helper`
   method of the *underlying `*testing.T`*, without any wrapping methods. Even
   embedding doesn't help, as the Go compiler creates an [implicit wrapper](https://play.golang.org/p/Z8MHOrGAAt)
   for that.
2. [(log.Logger).Output](http://godoc.org/log#Logger.Output) lets you
   specify a number of call-frames to skip. We could add a similar method to
   our log sink interface. And wrapping loggers would then need to increment
   the passed in number, when calling a wrapped sink. It's possible to do this,
   but it wouldn't help with test-logs.

This is a very similar problem to the ones I wrote about
[last week]({{site.url}}/2017/07/30/the-trouble-with-optional-interfaces.html).
For now, I am using the technique I described as [Extraction
Methods](https://blog.merovius.de//2017/07/30/the-trouble-with-optional-interfaces.html#extraction-methods).
That is, the modified API is now this:

```go
// Logger is a logging sink.
type Logger interface {
	// Logf logs a text message with the given format and values to the sink.
	Logf(format string, v ...interface{})

	// Helpers returns a list of Helpers to call into from all helper methods,
	// when wrapping this Logger. This is used to skip frames of logging
	// helpers when determining caller information.
	Helpers() []Helper
}

type Helper interface {
	// Helper marks the current frame as a helper method. It is then skipped
	// when determining caller information during logging.
	Helper()
}

// Callers can be used as a Helper for log sinks who want to log caller
// information. An empty Callers is valid and ready for use.
type Callers struct {
	// ...
}

// Helper marks the calling method as a helper. When using Callers in a
// Logger, you should also call this to mark your methods as helpers.
func (*Callers) Helper() {
	// ...
}

type Caller struct {
	Name string
	File string
	Line int
}

// Caller can be used to determine the caller of Logf in a Logger, skipping all
// frames marked via Helper.
func (*Callers) Caller() Caller {
	// ...
}

// TestingT is a subset of the methods of *testing.T, so that this package
// doesn't need to import testing.
type TestingT interface {
	Logf(format string, v ...interface{})
	Helper()
}

// Testing returns a Logger that logs to t. Log lines are discarded, if the
// test succeeds.
func Testing(t TestingT) Logger {
	return testLogger{t}
}

type testLogger struct {
	t TestingT
}

func (l testLogger) Logf(format string, v ...interface{}) {
	l.t.Helper()
	l.t.Logf(format, v...)
}

func (l testLogger) Helpers() []Helper {
	return []Helper{l.t}
}

// New returns a logger writing to w, prepending caller-information.
func New(w io.Writer) Logger {
	return simple{w, new(Callers)}
}

type simple struct {
	w io.Writer
	c *Callers
}

func (l *simple) Logf(format string, v ...interface{}) {
	l.c.Helper()
	c := l.c.Caller()
	fmt.Fprintf(l.w, "%s:%d: " + format, append([]interface{}{c.File, c.Line}, v...)...)
}

func (l *simple) Helpers() []Helper {
	return []Helper{l.c}
}

// Discard discards all logs.
func Discard() Logger {
	return discard{}
}

type discard struct{}

func (Discard) Logf(format string, v ...interface{}) {
}

func (Discard) Helpers() []Helper {
	return nil
}

// MultiLogger duplicates all Logf-calls to a list of loggers.
func MultiLogger(ls ...Logger) Logger {
	var m multiLogger
	for _, l := range ls {
		m.helpers = append(m.helpers, l.Helpers()...)
	}
	m.loggers = ls
	return m
}

type multiLogger struct {
	loggers []Logger
	helpers []Helper
}

func (m multiLogger) Logf(format string, v ...interface{}) {
	for _, h := range m.helpers {
		h.Helper()
	}
	for _, l := range m.loggers {
		l.Logf(format, v...)
	}
}

func (m multiLogger) Helpers() []Helper {
	return m.helpers
}

```

It's a kind of clunky API and I have no idea about the performance implications
of all the Helper-code.  But it *does* work, so it is, what I ended up with for
now.  Notably, it puts the implementation complexity into the *implementers* of
Logger, in favor of making the actual consumers of them as simple as possible.
]]></content:encoded>
      <dc:date>2017-08-06T20:08:56+00:00</dc:date>
    </item>
    <item>
      <title>The trouble with optional interfaces</title>
      <link>https://blog.merovius.de//2017/07/30/the-trouble-with-optional-interfaces.html</link>
      <description><![CDATA[tl;dr: I take a look at the pattern of optional interfaces in Go: what they
are used for, why they are bad and what we can do about it.
]]></description>
      <pubDate>Sun, 30 Jul 2017 18:39:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/07/30/the-trouble-with-optional-interfaces.html</guid>
      <content:encoded><![CDATA[**tl;dr: I take a look at the pattern of optional interfaces in Go: what they
are used for, why they are bad and what we can do about it.**

*Note: I wrote most of this article on Wednesday, with the intention to finish
and publish it on the weekend. While I was sleeping, Jack Lindamood published
[this
post](https://medium.com/@cep21/interface-wrapping-method-erasure-c523b3549912),
which talks about much of the same problems.
[This](https://twitter.com/TheMerovius/status/890472264931708928) was the exact
moment I saw that post :) I decided, to publish this anyway; it contains, in my
opinion, enough additional content, to be worth it. But I do encourage to
(also?) read his post.*

#### What are optional interfaces?

Optional interfaces are interfaces which can optionally be extended by
implementing some other interface. A good example is
[http.Flusher](http://godoc.org/net/http#Flusher) (and similar), which is
optionally implemented by an
[http.ResponseWriter](http://godoc.org/net/http#ResponseWriter). If a request
comes in via HTTP/2, the ResponseWriter will implement this interface to
support [HTTP/2 Server Push](https://en.wikipedia.org/wiki/HTTP/2_Server_Push).
But as not all requests will be over HTTP/2, this isn't part of the normal
ResponseWriter interface and instead provided via an optional interface that
needs to be type-asserted at runtime.

In general, whenever some piece of code is doing a type-assertion with an
interface type (that is, use an expression `v.(T)`, where `T` is an interface
type), it is very likely offering an optional interface.

A far from exhaustive list of where the optional interface pattern is used (to
roughly illustrate the scope of the pattern):

* [io](http://godoc.org/io#Copy)
* [net/http](http://godoc.org/net/http#ResponseWriter#Flusher)
* [database/sql/driver](http://godoc.org/database/sql/driver#ConnBeginTx)
* [go/types](http://godoc.org/go/types#Importer)
* Dave Chaney's [errors package](http://godoc.org/github.com/pkg/errors#Cause)

#### What are people using them for?

There are multiple reasons to use optional interfaces. Let's find examples for
them. Note that this list neither claims to be exhaustive (there are probably
use cases I don't know about) nor disjunct (in some cases, optional interfaces
will carry more than one of these use cases). But I think it's a good rough
partition to discuss.

##### Passing behavior through API boundaries

This is the case of `ResponseWriter` and its optional interfaces. The API, in
this case, is the `http.Handler` interface that users of the package implement
and that the package accepts. As features like HTTP/2 Push or connection
hijacking are not available to all connections, this interface needs to use the
lowest common denominator between all possible behaviors. So, if more features
need to be supported, we must somehow be able to pass this optional behavior
through the `http.Handler` interface.

##### Enabling optional optimizations/features

[io.Copy](http://godoc.org/io#Copy) serves as a good example of this. The
required interfaces for it to work are just `io.Reader` and `io.Writer`. But it
can be made more efficient, if the passed values also implement `io.WriterTo`
or `io.ReaderFrom`, respectively. For example, a
[bytes.Reader](http://godoc.org/bytes#Reader.WriteTo) implements `WriteTo`.
This means, you need less copying if the source of an `io.Copy` is a
`bytes.Reader`. Compare these two (somewhat naive) implementations:

```go
func Copy(w io.Writer, r io.Reader) (n int64, err error) {
	buf := make([]byte, 4096)
	for {
		rn, rerr := r.Read(buf)
		wn, werr := w.Write(buf[:rn])
		n += int64(wn)
		if rerr == io.EOF {
			return n, nil
		}
		if rerr != nil {
			return n, rerr
		}
		if werr != nil {
			return n, werr
		}
	}
}

func CopyTo(w io.Writer, r io.WriterTo) (n int64, err error) {
	return r.WriteTo(w)
}

type Reader []byte

func (r *Reader) Read(b []byte) (n int, err error) {
	n = copy(b, *r)
	*r = (*r)[n:]
	if n == 0 {
		err = io.EOF
	}
	return n, err
}

func (r *Reader) WriteTo(w io.Writer) (int64, error) {
	n, err := w.Write(*r)
	*r = (*r)[n:]
	return int64(n), err
}
```

`Copy` needs to first allocate a buffer, then copy all the data from the
`*Reader` to that buffer, then pass it to the Writer. `CopyTo`, on the other
hand, can directly pass the byte-slice to the Writer, saving an allocation and
a copy.

Some of that cost can be amortized, but in general, its existence is a forced
consequence of the API. By using optional interfaces, `io.Copy` can use the
more efficient method, if supported, and fall back to the slow method, if not.

##### Backwards compatible API changes

When `database/sql` upgraded to use `context`, it needed help from the drivers
to actually implement cancellation and the like. So it needed to add contexts
to the methods of [driver.Conn](http://godoc.org/database/sql/driver#Conn). But
it can't just do that change; it would be a backwards incompatible API change,
violating the Go1 compatibility guarantee. It also can't add a new method to
the interface to be used, as there are third-party implementations for drivers,
which would be broken as they don't implement the new method.

So it instead resorted to
[deprecate](https://golang.org/src/database/sql/driver/driver.go#L159) the old
methods and instead encourage driver implementers to add optional methods
including the context.

#### Why are they bad?

There are several problems with using optional interfaces. Some of them have
workarounds (see below), but all of them have drawbacks on their own.

##### They violate static type safety

In a lot of cases, the consumer of an optional interface can't really treat it
as optional. For example, `http.Hijacker` is usually used to support
WebSockets. A handler for WebSockets will, in general, not be able to do
anything useful, when called with a `ResponseWriter` that does not implement
`Hijacker`. Even when it correctly does a comma-ok type assertion to check
for it, it can't do anything but serve an error in that case.

The http.Hijacker type conveys the necessity of hijacking a connection, but
since it is provided as an optional interface, there is no possibility to
require this type statically. In that way, optional interfaces hide static type
information.

##### They remove a lot of the power of interfaces

Go's interfaces are really powerful by being very small; in general, the
advice is to only add one method, maybe a small handful. This advice enables
easy and powerful composition. `io.Reader` and `io.Writer` have a myriad of
implementations inside and outside of the standard library. This makes it
really easy to, say, read uncompressed data from a compressed network
connection, while streaming it to a file and hashing it at the same time to
write to some content-addressed blob storage.

Now, this composition will, in general, destroy any optional interfaces of
those values. Say, we have an HTTP middleware to log requests. It wants to wrap
an `http.Handler` and log the requests method, path, response code and duration
(or, equivalently, collect them as metrics to export). This is, in principle,
easy to do:

```go
type logResponder struct {
	http.ResponseWriter
	code int
	set bool
}

func (rw *logResponder) WriteHeader(code int) {
	rw.code = code
	rw.set = bool
	rw.ResponseWriter.WriteHeader(code)
}

func LogRequests(h http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		lr := &logResponder{ResponseWriter: w}
		m, p, start := r.Method, r.Path, time.Now()
		defer func() {
			log.Printf("%s %s -> %d (%v)", m, p, lr.code, time.Now().Sub(start))
		}()
		h(lr, r)
	})
}
```

But `*logResponder` will now *only* support the methods declared by
`http.ResponseWriter`, even if the wrapped `ResponseWriter` also supports some
of the optional interfaces. That is because method sets of a type are
determined at compile time.

Thus, by using this middleware, the wrapped handler is suddenly unable to use
websockets, or HTTP/2 server push or any of the other use cases of optional
interfaces. Even worse: this deficiency will only be discovered at runtime.

Optimistically adding the optional interface's methods and type-asserting the
underlying ResponseWriter at runtime doesn't work either: handlers would
incorrectly conclude the optional interface is always present. If the
underlying `ResponseWriter` does not support adding at the underlying
connection there just is no useful way to implement `http.Hijacker`.

There is one way around this, which is to dynamically check the wrapped
interface and create a type with the correct method set, e.g.:

```go
func Wrap(wrap, with http.ResponseWriter) http.ResponseWriter {
	var (
		flusher http.Flusher
		pusher http.Pusher
		// ...
	)
	flusher, _ = wrap.(http.Flusher)
	pusher, _ = wrap.(http.Pusher)
	// ...
	if flusher == nil && pusher == nil {
		return with
	}
	if flusher == nil && pusher != nil {
		return struct{
			http.ResponseWriter
			http.Pusher
		}{with, pusher}
	}
	if flusher != nil && pusher == nil {
		return struct{
			http.ResponseWriter
			http.Flusher
		}{with, flusher}
	}
	return struct{
		http.ResponseWriter
		http.Flusher
		http.Pusher
	}{with, flusher, pusher}
}
```

This has two major drawbacks:

* Both code-size and running time of this will increase exponentially with the
  number of optional interfaces you have to support (even if you generate the
  code).
* You need to know every single optional interface that might be used. While
  supporting everything in `net/http` is certainly tenable, there might be
  other optional interfaces, defined by some framework unbeknownst to you. If
  you don't know about it, you can't wrap it.

#### What can we use instead?

My general advice is, to avoid optional interfaces as much as possible. There
are alternatives, though they also are not entirely satisfying.

##### Context.Value

`context` was added after most of the optional interfaces where already
defined, but its `Value` method was meant exactly for this kind of thing: to
pass optional behavior past API boundaries. This will still not solve the
static type safety issue of optional interfaces, but it does mean you can
easily wrap them.

For example, `net/http` could instead do

```go
var ctxFlusher = ctxKey("flusher")

func GetFlusher(ctx context.Context) (f Flusher, ok bool) {
	f, ok = ctx.Value(ctxFlusher).(Flusher)
	return f, ok
}
```

This would enable you to do

```go
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	f, ok := http.GetFlusher(r.Context())
	if ok {
		f.Flush()
	}
}
```

If now a middleware wants to wrap `ResponseWriter`, that's not a problem, as it
will not touch the Context. If a middleware wants to add some other optional
behavior, it can do so easily:

```go
type Frobnicator interface{
	Frobnicate()
}

var ctxFrobnicator = ctxKey("frobnicator")

func GetFrobnicator(ctx context.Context) (f Frobnicator, ok bool) {
	f, ok = ctx.Value(ctxFrobnicator).(Frobnicator)
	return f, ok
}
```

As contexts form a linked list of key-value-pairs, this will interact nicely
with whatever optional behavior is already defined.

There are good reasons to frown upon the usage of `Context.Value`; but they
apply just as much to optional interfaces.

##### Extraction methods

If you know an interface type that is probable to be wrapped and *also* has
optional interfaces associated it is possible to enforce the possibility of
dynamic extension in the optional type. So, e.g.:

```go
package http

type ResponseWriter interface {
	// Methods…
}

type ResponseWriterWrapper interface {
	ResponseWriter

	WrappedResponseWriter() ResponseWriter
}

// GetFlusher returns an http.Flusher, if res wraps one.
// Otherwise, it returns nil.
func GetFlusher(res ResponseWriter) Flusher {
	if f, ok := res.(Flusher); ok {
		return f
	}
	if w, ok := res.(ResponseWriterWrapper); ok {
		return GetFlusher(w.WrappedResponseWriter())
	}
	return nil
}

package main

type logger struct {
	res ResponseWriter
	req *http.Request
	log *log.Logger
	start time.Time
}

func (l *logger) WriteHeader(code int) {
	d := time.Now().Since(l.start)
	l.log.Write("%s %s -> %d (%v)",	l.req.Method, l.req.Path, code, d)
	l.res.WriteHeader(code)
}

func (l *logger) WrappedResponseWriter() http.ResponseWriter {
	return l.res
}

func LogRequests(h http.Handler, l *log.Logger) http.Hander {
	return http.HandlerFunc(res http.ResponseWriter, req *http.Request) {
		res = &logger{
			res: res,
			req: req,
			log: l,
			start: time.Now(),
		}
		h.ServeHTTP(res, req)
	}
}

func ServeHTTP(res http.ResponseWriter, req *http.Request) {
	if f := http.GetFlusher(res); f != nil {
		f.Flush()
	}
}
```

This still doesn't address the static typing issue and explicit dependencies,
but at least it enables you to wrap the interface conveniently.

Note, that this is conceptually similar to the [errors
package](https://github.com/pkg/errors), which calls the wrapper-method
"Cause". This package also shows an issue with this pattern; it only
works if *all* wrappers use it. That's why I think it's important for the
wrapping interface to live in the same package as the wrapped interface; it
provides an authoritative way to do that wrapping, preventing fragmentation.

##### Provide statically typed APIs

`net/http` could provide alternative APIs for optional interfaces that
explicitly include them. For example:

```go
type Hijacker interface {
	ResponseWriter
	Hijack() (net.Conn, *bufio.ReadWriter, error)
}

type HijackHandler interface{
	ServeHijacker(w Hijacker, r *http.Request)
}

func HandleHijacker(pattern string, h HijackHandler) {
	// ...
}
```

For some use cases, this provides a good way to side-step the issue of unsafe
types. Especially if you can come up with a limited set of scenarios that would
rely on the optional behavior, putting them into their own type would be
viable.

The `net/http` package could, for example, provide separate `ResponseWriter`
types for different connection types (for example `HTTP2Response`). It could
then provide a `func(HTTP2Handler) http.Handler`, that serves an error if it is
asked to serve an unsuitable connection and otherwise delegates to the passed
Handler. Now, the programmer needs to explicitly wire a handler that requires
HTTP/2 up accordingly. They can rely on the additional features, while also
making clear what paths must be used over HTTP/2.

##### Gradual repair

I think the use of optional interfaces as in `database/sql/driver` is perfectly
fine - *if* you plan to eventually remove the original interface. Otherwise,
users will have to continue to implement both interfaces to be usable with your
API, which is especially painful when wrapping interfaces. For example, I
recently wanted to wrap
[importer.Default](http://godoc.org/go/importer#Default) to add behavior and
logging. I also needed [ImporterFrom](http://godoc.org/go/types#ImporterFrom),
which required separate implementations, depending on whether the importer
returned by Default implements it or not. Most modern code, however, shouldn't
need that.

So, for third party packages (the stdlib can't do that, because of
compatibility guarantees), you should consider using the methodology described
in Russ Cox' excellent [Codebase Refactoring](https://talks.golang.org/2016/refactor.article)
article and actually *deprecate* and eventually *remove* the old interface. Use
optional interfaces as a transition mechanism, not a fix.

#### How could Go improve the situation?

##### Make it possible for reflect to create methods

There are currently at least two GitHub issues which would make it possible to
do extend interfaces dynamically:
[reflect: NamedOf](https://github.com/golang/go/issues/16522), [reflect: MakeInterface](https://github.com/golang/go/issues/4146).
I believe this would be the easiest solution - it is backwards compatible and
doesn't require any language changes.

##### Provide a language mechanism for extension

The language could provide a native mechanism to express extension, either by
adding a
[keyword](https://medium.com/@cep21/interface-wrapping-method-erasure-c523b3549912#13bc)
for it or, for Go2, by considering to make extension the default behavior for
`interface->struct` embedding. I'm not sure either is a good idea, though. I
would probably prefer the latter, because of my distaste for keywords. Note,
that it would still be possible to then compose an interface into a struct,
just not via embedding but by adding a field and delegation-methods.
Personally, I'm not a huge fan of embedding interfaces in structs anyway except
when I'm explicitly trying to extend them with additional behavior.  Their
zero-value is not usable, so it requires additional hoops to jump through.

#### Conclusion

I recommend:

* If at all possible, avoid optional interfaces in APIs you provide. They are
  just too inconvenient and un-Go-ish.
* Be careful when wrapping interfaces, in particular when there are known
  optional interfaces for them.

Using optional interfaces correctly is inconvenient and cumbersome. That should
signal that you are fighting the language. The workarounds needed all try to
circumvent one or more design decision of Go: to value composition over
inheritance, to prefer static typing and to make computation and behavior
obvious from code. To me, that signifies that optional interfaces are
fundamentally not a good fit for the language.
]]></content:encoded>
      <dc:date>2017-07-30T18:39:00+00:00</dc:date>
    </item>
    <item>
      <title>Using Hilbert Curves to 100% Zelda</title>
      <link>https://blog.merovius.de//2017/07/22/using-hilbert-curves-to-100-zelda.html</link>
      <description><![CDATA[tl;dr: I used Hilbert Curves to make it quicker to walk through a list of locations on a map, so I could could fully complete a video game.
]]></description>
      <pubDate>Sat, 22 Jul 2017 23:56:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/07/22/using-hilbert-curves-to-100-zelda.html</guid>
      <content:encoded><![CDATA[**tl;dr: I used Hilbert Curves to make it quicker to walk through a list of locations on a map, so I could could fully complete a video game.**

As you probably know recently the question of what [the best Zelda
Game](https://s-media-cache-ak0.pinimg.com/originals/eb/31/e5/eb31e5c0a14d4a68ab8d492e848de608.jpg)
is was finally settled by Breath Of The Wild. Like most people I know I ended
up playing. And to keep me engaged I early on decided that I would get as close
as possible to 100% of the game before finishing it. That is I wanted to finish
all shrines, find all Korok Seeds, max out all armor and do all quests before
killing Ganon. I recently finished that and finally killed Ganon. Predictably,
I was in for a disappointment:

![98.59%]({{ site.url }}/assets/botw_9859.jpg)

98.59 percent! I did expect that though. The reason is that only certain things
count into the percentage as displayed; Korok Seeds are one of them, Shrines
are another. But it also counts landmarks and locations as shown on the map.
Each contributes 1/12% to the total.

So I started on the onerous task of finding the last 17 locations. I'm not
above using help for that so I carefully scrolled through [an online
map](https://www.zeldadungeon.net/breath-of-the-wild-interactive-map) of the
BotW universe, maticulously comparing the locations on it with the ones already
on my in-game map. Anything I haven't visited was marked and visited. But that
only put me to 99.58%; I was still missing 5 locations. apparently I didn't
compare carefully enough.

I needed a more systematic approach. I started to instead go through an
[alphabetical list of
locations](http://www.ign.com/wikis/the-legend-of-zelda-breath-of-the-wild/Locations_by_Region),
looking up each on the map and see if I already had it mapped. But that got old
*really* quickly. Alphabetical just wasn't a great way to organize these; I
wanted a list that I could systematically check. But I didn't want it
alphabetically but geographically. I didn't want to have to jump around the map
to try and find the next one. Which is when I realized that this would be the
perfect application for a [Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve).

If you don't know (though you should really just read the Wikipedia Article),
the Hilbert curve is a space filling fractal curve, that is a continuous
bijective map from the real number line to the plane. It is iteratively
defined as the limit of finite curves that get denser and denser. One of the
most interesting properties of the curve and its finite approximations is that
points that are close on the real number line get mapped to points that are
close in the plane. So if we could extract all locations from the online map,
figure out for each what real number gets mapped to that point and order the
locations by those numbers, we'd get a list of locations where neighbors in the
list are close to each other on the map. Presumably, that
would make for easy checking of the list: The next location should be pretty
much neighbouring the previous one and if I can't find a location nearby,
chances are that I didn't visit it yet (and I can then look it up specifically).

**\[edit\] Commentors on
[reddit](https://www.reddit.com/r/programming/comments/6oxra8/using_hilbert_curves_to_100_zelda/dklhina/)
and [Hacker news](https://news.ycombinator.com/item?id=14830691) have pointed
out correctly, that all curves satisfy the property that near point on the line
map to near points on the plane. What makes the Hilbert Curve special, is that
we work on finite approximations and with the Hilbert Curve, we don't have to
worry about the "correct" level of discrete approximation.**

**To see what that means, we can look at a zig-zag curve. Say, we split our map
into a 100000x100000 grid and move in a zig-zag, left-to-right, top-to-bottom.
Given how sparse our point-set is, this would mean that most of the rows are
empty and some of them would have only one point on them. So we wozuld have to
constantly move along the entire width of the map. On the other hand, if we
split it into a 2x2 grid, it wouldn't be very helpful; a lot of points would
end up in the same quadrant, which would be very large, so we wouldn't have won
anything. So there would have to be some fineness of the grid that's "just
right" somewhere in the middle, which we'd need to find.**

**On the other hand with Hilbert Curves, this isn't a problem. That's because the
*limit* of the finite approximations is continuous (which isn't the case with
the limit of zig-zag curves). What that means, in essence, is that where a
point falls on the curve won't jump around a lot when we make our grid finer,
it will "home in" to its final location on the continuous curve. A first order
Hilbert Curve is just a zig-zag curve, so it has the same problem as the
2x2-grid zig-zag line. But as we increase it's order, the points will just
become more and more local, instead of requiring scanning empty space. That is
the interesting consequence of the Hilbert curve being space-filling.**

**Really, [this video](https://www.youtube.com/watch?v=3s7h2MHQtxc) explains it
much better than I ever could (even though I find the example given there
slightly ridiculous). In the end, I mostly agree with the commentors; it
probably wouldn't have been too hard to find a good approximation that would
make a zig-zag curve work well. But I had Hilbert Curves ready anyway and
appreciated the opportunity to usue them.\[/edit\]**

The first step for this was to get a list of locations and their corresponding
positions. I was pretty sure that the online map should have that available
somehow, as it uses some Google Maps framework to draw the map. So I
looked at the network tab of the Chrome developer tools, found the URL that
loaded the landmark data, copied the request as curl and saved the output for
further massaging.

![Chrome developer tools - copy as cURL]({{ site.url }}/assets/botw_curl.jpg)

The returned file turns out to not actually be JSON (that'd be too easy, I
guess) but some kind of javascript-code which is then probably eventually
eval'd to get the data (**edit: It has been pointed out, that this is just
[JSONP](https://en.wikipedia.org/wiki/JSONP). I was aware that this is probably
the case, but didn't feel comfortable using the term, as I don't know enough
about it. I also didn't consider it very important**) :

```
/**/jQuery31106443585752152035_1500757689075(/* json-data */)
```

I just removed everything but the actual JSON with my editor and ran it through
a pretty-printer, to get at it's actual structure. I spare you the details; it
turns out the list of locations isn't even simply contained in that it's
embedded as another string, with HTML tags, as a property (twice!).

So I quickly hacked together some go code to dissect the data and voilà: Got a
list of location names with the corresponding positions:

```go
func main() {
	var data struct {
		Parse struct {
			Properties []struct {
				Name    string `json:"name"`
				Content string `json:"*"`
			} `json:"properties"`
		} `json:"parse"`
	}

	if err := json.NewDecoder(os.Stdin).Decode(&data); err != nil {
		panic(err)
	}

	var content string

	for _, p := range data.Parse.Properties {
		if p.Name == "description" {
			content = p.Content
		}
	}

	if content == "" {
		panic("no content")
	}

	var landmarks []struct {
		Type     string
		Geometry struct {
			Type        string
			Coordinates []float64
		}
		Properties struct {
			Type string
			Id   string
			Name string
			Link string
			Src  string
		}
	}

	if err := json.NewDecoder(arrayReader(content)).Decode(&landmarks); err != nil {
		panic(err)
	}

	for _, m := range landmarks {
		fmt.Printf("%s: %v\n", m.Properties.Name, m.Geometry.Coordinates)
	}
}

func arrayReader(s string) io.Reader {
	s = strings.TrimSuffix(strings.TrimSpace(s), ",")
	return io.MultiReader(strings.NewReader("["), strings.NewReader(s), strings.NewReader("]"))
}
```

This bode well. Now all I needed to do was to calculate the Hilbert Curve
coordinate for each of them and I'd have what I need. The Wikipedia Article
helpfully contains an
[implementation](https://en.wikipedia.org/wiki/Hilbert_curve#Applications_and_mapping_algorithms)
of the corresponding algorithm in C. `xy2d` assumes a discrete grid of n² cells
and returns an integer preimage of the given coordinates. The coordinates we have
are all floating point numbers between 0 and 2 (ish) with 5 significant digits.
I figured that 65536 should be able to represent the granularity of points well
enough, so I chose that as an n, ported the code to go, sorted the locations
accordingly and it *actually worked*!

```go
func main() {
	// Same stuff as before

	sort.Slice(landmarks, func(i, j int) bool {
		xi := f2d(landmarks[i].Geometry.Coordinates[0])
		yi := f2d(landmarks[i].Geometry.Coordinates[1])
		xj := f2d(landmarks[j].Geometry.Coordinates[0])
		yj := f2d(landmarks[j].Geometry.Coordinates[1])
		di := xy2d(1<<16, xi, yi)
		dj := xy2d(1<<16, xj, yj)
		return di < dj
	})

	for _, m := range landmarks {
		fmt.Printf("%s: %v\n", m.Properties.Name, m.Geometry.Coordinates)
	}
}

func xy2d(n, x, y int) int {
	var d int
	for s := n / 2; s > 0; s = s / 2 {
		var rx, ry int
		if (x & s) > 0 {
			rx = 1
		}
		if (y & s) > 0 {
			ry = 1
		}
		d += s * s * ((3 * rx) ^ ry)
		x, y = rot(s, x, y, rx, ry)
	}
	return d
}

func rot(n, x, y, rx, ry int) (int, int) {
	if ry == 0 {
		if ry == 1 {
			x = n - 1 - x
			y = n - 1 - y
		}
		x, y = y, x
	}
	return x, y
}

func f2d(f float64) int {
	return int((1 << 15) * f)
}
```

In the end, there was still a surprising amount of jumping around involved. I
don't know whether that's accidental (i.e. due to my code being wrong) or
inherent (that is the Hilbert curve just can't map this perfectly well). I
assume it's a bit of both. The list also contains the same landmark multiple
times. This is because things like big lakes or plains where marked multiple
times. It would be trivial to filter duplicates but I actually found them
reasonably helpfull when having to jump around.

There might also be better approaches than Hilbert Curves. For example, we
could view it as an instance of the [Traveling Salesman
Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) with a
couple of hundred points; it should be possible to have a good heuristic
solution for that. On the other hand, a TSP solution doesn't necessarily only
have short jumps, so it *might* not be that good?

In any case, this approach was definitely good enough for me and it's probably
the nerdiest thing I ever did :)

![100%]({{ site.url }}/assets/botw_1000.jpg)
]]></content:encoded>
      <dc:date>2017-07-22T23:56:00+00:00</dc:date>
    </item>
    <item>
      <title>How to not use an http-router in go</title>
      <link>https://blog.merovius.de//2017/06/18/how-not-to-use-an-http-router.html</link>
      <description><![CDATA[If you don&#39;t write web-thingies in go you can stop reading now. Also, I am
somewhat snarky in this article. I intend that to be humorous but am probably
failing. Sorry for that
]]></description>
      <pubDate>Sun, 18 Jun 2017 22:57:21 +0000</pubDate>
      <guid>https://blog.merovius.de//2017/06/18/how-not-to-use-an-http-router.html</guid>
      <content:encoded><![CDATA[**If you don't write web-thingies in go you can stop reading now. Also, I am
somewhat snarky in this article. I intend that to be humorous but am probably
failing. Sorry for that**

As everyone™ knows, people need to [stop writing
routers/muxs](https://twitter.com/bketelsen/status/875435750770089984) in go.
Some people
[attribute](https://twitter.com/markbates/status/875517884931473409) the
abundance of routers to the fact that the `net/http` package fails to provide a
sufficiently powerful router, so people roll their own. This is also reflected
in [this post](https://medium.com/@joeybloggs/gos-std-net-http-is-all-you-need-right-1c5555a9f2f6),
in which a gopher complains about how complex and hard to maintain it would be
to route requests using `net/http` alone.

I disagree with both of these. I don't believe the problem is a lack of a
powerful enough router in the stdlib. I also disagree that routing based purely
on `net/http` has to be complicated or hard to maintain.

However, I *do* believe that the community currently lacks good guidance on
*how* to properly route requests using `net/http`. The default result seems to
be that people assume they are supposed to use `http.ServeMux` and get
frustrated by it. In this post I want to explain why routers *in general* -
including `http.ServeMux` - should be avoided and what I consider simple,
maintainable and scalable routing using nothing but the stdlib.

#### But why?

![But why?](https://i.giphy.com/1M9fmo1WAFVK0.webp)

Why do I believe that routers should not be used? I have three arguments for
that: They need to be very complex to be useful, they introduce strong coupling
and they make it hard to understand how requests are flowing.

The basic idea of a router/mux is, that you have a single component which
looks at a request and decides what handler to dispatch it to. In your `func
main()` you then create your router, you define all your routes with all your
handlers and then you call `Serve(l, router)` and everything's peachy.

But since URLs can encode a lot of important information to base your routing
decisions on, doing it this way requires a lot of extra features. The [stdlib
ServeMux](https://godoc.org/net/http#ServeMux) is an incredibly simple router
but even that contains a certain amount of magic in its routing decisions;
depending on whether a pattern contains a trailing slash or not it might either
be matched as a prefix or as a complete URL and longer patterns take precedence
over shorter ones and oh my. But the stdlib router isn't even powerful enough.
Many people need to match URLs like `"/articles/{category}/{id:[0-9]+}"` in
their router and while we're at it also extract those nifty arguments. So
they're using [gorilla/mux](https://godoc.org/github.com/gorilla/mux) instead.
An awful lot of code to route requests.

Now, without cheating (and actually knowing that package counts as cheating),
tell me for each of these requests:

* `GET /foo`
* `GET /foo/bar`
* `GET /foo/baz`
* `POST /foo`
* `PUT /foo`
* `PUT /foo/bar`
* `POST /foo/123`

What handler they map to and what status code do they return ("OK"? "Bad
Request"? "Not Found"? "Method not allowed"?) in this routing setup?

```go
r := mux.NewRouter()
r.PathPrefix("/foo").Methods("GET").HandlerFunc(Foo)
r.PathPrefix("/foo/bar").Methods("GET").HandlerFunc(FooBar)
r.PathPrefix("/foo/{user:[a-z]+}").Methods("GET").HandlerFunc(FooUser)
r.PathPrefix("/foo").Methods("POST").HandlerFunc(PostFoo)
```

What if you permute the lines in the routing-setup?

You might guess correctly. You might not. There are multiple sane routing
strategies that you could base your guess on. The routes might be tried in
source order. The routes might be tried in order of specificity. Or a
complicated mixture of all of them. The router might realize that it could
match a Route if the method were different and return a 405. Or it might not not. Or that
`/foo/123` is, technically, an illegal argument, not a missing page. I couldn't
really find a good answer to any of these questions in the documentation of
`gorilla/mux` for what it's worth. Which meant that when my web app suddenly
didn't route requests correctly, I was stumped and needed to dive into code.

You could say that people just have to learn how `gorilla/mux` decides it's
routing (I believe it's "as defined in source order", by the way). But there
are at least fifteen thousand routers for go and no newcomer to your
application will ever know all of them. When a request does the wrong thing, I
don't want to have to debug your router first to find out what handler it is
actually going to and then debug that handler. I want to be able to follow the
request through your code, even if I have next to zero familiarity with it.

Lastly, this kind of setup requires that all the routing decisions for your
application are done in a central place. That introduces edit-contention, it
introduces strong coupling (the router needs to be aware of all the paths and
packages needed in the whole application) and it becomes unmaintainable after a
while. You can alleviate that by delegating to subrouters though; which really
is the basis of how I prefer to do all of this these days.

#### How to use the stdlib to route

Let's build the toy example from [this medium post](https://medium.com/@joeybloggs/gos-std-net-http-is-all-you-need-right-1c5555a9f2f6).
It's not terribly complicated but it serves nicely to illustrate the general
idea. The author intended to show that using the stdlib for routing would be
too complicated and wouldn't scale. But my thesis is that the issue is that
*they are effectively trying to write a router*. They are trying to
encapsulate all the routing decisions into one single component. Instead,
separate concerns and make small, easily understandable routing decisions
locally.

Remember how I told you that we're going to use only the stdlib for routing?

![Those where lies, plain and simple](https://i.giphy.com/l4FGmlJviGJcYM2sM.webp)

We are going to use this one helper function:

```go
// ShiftPath splits off the first component of p, which will be cleaned of
// relative components before processing. head will never contain a slash and
// tail will always be a rooted path without trailing slash.
func ShiftPath(p string) (head, tail string) {
	p = path.Clean("/" + p)
	i := strings.Index(p[1:], "/") + 1
	if i <= 0 {
		return p[1:], "/"
	}
	return p[1:i], p[i:]
}
```

Let's build our app. We start by defining a handler type. The premise of this
approach is that handlers are strictly separated in their concerns. They either
correctly handle a request with the correct status code or they delegate to
another handler which will do that. They only need to know about the immediate
handlers they delegate to and they only need to know about the sub-path they
are rooted at:

```go
type App struct {
	// We could use http.Handler as a type here; using the specific type has
	// the advantage that static analysis tools can link directly from
	// h.UserHandler.ServeHTTP to the correct definition. The disadvantage is
	// that we have slightly stronger coupling. Do the tradeoff yourself.
	UserHandler *UserHandler
}

func (h *App) ServeHTTP(res http.ResponseWriter, req *http.Request) {
	var head string
	head, req.URL.Path = ShiftPath(req.URL.Path)
	if head == "user" {
		h.UserHandler.ServeHTTP(res, req)
		return
	}
	http.Error(res, "Not Found", http.StatusNotFound)
}

type UserHandler struct {
}

func (h *UserHandler) ServeHTTP(res http.ResponseWriter, req *http.Request) {
	var head string
	head, req.URL.Path = ShiftPath(req.URL.Path)
	id, err := strconv.Atoi(head)
	if err != nil {
		http.Error(res, fmt.Sprintf("Invalid user id %q", head), http.StatusBadRequest)
		return
	}
	switch req.Method {
	case "GET":
		h.handleGet(id)
	case "PUT":
		h.handlePut(id)
	default:
		http.Error(res, "Only GET and PUT are allowed", http.StatusMethodNotAllowed)
	}
}

func main() {
	a := &App{
		UserHandler: new(UserHandler),
	}
	http.ListenAndServe(":8000", a)
}
```

This seems very simple to me (not necessarily in "lines of code" but
definitely in "understandability"). You don't need to know anything about any
routers. If you want to understand how the request is routed you start by
looking at `main`. You see that `(*App).ServeHTTP` is used to serve any
request so you `:GoDef` to its definition. You see that it decides to dispatch
to `UserHandler`, you go to its `ServeHTTP` method and you see directly how it
parses the URL and what the decisions are that it made on its base.

We still need to add some patterns to our application. Let's add a profile
handler:

```go
type UserHandler struct{
	ProfileHandler *ProfileHandler
}

func (h *UserHandler) ServeHTTP(res http.ResponseWriter, req *http.Request) {
	var head string
	head, req.URL.Path = ShiftPath(req.URL.Path)
	id, err := strconv.Atoi(head)
	if err != nil {
		http.Error(res, fmt.Sprintf("Invalid user id %q", head), http.StatusBadRequest)
		return
	}

	if req.URL.Path != "/" {
		head, tail := ShiftPath(req.URL.Path)
		switch head {
		case "profile":
			// We can't just make ProfileHandler an http.Handler; it needs the
			// user id. Let's instead…
			h.ProfileHandler.Handler(id).ServeHTTP(res, req)
		case "account":
			// Left as an exercise to the reader.
		default:
			http.Error(res, "Not Found", http.StatusNotFound)
		}
		return
	}
	// As before
	...
}

type ProfileHandler struct {
}

func (h *ProfileHandler) Handler(id int) http.Handler {
	return http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) {
		// Do whatever
	})
}
```

This may, again, seem complicated but it has the cool advantage that the
dependencies of `ProfileHandler` are clear at compile time. It needs a user id
which needs to come from *somewhere*. Providing it via this kind of method
ensures this is the case. When you refactor your code, you won't accidentally
forget to provide it; it's impossible to miss!

There are two potential alternatives to this if you prefer them: You could put
the user-id into `req.Context()` or you could be super-hackish and add them to
`req.Form`. But I prefer it this way.

You might argue that `App` still needs to know all the transitive dependencies
(because they are members, transitively) so we haven't actually reduced
coupling. But that's not true. Its `UserHandler` could be created by a
`NewUserHandler` function which gets passed its dependencies via the mechanism
of your choice (flags, dependency injection,…) and gets wired up in `main`. All
`App` needs to know is the API of the handlers it's *directly* invoking.

#### Conclusion

I hope I convinced you that routers *in and off itself* are harmful. Pulling
the routing into one component means that that component needs to encapsulate
an awful lot of complexity, making it hard to debug. And as no single existing
router will contain all the complicated cleverness you want to base your
routing decisions on, you are tempted to write your own. Which everyone does.

Instead, split your routing decisions into small, independent chunks and
express them in their own handlers. And wire the dependencies up at compile
time, using the type system of go, and reduce coupling.
]]></content:encoded>
      <dc:date>2017-06-18T22:57:21+00:00</dc:date>
    </item>
    <item>
      <title>I've been diagnosed with ADHD</title>
      <link>https://blog.merovius.de//2016/08/31/ive-been-diagnosed-with-adhd.html</link>
      <description><![CDATA[tl;dr: I&#39;ve been diagnosed with ADHD. I ramble incoherently for a while and I
might do some less rambling posts about it in the future.
]]></description>
      <pubDate>Wed, 31 Aug 2016 02:22:38 +0000</pubDate>
      <guid>https://blog.merovius.de//2016/08/31/ive-been-diagnosed-with-adhd.html</guid>
      <content:encoded><![CDATA[**tl;dr: I've been diagnosed with ADHD. I ramble incoherently for a while and I
might do some less rambling posts about it in the future.**

As the title says, I've been recently diagnosed with ADHD and I thought I'd try
to be as open about it as possible and share my personal experiences with
mental illness. That being said, I am also adding the disclaimer, that I have
no training or special knowledge about it and that the fact that I have been
*diagnosed* with ADHD does not mean I am an authoritative source on its
effects, that this diagnoses is going to stick or that my experiences in any
way generalize to other people who got the same diagnosis.

This will hopefully turn into a bit of a series of blog posts and I'd like to
start it off with a general description of what lead me to look for a diagnosis
and treatment in the first place. Some of the things I am only touching on here
I might write more about in the future (see below for a non-exhaustive list).
Or not. I have not yet decided :)

---

It is no secret (it's actually kind of a running gag) that I am a serious
procrastinator. I always had trouble starting on something and staying with it;
my graveyard of unfinished projects is huge. For most of my life, however, this
hasn't been a huge problem to me. I was reasonably successful in compensating
for it with a decent amount of intelligence (arrogant as that sounds). I never
needed any homework and never needed to learn for tests in school and even
at university I only spent very little time on both. The homework we got was
short-term enough that procrastination was not a real danger, I finished it
quickly and whenever there *was* a longer-term project to finish (such as a
seminar-talk or learning for exams) I could cram for a night and get enough of
an understanding of things to do a decent job.

However, that strategy did not work for either my bachelor, nor my master
thesis, which predictably lead to both turning out a lot worse than I would've
wished for (I am not going to go into too much detail here). Self-organized
long-term work seemed next to impossible. This problem got *much* worse when I
started working full-time. Now almost all my work was self-organized and
long-term. Goals are set on a quarterly basis, the decision when and how and
how much to work is completely up to you. Other employers might frown at their
employees slacking off at work; where I work, it's almost expected. I was good
at being oncall, which is mainly reactive, short-term problem solving. But I
was (and am) completely dissatisfied with my project work. I felt that I did
not get nearly as much done as I should or as I would *want*. My projects in my
first quarter had very clear deadlines and I finished on time (I still
procrastinated, but at least at some point I sat down until I got it done. It
still meant staying at work until 2am the day before the deadline) but after
that it went downhill fast, with projects that needed to be done ASAP, but not
with a *deadline*. So I started slipping. I didn't finish my projects (in fact,
the ones that I didn't drop I am still not done with), I spent weeks doing
effectively nothing (I am not exaggerating here. I spent whole weeks not
writing a single line of code, closing a single bug or running a single
command, doing nothing but browse reddit, twitter and wasting my time in
similar ways. Yes, you can waste a week doing nothing, while sitting at your
desk), not being able to get myself to start working on anything and hating
myself for it.

And while I am mostly talking about work, this affected my personal life too.
Mail remains unopened, important errands don't get done, I am having trouble
keeping in contact with friends, because I can always write or visit them some
other time…

I tried (and had tried over the past years) several systems to organize myself
better, to motivate myself and to remove distractions. I was initially
determined to try to solve my problems on my own, that I did not really need
professional help. However, at some point, I realized that I won't be able to
fix this just by willing myself to it. I realized it in the final months of
my master thesis, but convinced myself that I [don't have time to fix it
properly, after all, I have to write my
thesis](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAJWAAAAJDIwZTMwODMwLThkZTYtNDU4Ny04ZmI3LTE3N2E2MWYxZGVlMA.jpg).
I then kind of forgot about it (or rather: I procrastinated it) in the
beginning of starting work, because things where going reasonably well. But it
came back to me around the start of this year. After not finishing any of my
projects in the first quarter. And after telling my coworkers and friends of my
problems and them telling me that it's just impostor syndrome and a distorted
view of myself (I'll go into why they where wrong some more later, possibly).

I couldn't help myself and I couldn't get effective help from my coworkers. So,
in April, I finally decided to see a Psychologist. Previously the fear of
the potential cost (or the stress of dealing with how that works with my
insurance), the perceived complexity of finding one that is both accepting
patients that are only publicly insured and is specialized on my particular
kinds of issues and the perceived lack of time prevented me from doing so.
Apart from a general doubt about its effectiveness and fear of the
implications for my self-perception and world-view, of course.

Luckily one of the employee-benefits at my company is free and uncomplicated
access to a Mental Health (or "Emotional well-being", what a fun euphemism)
provider. It only took a single E-Mail and the meetings happen around 10 meters
away from my desk. So I started seeing a psychologist on a regular basis (on
average probably every one and a half weeks or so) and talking about my issues.
I explained and described my problems and it went about as good as I feared;
they tried to convince me that the real issue isn't my performance, but my
perception of it (and I concede that I still have trouble coming up with hard,
empirical evidence to present to people. Though it's performance review season
right now. As I haven't done anything of substance in the past 6 months, maybe
I will finally get that evidence…) and they tried to get me to adopt more
systems to organize myself and remove distractions. All the while, I got worse
and worse. My inability to get even the most basic things done or to
concentrate even for an hour, even for five minutes, on anything of substance,
combined with the inherent social isolation of moving to a new city and
country, lead me into deep depressive episodes.

Finally, when my Psychologist in a session tried to get me to write down what
was essentially an [Unschedule](http://www.neilfiore.com/now-habit-schedules/)
(a system I knew about from reading "The Now Habit" myself when working at my
bachelor thesis and that I even had moderate success with; for about two
weeks), I broke down. I told them that I do not consider this a valuable use of
these sessions, that I tried systems before, that I tried *this particular*
system before and that I can find these kind of lifestyle advise on my own, in
my free time. That the reason I was coming to these sessions was to get
systematic, medical, professional help of the kind that I *can't* get from
books. So we agreed, at that point, to pursue a diagnosis, as a precondition
for treatment.

Which, basically, is where we are at now. The diagnostic process consisted of
several sessions of questions about my symptoms, my childhood and my life in
general, of filling out a couple of diagnostic surveys and having my siblings
fill them out too (in the hope that they can fill in some of the blanks in my
own memory about my childhood) and of several sessions of answering more
questions from more surveys. And two weeks ago, I officially got the diagnosis
ADHD. And the plan to attack it by a combination of therapy and medication (the
latter, in particular, is *really* hard to get from books, for some reason :) ).

I just finished my first day on
[Methylphenidate](https://en.wikipedia.org/wiki/Methylphenidate) (the active
substance in Ritalin), specifically Concerta. And though this is definitely
*much* too early to actually make definitive judgments on its effects and
effectiveness, at least for this one day I was feeling really great and
actively happy. Which, coincidentally, helped me to finally start on this post,
to talk about mental health issues; a topic I've been wanting to talk about
ever since I started this blog (again), but so far didn't really felt I could.

---

As I said, this is hopefully the first post in a small ongoing series. I am
aware that it is long, rambling and probably contentious. It definitely won't get
all my points across and will change the perception some people have of me (I
can hear you thinking how all of this doesn't really speak "mental illness",
how it seems implausible that someone with my CV would actually, objectively,
get nothing done and how I am a drama queen and shouldn't try to solve my
problems with dangerous medication). It's an intentionally broad overview of my
process and its main purpose is to "out" myself publicly and create starting
points for multiple, more specific, concise, interesting and convincing posts
in the future. Things I might, or might not talk about are

* My specific symptoms and how this has and still is influencing my life (and
  how, yes, this is actual an *illness*, not just a continuous label). In
  particular, there are things I wasn't associating with ADHD, which turn out
  to be relatively tightly linked.
* How my medication is specifically affecting me and what it does to those
  symptoms. I can not overstate how *fascinated* I am with today's experience.
  I was wearing a visibly puzzled expression all day because I couldn't figure
  out what was happening. And then I couldn't stop smiling. :)
* Possibly things about my therapy? I really don't know what to expect about
  that, though. Therapy is kind off the long play, so it's much harder to
  evaluate and talk about its effectiveness.
* Why I consider us currently to be in the Middle Ages of mental health and why
  I think that in a hundred years or so people will laugh at how we currently
  deal with it. And possibly be horrified.
* My over ten years (I'm still young, mkey‽) of thinking about my own mental
  health and mental health in general and my thoughts of how mental illness
  interacts with identity and self-definition.
* How much I *loathe* the term "impostor syndrome" and why I am (still)
  convinced that I don't get enough done, even though I can't produce empirical
  evidence for that and people try to convince me otherwise. And what it does
  to you, to need to take the "I suck" side of an argument and still don't have
  people believe you.

Let me know, what you think :)
]]></content:encoded>
      <dc:date>2016-08-31T02:22:38+00:00</dc:date>
    </item>
    <item>
      <title>Backwards compatibility in go</title>
      <link>https://blog.merovius.de//2015/07/29/backwards-compatibility-in-go.html</link>
      <description><![CDATA[tl;dr: There are next to no &quot;backwards compatible API changes&quot; in go. You
should explicitely name your compatibility-guarantees.
]]></description>
      <pubDate>Wed, 29 Jul 2015 01:10:11 +0000</pubDate>
      <guid>https://blog.merovius.de//2015/07/29/backwards-compatibility-in-go.html</guid>
      <content:encoded><![CDATA[**tl;dr: There are next to no "backwards compatible API changes" in go. You
should explicitely name your compatibility-guarantees.**

I really love go, I really hate vendoring and up until now I didn't really get,
why anyone would think go should need something like that. After all, go seems
to be predestined to be used with automatically checked semantic versioning.
You can enumerate all possible changes to an API in go and the list is quite
short. By looking at vcs-tags giving semantic versions and diffing the API, you
can automatically check that you never break compatibility (the go compiler and
stdlib actually do something like that). Heck, in theory you could even write a
package manager that automatically (without any further annotations) determines
the latest version of a package that still builds all your stuff or gives you
the minimum set of packages that need changes to reconcile conflicts.

This thought lead me to contemplate what makes an API change a breaking
change. After a bit of thought, my conclusion is that almost every API change
is a breaking change, which might surprise you.

For this discussion we first need to make some assumptions about what
constitutes breakage. We will use the [go1 compatibility promise](http://golang.org/doc/go1compat).
The main gist is: Stuff that builds before is guaranteed to build after.
Notable exceptions (apart from necessary breakages due to security or other
bugs) are unkeyed struct literals and dot-imports.

**[edit]**
I should clarify, that whenever I talk about an API-change, I mean
your exported API as defined by Code (as opposed to comments/documentation).
This includes the public identifiers exported by your package, including type
information. It excludes API-requirements specified in documentation, like on
[io.Writer](http://golang.org/pkg/io/#Writer). These are just too complex to
talk about in a meaningful way and must be dealt with separately anyway.
**[/edit]**

So, given this definition of breakage, we can start enumerating all the
possible changes you could do to an API and check whether they are breaking
under the definition of the go1 compatibility promise:

#### Adding func/type/var/const at package scope

This is the only thing that seems to be fine under the stability guarantee.  It
turns out the go authors thought about this one and put the exception of
dot-imports into the compatibility promise, which is great.

dot-imports are imports of the form `. import "foo"`. They import every
package-level identifier of package `foo` into the scope of the current file.

Absence of dot-imports means, every identifier at your package scope must be
referenced with a selector-expression (i.e. `foo.Bar`) which can't be redeclared
by downstream. It also means that you should never use dot-imports in your
packages (which is a bad idea for other reasons too). Treat dot-imports as a
historic artifact which is completely deprecated. An exception is the need
to use a separate `foo_test` package for your tests to break dependency cycles.
In that case it is widely deemed acceptable to `. import "foo"` to save typing
and add clarity.

#### Removing func/type/var/const at package scope

Downstream might use the removed function/type/variable/constant, so this is
obviously a breaking change.

#### Adding a method to an interface

Downstream might want to create an implementation of your interface and try to
pass it. After you add a method, this type doesn't implement your interface
anymore and downstreams code will break.

#### Removing a method from an interface

Downstream might want to call this method on a value of your interface type, so
this is obviously a breaking change.

#### Adding a field to a struct

This is perhaps surprising, but adding a field to a struct is a breaking
change. The reason is, that downstream might embed two types into a struct. If
one of them has a field or method Bar and the other is a struct you added the
Field Bar to, downstream will fail to build (because of an ambiguous selector
expression).

So, e.g.:

```go
// foo/foo.go
package foo

type Foo struct {
	Foo string
	Bar int // Added after the fact
}

// bar/bar.go
package bar

type Baz struct {
	Bar int
}

type Spam struct {
	foo.Foo
	Baz
}

func Eggs() {
	var s Spam
	s.Bar = 42 // ambiguous selector s.Bar
}
```

This is what the compatibility *might* refer to with the following quote:

> Code that uses unkeyed struct literals (such as pkg.T{3, "x"}) to create values
> of these types would fail to compile after such a change. However, code that
> uses keyed literals (pkg.T{A: 3, B: "x"}) will continue to compile after such a
> change.  We will update such data structures in a way that allows keyed struct
> literals to remain compatible, although unkeyed literals may fail to compile.
> (**There are also more intricate cases involving nested data structures or
> interfaces**, but they have the same resolution.)

(emphasis is mine). By "the same resolution" they *might* refer to only accessing
embedded Fields via a keyed selector (so e.g. `s.Baz.Bar` in above example). If
so, that is pretty obscure and it makes struct-embedding pretty much
useless. Every usage of a field or method of an embedded type must be
explicitly Keyed, which means you can just *not* embed it after all. You need
to write the selector and wrap every embedded method anyway.

I hope we all agree that type embedding is awesome and shouldn't need to be
avoided :)

#### Removing a field from a struct

Downstream might use the now removed field, so this is obviously a breaking change.

#### Adding a method to a type

The argument is pretty much the same as adding a field to a struct: Downstream
might embed your type and suddenly get ambiguities.

#### Removing a method from a type

Downstream might call the now removed method, so this is obviously a breaking change.

#### Changing a function/method signature

Most changes are obviously breaking. But as it turns out you can't do *any*
change to a function or method signature. This includes adding a variadic
argument which *looks* backwards compatible on the surface. After all, every
call site will still be correct, right?

The reason is, that downstream might save your function or method in a variable
of the old type, which will break because of nonassignable types.

#### Conclusion

It looks to me like anything that isn't just adding a new Identifier to the
package-scope will potentially break *some* downstream. This severely limits
the kind of changes you can do to your API if you want to claim backwards
compatibility.

This of course doesn't mean that you should never ever make any changes to your
API ever. But you should think about it and you should clearly document, what
kind of compatibility guarantees you make. When you do any changes named in
this document, you should check your downstreams, whether they are affected by
it. If you claim a similar level of compatibility as the go standard library,
you should definitely be aware of the implications and what you can and can't
do.

We, the go community, should probably come up with some coherent definition of
what changes we deem backwards compatible and which we don't. A tool to
automatically looks up all your (public) importerts on
[godoc.org](https://godoc.org/), downloads the latest version and tries to
build them with your changes should be fairly simple to write in go (and may
even already exist). We should make it a standard check (like go vet and
golint) for upstream package authors to do that kind of thing before push to
prevent frustrated downstreams.

Of course there is still the possibility, that my reading of the go1
compatibility promise is wrong or inaccurate. I would welcome comments on that,
just like on everything else in this post :)
]]></content:encoded>
      <dc:date>2015-07-29T01:10:11+00:00</dc:date>
    </item>
    <item>
      <title>Lazy evaluation in go</title>
      <link>https://blog.merovius.de//2015/07/17/lazy-evaluation-in-go.html</link>
      <description><![CDATA[tl;dr: I did lazy evaluation in go
]]></description>
      <pubDate>Fri, 17 Jul 2015 19:31:10 +0000</pubDate>
      <guid>https://blog.merovius.de//2015/07/17/lazy-evaluation-in-go.html</guid>
      <content:encoded><![CDATA[**tl;dr: I did [lazy evaluation in go](https://godoc.org/merovius.de/go-misc/lazy)**

A small pattern that is usefull for some algorithms is [lazy
evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation). Haskell is famous
for making extensive use of it. One way to emulate goroutine-safe lazy
evaluation is using closures and [the sync-package](https://godoc.org/sync):

```go
type LazyInt func() int

func Make(f func() int) LazyInt {
	var v int
	var once sync.Once
	return func() int {
		once.Do(func() {
			v = f()
			f = nil // so that f can now be GC'ed
		})
		return v
	}
}

func main() {
	n := Make(func() { return 23 }) // Or something more expensive…
	fmt.Println(n())                // Calculates the 23
	fmt.Println(n() + 42)           // Reuses the calculated value
}
```

This is not the fastest possible code, but it already has less overhead than
one would think (and it is pretty simple to deduce a faster implementation from
this). I have implemented a [simple command](https://godoc.org/merovius.de/go-misc/cmd/go-lazy),
that generates these implementations (or rather, more optimized ones based on
the same idea) for different
[types](https://godoc.org/merovius.de/go-misc/lazy).

This is of course just the simplest use-case for lazynes. In practice, you might also want Implementations of Expressions

```go
func LazyAdd(a, b LazyInt) LazyInt {
	return Make(func() { return a() + b() })
}
```

or lazy slices (slightly more complicated to implement, but possible) but I
left that for a later improvement of the package (plus, it makes the already
quite big API even bigger) :)
]]></content:encoded>
      <dc:date>2015-07-17T19:31:10+00:00</dc:date>
    </item>
    <item>
      <title>SQL authentication timing side-channels</title>
      <link>https://blog.merovius.de//2015/04/13/difficulties-making-sql-based-au.html</link>
      <description><![CDATA[I&#39;ve been thinking about how to do an authentication scheme, that uses some
kind of relational database (it doesn&#39;t matter specifically, that the database
is relational, the concerns should pretty much apply to all databases) as a
backing store, in a way that is resilient against timing side-channel attacks
and doesn&#39;t leak any data about which usernames exist in the system and which
don&#39;t.
]]></description>
      <pubDate>Mon, 13 Apr 2015 02:49:53 +0000</pubDate>
      <guid>https://blog.merovius.de//2015/04/13/difficulties-making-sql-based-au.html</guid>
      <content:encoded><![CDATA[I've been thinking about how to do an authentication scheme, that uses some
kind of relational database (it doesn't matter specifically, that the database
is relational, the concerns should pretty much apply to all databases) as a
backing store, in a way that is resilient against timing side-channel attacks
and doesn't leak any data about which usernames exist in the system and which
don't.

The first obvious thing is, that you need to do a constant time comparison of
password-hashes. Luckily, most modern crypto libraries should include something
like that (at least go's bcrypt implementation comes with that).

But now the question is, how you prevent enumerating users (or checking for
existence). A naive query will return an empty result set if the user does not
exists, so again, obviously, you need to compare against *some* password, even
if the user isn't found. But just doing, for example

```go
if result.Empty {
	// Compare against a prepared hash of an empty password, to have constant
	// time check.
	bcrypt.CompareHashAndPassword(HashOfEmptyPassword, enteredPassword)
} else {
	bcrypt.CompareHashAndPassword(result.PasswordHash, enteredPassword)
}
```

won't get you very far. Because (for example) the CPU will predict either of
the two branches (and the compiler might or might not decide to "help" with
that), so again an attacker might be able to distinguish between the two cases.
The best way, to achieve resilience against timing side-channels is to make
sure, that your control flow does not depend on input data *at all*. Meaning no
branch or loop should ever take in any way into account, what is actually input
into your code (including the username and the result of the database query).

So my next thought was to modify the query to return the hash of an empty
password as a default, if no user is found. That way, your code is guaranteed
to always get a well-defined bcrypt-hash from the database and your control
flow does not depend on whether or not the user exists (and an empty password
can be safely excluded in advance, as returning early for that does not give
any new data to the attacker).

Which sounds well, but now the question is, if maybe the timing *of your
database query* tells the attacker something. And this is where I hit a
roadblock: If the attacker knows enough about your code (i.e. what database
engine you are using, what machine you are running on and what kind of indices
your database uses) they can potentially enumerate users by timing your
database queries. To illustrate: If you would use a simple linear list as an
index, a failed search has to traverse the whole list, whereas a successfull
search will abort early. The same issue exists with balanced trees. An attacker
could potentially hammer your application with unlikely usernames and measure
the mean time to answer. They can then test individual usernames and measure if
the time to answer is significantly below the mean for failures, thus
enumerating usernames.

Now, I haven't tested this for practicality yet (might be fun) and it is pretty
likely that this can't be exploited in reality. Also, the possibility of
enumerating users isn't particularly desirable, but it is also far from a
security meltdown of your authentication-system. Nevertheless, the idea that
this theoretical problem exists makes me uneasy.

An obvious fix would be to make sure, that every query always has to search
the complete table on every lookup. I don't know if that is possible, it might
be just trivial by not giving a limit and not marking the username column as
unique, but it might also be hard and database-dependent because there will
still be an index over this username column which might still create the same
kind of issues. There will also likely still be a variance, because we
basically just shifted the condition from our own code into the DBMS. I have
simply no idea.

So there you have it. I am happy to be corrected and pointed to some trivial
design. I will likely accept the possibity of being vulnerable here, as the
systems I am currently building aren't that critical. But I will probably still
have a look at how other projects are handling this. And maybe if there really
is a problem in practice.
]]></content:encoded>
      <dc:date>2015-04-13T02:49:53+00:00</dc:date>
    </item>
    <item>
      <title>The four things I miss about go</title>
      <link>https://blog.merovius.de//2014/09/12/the-four-things-i-miss-from-go.html</link>
      <description><![CDATA[As people who know me know, my current favourite language is
go. One of the best features of go is the lack of
features. This is actually the reason I preferred C over most scripting
languages for a long time – it does not overburden you with language-features
that you first have to wrap your head around. You don&#39;t have to think for a
while about what classes or modules or whatever you want to have, you just
write your code down and the (more or less) entire language can easily fit
inside your head. One of the best writeups of this (contrasting it with python)
was done by Gustavo Niemeyer in a
blogpost
a few years back.
]]></description>
      <pubDate>Fri, 12 Sep 2014 17:10:28 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/09/12/the-four-things-i-miss-from-go.html</guid>
      <content:encoded><![CDATA[As people who know me know, my current favourite language is
[go](http://golang.org/). One of the best features of go is the lack of
features. This is actually the reason I preferred C over most scripting
languages for a long time – it does not overburden you with language-features
that you first have to wrap your head around. You don't have to think for a
while about what classes or modules or whatever you want to have, you just
write your code down and the (more or less) entire language can easily fit
inside your head. One of the best writeups of this (contrasting it with python)
was done by Gustavo Niemeyer in a
[blogpost](http://blog.labix.org/2012/06/26/less-is-more-and-is-not-always-straightforward)
a few years back.

So when I say, there are a few things popping up I miss about go, this does not
mean I wish them to be included. I subjectively miss them and it would
definitely make me happy, if they existed. But I still very much like the go
devs for prioritizing simplicity over making me happy.

So let's dig in.

1. [Generics](#generics)
2. [Weak references](#weakrefs)
3. [Dynamic loading of go code](#dynload)
4. [Garbage-collected goroutines](#gcgoroutines)

<a name="generics"></a>
#### Generics

So let's get this elephant out of the room first. I think this is the most
named feature lacking from go. They are asked so often, they have their own
entry in the [go FAQ](http://golang.org/doc/faq#generics). The usual answers
are anything from "maybe they will get in" to "I don't understand why people
want generics, go has generic programming using interfaces". To illustrate one
shortcoming of the (current) interface approach, consider writing a (simple)
graph-algorithm:

```go
type Graph [][]int

func DFS(g Graph, start int, visitor func(int)) {
	visited := make([]bool, len(g))

	var dfs func(int)
	dfs = func(i int) {
		if visited[i] {
			return
		}
		visitor(i)
		visited[i] = true
		for _, j := range g[i] {
			dfs(j)
		}
	}

	dfs(start)
}
```

This uses an adjacency list to represent the graph and does a recursive
depth-first-search on it. Now imagine, you want to implement this algorithm
generically (given, a DFS is not really hard enough to justify this, but you
could just as easily have a more complex algorithm). This could be done like this:

```go
type Node interface{}

type Graph interface {
	Neighbors(Node) []Node
}

func DFS(g Graph, start Node, visitor func(Node)) {
	visited := make(map[Node]bool)

	var dfs func(Node)
	dfs = func(n Node) {
		if visited[n] {
			return
		}
		visitor(n)
		visited[n] = true
		for _, n2 := range g.Neighbors(n) {
			dfs(n2)
		}
	}

	dfs(start)
}
```

This seems simple enough, but it has a lot of problems. For example, we loose
type-safety: Even if we write `Neighbors(Node) []Node` there is no way to tell
the compiler, that these instances of `Node` will actually always be the same.
So an implementation of the graph interface would have to do type-assertions
all over the place. Another problem is:

```go
type AdjacencyList [][]int

func (l AdjacencyList) Neighbors(n Node) []Node {
	i := n.(int)
	var result []Node
	for _, j := range l[i] {
		result = append(result, j)
	}
	return result
}
```

An implementation of this interface as an adjacency-list actually performs
pretty badly, because it can not return an `[]int`, but must return a `[]Node`,
and even though `int` satisfies `Node`, `[]int` is not assignable to `[]Node`
(for good reasons that lie in the implementation of interfaces, but still).

The way to solve this, is to always map your nodes to integers. This is what
the standard library does in the
[sort-package](http://golang.org/pkg/sort/#Interface). It is exactly the same
problem. But it might not always be possible, let alone straightforward, to do
this for Graphs, for example if they do not fit into memory (e.g. a
web-crawler). The answer is to have the caller maintain this mapping via a
`map[Node]int` or something similar, but… meh.

<a name="weakrefs"></a>
#### Weak references

I have to admit, that I am not sure, my use case here is really an important or
even very nice one, but let's assume I want to have a database abstraction that
transparently handles pointer-indirection. So let's say I have two tables T1
and T2 and T2 has a foreign key referencing T1. I think it would be pretty
neat, if a database abstraction could automatically deserialize this into a
pointer to a T1-value `A`. But to do this. we would a) need to be able to
recognize `A` a later Put (so if the user changes `A` and later stores it, the
database knows what row in T1 to update) and b) hand out the *same* pointer, if
another row in T2 references the same id.

The only way I can think how to do this is to maintain a `map[Id]*T1` (or
similar), but this would prevent the handed out values to ever be
garbage-collected. Even though there a
[hacks](https://groups.google.com/forum/#!topic/golang-nuts/1ItNOOj8yW8/discussion)
that would allow some use cases for weak references to be emulated, I don't see
how they would work here.

So, as in the case of generics, this mainly means that some elegant APIs are
not possible in go for library authors (and as I said, in this specific case it
probably isn't a very good idea. For example you would have to think about what
happens, if the user gets the same value in two different goroutines from the
database).

<a name="dynload"></a>
#### Dynamic loading of go code

It would be useful to be able to dynamically load go code at runtime, to build
plugins for go software. Specifically I want a good go replacement for
[jekyll](http://jekyllrb.com/) because I went through some ruby-version-hell
with it lately (for example `jekyll serve -w` still does not work for me with
the version packaged in debian) and I think a statically linked go-binary would
take a lot of possible pain-points out here. But plugins are a really important
feature of jekyll for me, so I still want to be able to customize a page with
plugins (how to avoid introducing the same version hell with this is another
topic).

The currently recommended ways to do plugins are a) as go-packages and
recompiling the whole binary for every change of a plugin and b) using
sub-processes and [net/rpc](http://golang.org/pkg/net/rpc).

I don't feel a) being a good fit here, because it means maintaining a separate
binary for every jekyll-site you have which just sounds like a smallish
nightmare for binary distributions (plus I have use cases for plugins where even
the relatively small compilation times of go would result in an intolerable
increase in startup-time).

b) on the other hand results in a lot of runtime-penalty: For example I can not
really pass interfaces between plugins, let alone use channels or something and
every function call has to have its parameters and results serialized and
deserialized.  Where in the same process I can just define a transformation
between different formats as a `func(r io.Reader) io.Reader` or something, in
the RPC-context I first have to transmit the entire file over a socket, or have
the plugin-author implement a `net/rpc` server himself and somehow pass a
reference to it over the wire. This increases the burden on the plugin-authors
too much, I think.

Luckily, it seems there seems to be
[some thought](https://groups.google.com/forum/#!topic/golang-dev/0_N7DLmrUFA)
put forward recently on how to implement this, so maybe we see this in the
nearish future.

<a name="gcgoroutines"></a>
#### Garbage-collected goroutines

Now, this is the only thing I really don't understand why it is not part of the
language. Concurrency in go is a first-class citizen and garbage-collection is
a feature emphasized all the time by the go-authors as an advantage. Yet, they
both seem to not play entirely well together, making concurrency worse than it
has to be.

Something like the standard example of how goroutines and channels work goes a
little bit like this:

```go
func Foo() {
	ch := make(chan int)
	go func() {
		i := 0
		for {
			ch <- i
			i++
		}
	}()

	for {
		fmt.Println(<-ch)
	}
}
```

Now, this is all well, but what if we want to exit the loop prematurely? We
have to do something like this:

```go
func Foo() {
	ch := make(chan int)
	done := make(chan bool)
	go func() {
		i := 0
		for {
			select {
				case ch <- i:
					i++
				case <-done:
					return
			}
		}
	}()
	for {
		i := <-ch
		if i > 1000 {
			break
		}
		fmt.Println(i)
	}
}
```

Because otherwise the goroutine would just stay around for all eternity,
effectively being leaked memory. There are
[entire](http://youtu.be/f6kdp27TYZs) [talks](http://youtu.be/QDDwwePbDtw)
build around this and similar problems, where I don't really understand why. If
we add a `break` to our first version, `Foo` returns and suddenly, all other
references to `ch`, except the one the goroutine is blocking on writing to are
gone and can be garbage-collected. The runtime can already detect if all
goroutines are sleeping and we have a deadlock, the garbage-collector can
accurately see what references there are to a given channel, why can we not
combine the two to just see "there is absolutely *no* way, this channel-write
can *ever* succeed, so let's just kill it and gc all it's memory"? This would
have zero impact on existing programs (because as you can not get any
references to goroutines, a deadlocked one can have no side-effect on the rest
of the program), but it would make channels *so* much more fun to work with. It
would make channels as iterators a truly elegant pattern, it would simplify
[pipelines](http://blog.golang.org/pipelines) and it would possibly allow a
myriad other use cases for channels I can not think of right now. Heck, you
could even think about (not sure if this is possible or desirable) running any
deferred statements, when a goroutine is garbage-collected, so all other
resources held by it will be correctly released.

This is the *one* thing I really wish to be added to the language. Really
diving into channels and concurrency right now is very much spoiled for me
because I always have to think about draining every channel, always think about
what goroutine closes what channels, passing cancellation-channels…
]]></content:encoded>
      <dc:date>2014-09-12T17:10:28+00:00</dc:date>
    </item>
    <item>
      <title>Applying permutation in constant space (and linear time)</title>
      <link>https://blog.merovius.de//2014/08/12/applying-permutation-in-constant.html</link>
      <description><![CDATA[I stumbled upon a mildly interesting problem yesterday: Given an Array a and a
permutation p, apply the permutation (in place) to the Array, using only O(1)
extra space.  So, if b is the array after the algorithm, we want that
a[i] == b[p[i]].
]]></description>
      <pubDate>Tue, 12 Aug 2014 11:10:21 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/08/12/applying-permutation-in-constant.html</guid>
      <content:encoded><![CDATA[I stumbled upon a mildly interesting problem yesterday: Given an Array a and a
permutation p, apply the permutation (in place) to the Array, using only O(1)
extra space.  So, if b is the array after the algorithm, we want that
`a[i] == b[p[i]]`.

Naively, we would solve our problem by doing something like this (I'm using go
here):

```go
func Naive(vals, perm []int) {
	n := len(vals)
	res := make([]int, n)
	for i := range vals {
		res[perm[i]] = vals[i]
	}
	copy(vals, res)
}
```

This solves the problem in O(n) time, but it uses of course O(n) extra space
for the result array. Note also, that it does not really work in place, we have
to copy the result back.

The simplest iteration of this, would be to simply use a sorting-algorithm of
our choice, but use as a sorting key not the value of the elements, but the
position of the corresponding field in the permutation array:

```go
import "sort"

type PermSorter struct {
	vals []int
	perm []int
}

func (p PermSorter) Len() int {
	return len(p.vals)
}

func (p PermSorter) Less(i, j int) bool {
	return p.perm[i] < p.perm[j]
}

func (p PermSorter) Swap(i, j int) {
	p.vals[i], p.vals[j] = p.vals[j], p.vals[i]
	p.perm[i], p.perm[j] = p.perm[j], p.perm[i]
}

func Sort(vals, perm []int) {
	sort.Sort(PermSorter{vals, perm})
}
```

This appears a promising idea at first, but as it turns out, this doesn't
*really* use constant space after all (at least not generally). The go sort
package uses introsort internally, which is a combination of quick- and
heapsort, the latter being chosen if the recursion-depth of quicksort exceeds a
limit in O(log(n)). Thus it uses actually O(log(n)) auxiliary space. Also, the
running time of sorting is O(n log(n)) and while time complexity wasn't part of
the initially posed problem, it would actually nice to have linear running
time, if possible.

Note also another point: The above implementation sorts perm, thus destroying
the permutation array. Also not part of the original problem, this might pose
problems if we want to apply the same permutation to multiple arrays. We can
rectify that in this case by doing the following:

```go
type NDPermSorter struct {
	vals []int
	perm []int
}

func (p NDPermSorter) Len() int {
	return len(p.vals)
}

func (p NDPermSorter) Less(i, j int) bool {
	return p.perm[p.vals[i]] < p.perm[p.vals[j]]
}

func (p NDPermSorter) Swap(i, j int) {
	p.vals[i], p.vals[j] = p.vals[j], p.vals[i]
}

func NDSort(vals, perm []int) {
	sort.Sort(NDPermSorter{vals, perm})
}
```

But note, that this only works, because we want to sort an array of consecutive
integers. In general, we don't want to do that. And I am unaware of a solution
that doesn't have this problem (though I also didn't think about it a lot).

The solution of solving this problem in linear time lies in a simple
observation: If we start at any index and iteratively jump to the *target*
index of the current one, we will trace out a cycle. If any index is not in the
cycle, it will create another cycle and both cycles will be disjoint. For
example the permutation

```text
i    0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19
p[i] 2  13 1  5  3  15 14 12 8  10 4  19 16 11 9  7  18 6  17 0
```

will create the following set of cycles:
<a href="/assets/permutation.svg"><img src="/assets/permutation.svg"></a>

So the idea is to resolve every cycle separately, by iterating over the indices
and moving every element to the place it belongs:

```go
func Cycles(vals, perm []int) {
	for i := 0; i < len(vals); i++ {
		v, j := vals[i], perm[i]
		for j != i {
			vals[j], v = v, vals[j]
			perm[j], j = j, perm[j]
		}
		vals[i], perm[i] = v, i
	}
}
```

This obviously only needs O(1) space. The secret, why it also only uses O(n)
time lies in the fact, that the inner loop will not be entered for elements,
that are already at the correct position. Thus this is (from a complexity
standpoint at least) the optimal solution to the problem, as it is impossible
to use *less* than linear time for applying a permutation.

There is still one small problem with this solution: It also sorts the
permutation array. We need this, to know when a position is already occupied by
it's final element. In our algorithm this is represented by the fact, that the
permutation is equal to it's index at that point. But really, it would be nice
if we could mark the index *without* losing the order of the permutation. But
that is not hard either - because every index is non-negative, we can
simply negate every index we are done with. This will make a negative index out
of it and we can check for that if we encounter it later and skip it in this
case. After we are done, we only need to take care to flip everything back and
all should be fine:

```go
func NDCycles(vals, perm []int) {
	for i := 0; i < len(vals); i++ {
		if perm[i] < 0 {
			// already correct - unmark and go on
			// (note that ^a is the bitwise negation
			perm[i] = ^perm[i]
			continue
		}

		v, j := vals[i], perm[i]
		for j != i {
			vals[j], v = v, vals[j]
			// When we find this element in the future, we must not swap it any
			// further, so we mark it here
			perm[j], j = ^perm[j], perm[j]
		}
		vals[i] = v
	}
}
```

Here we only mark the elements we will again encounter in the *future*. The
current index will always be unmarked, once we are done with the outer loop.

I am aware, that this is technically cheating; This solution relies on the
fact, that the upper-most bit of the permutation elements won't ever be set.
Thus, we actually *do* have O(n) auxiliary space (as in n bit), because these
bits are not necessary for the algorithm. However, since it is pretty unlikely,
that we will find an architecture where this is not possible (and go guarantees
us that it actually is, because len(vals) is *always* signed, so we cant have
arrays that are big enough for the msb being set anyway), I think I am okay
with it ;)

I ran sum Benchmarks on this an these are the figures I came up with:

<table class="highlight">
	<thead>
		<tr>
			<th>n</th>
			<th>Naive</th>
			<th>NDCycle</th>
			<th>NDSort</th>
		</tr>
	</thead>

	<tbody>
		<tr>
			<td>10</td>
			<td>332 ns</td>
			<td>130 ns</td>
			<td>1499 ns</td>
		</tr>
		<tr>
			<td>100</td>
			<td>883 ns</td>
			<td>1019 ns</td>
			<td>27187 ns</td>
		</tr>
		<tr>
			<td>1000</td>
			<td>15046 ns</td>
			<td>17978 ns</td>
			<td>473078 ns</td>
		</tr>
		<tr>
			<td>10000</td>
			<td>81800 ns</td>
			<td>242121 ns</td>
			<td>4659433 ns</td>
		</tr>
	</tbody>
</table>

I did not measure space-use. The time of NDCycle for 10000 elements seems
suspicious - while it is not surprising, that in general it takes more time
than the naive approach, due to it's complexity, this jump is unexpected. Maybe
if I have the time I will investigate this and also measure memory use. In the
meantime, I
[uploaded](https://gist.github.com/Merovius/9e31f4dc6a42a78c1942) all the
code used here, so you can try it out yourself. You can run it with `go run
perm.go` and run the benchmarks with `go test -bench Benchmark.*`.
]]></content:encoded>
      <dc:date>2014-08-12T11:10:21+00:00</dc:date>
    </item>
    <item>
      <title>GPN14 GameJam - Crazy cat lady</title>
      <link>https://blog.merovius.de//2014/06/22/gpn14-gamejam-crazy-cat-lady.html</link>
      <description><![CDATA[tl:dr: We made a gamejam-game
]]></description>
      <pubDate>Sun, 22 Jun 2014 13:45:28 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/06/22/gpn14-gamejam-crazy-cat-lady.html</guid>
      <content:encoded><![CDATA[**tl:dr: We made a [gamejam-game](https://entropia.de/GPN14:GameJam:CrazyCatLady)**

At the GPN14 we (meaning me and Lea, with a bit of help by sECuRE) participated
in the [gamejam](https://entropia.de/GPN14:GameJam). It was the first time for
us both, I did all the coding and Lea provided awesome graphics.

The [result](https://entropia.de/GPN14:GameJam:CrazyCatLady) is a small
minigame “crazy cat lady”, where you throw cats at peoples faces and - if you
hit - scratch the shit out of them (by hammering the spacebar). The game
mechanics are kind of limited, but the graphics are just epic, in my opinion:

![Screenshot]({{ site.url }}/assets/crazycatlady1.png)

Because sounds make every game 342% more awesome, we added a creative commons
licensed
[background-music](http://freemusicarchive.org/music/fp/traces/05_fp_-_trace_5).
We also wanted some cat-meowing and very angry pissed of hissing, which was
more of a problem to come by. Our solution was to just wander about the GPN and
asking random nerds to make cat sounds and recording them. That gave a pretty
cool result, if you ask me.

On the technical side we used [LÖVE](https://love2d.org/), an open 2D game
engine for lua, widely used in gamejams. I am pretty happy with this engine,
it took about 3 hours to get most of the game-mechanics going, the rest was
pretty much doing the detailed animations. It is definitely not the nicest or
most efficient code, but for a gamejam it is a well suited language and engine.

If you want to try it (I don't think it is interesting for more than a few
minutes, but definitely worth checking out), you should install LÖVE (most
linux-distributions should have a package for that) and just
[download](http://merovius.de/crazycatlady.love) it, or check out the
[sourcecode](https://github.com/Merovius/crazycatlady).

We did not make first place, but that is okay, the game that won is a nice game
and a deserved winner. We had a lot of fun and we are all pretty happy with the
result as first-timers.
]]></content:encoded>
      <dc:date>2014-06-22T13:45:28+00:00</dc:date>
    </item>
    <item>
      <title>Python-fnord of the day</title>
      <link>https://blog.merovius.de//2014/05/06/python-fnord-of-the-day.html</link>
      <description><![CDATA[This is an argument of believe, but I think this is highly irregular and
unexpected behaviour of python:
]]></description>
      <pubDate>Tue, 06 May 2014 01:07:28 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/05/06/python-fnord-of-the-day.html</guid>
      <content:encoded><![CDATA[This is an argument of believe, but I think this is highly irregular and
unexpected behaviour of python:

```python
a = [1, 2, 3, 4]
b = a
a = a + [5]
print(b)
a = [1, 2, 3, 4]
b = a
a += [5]
print(b)
```

Output:

```
[1, 2, 3, 4]
[1, 2, 3, 4, 5]
```

Call me crazy, but in my world, `x += y` should behave exactly the same as `x =
x + y` and this is another example, why operator overloading can be abused in
absolutely horrible ways.

Never mind, that there is actually python [teaching
material](http://www.tutorialspoint.com/python/python_basic_operators.htm) [out
there](http://www.rafekettler.com/magicmethods.html#numeric) that teaches wrong
things. That is, there are actually people out there who think they know python
well enough to teach it, but don't know this. Though credit where credit is
due, the [official documentation](https://docs.python.org/2/reference/simple_stmts.html#augmented-assignment-statements)
mentions this behaviour.
]]></content:encoded>
      <dc:date>2014-05-06T01:07:28+00:00</dc:date>
    </item>
    <item>
      <title>Heartbleed: New certificates</title>
      <link>https://blog.merovius.de//2014/04/10/heartbleed-new-certificates.html</link>
      <description><![CDATA[Due to the Heartbleed vulnerability I had to recreate all TLS-keys of my
server. Since CACert appears to be mostly dead (or dying at least), I am
currently on the lookout for a new CA. In the meantime I switched to
self-signed certificates for all my services.
]]></description>
      <pubDate>Thu, 10 Apr 2014 21:28:25 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/04/10/heartbleed-new-certificates.html</guid>
      <content:encoded><![CDATA[Due to the Heartbleed vulnerability I had to recreate all TLS-keys of my
server. Since CACert appears to be mostly dead (or dying at least), I am
currently on the lookout for a new CA. In the meantime I switched to
self-signed certificates for all my services.

The new fingerprints are:
<table>
	<tr>
		<th>Service</th>
		<th>SHA1-Fingerprint</th>
	</tr>
	<tr>
		<td>merovius.de</td>
		<td>8C:85:B1:9E:37:92:FE:C9:71:F6:0E:C6:9B:25:9C:CD:30:2B:D5:35</td>
	</tr>
	<tr>
		<td>blog.merovius.de</td>
		<td>1B:DB:45:11:F3:EE:66:8D:3B:DF:63:B9:7C:D9:FC:26:A4:D1:E1:B8</td>
	</tr>
	<tr>
		<td>git.merovius.de</td>
		<td>65:51:16:25:1A:9E:50:B2:F7:D7:8A:2B:77:DE:DE:0C:02:3C:6C:ED</td>
	</tr>
	<tr>
		<td>smtp (mail.merovius.de)</td>
		<td>1F:E5:3F:9D:EE:B4:47:AE:2E:02:D8:2C:1E:2A:6C:FC:D6:62:99:F4</td>
	</tr>
	<tr>
		<td>jabber (merovius.de)</th>
		<td>15:64:29:49:82:0E:8B:76:47:1A:19:5B:98:6F:E4:56:24:D9:69:07</td>
	</tr>
</table>

This is of course useless in the general case, but if you already trust my
gpg-key, you can use

```sh
curl http://blog.merovius.de/2014/04/10/heartbleed-new-certificates.html | gpg
```

to get this post signed and verified.
]]></content:encoded>
      <dc:date>2014-04-10T21:28:25+00:00</dc:date>
    </item>
    <item>
      <title>go stacktraces</title>
      <link>https://blog.merovius.de//2014/02/19/go-stacktraces.html</link>
      <description><![CDATA[Let&#39;s say you write a library in go and want an easy way
to get debugging information from your users. Sure, you return errors from
everything, but it is sometimes hard to pinpoint where a particular error
occured and what caused it. If your package panics, that will give you a
stacktrace, but as you probably know you shouldn&#39;t panic in case of an error,
but just gracefull recover and return the error to your caller.
]]></description>
      <pubDate>Wed, 19 Feb 2014 02:17:59 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/02/19/go-stacktraces.html</guid>
      <content:encoded><![CDATA[Let's say you write a library in [go](http://golang.org/) and want an easy way
to get debugging information from your users. Sure, you return `error`s from
everything, but it is sometimes hard to pinpoint where a particular error
occured and what caused it. If your package `panic`s, that will give you a
stacktrace, but as you probably know you shouldn't `panic` in case of an error,
but just gracefull recover and return the error to your caller.

I recently discovered a pattern which I am quite happy with (for now). You can
include a stacktrace when returning an error. If you disable this behaviour by
default you should have as good as no impact for normal users, while making it
much easier to debug problems. Neat.

```
package awesomelib

import (
	"os"
	"runtime"
)

type tracedError struct {
	err   error
	trace string
}

var (
	stacktrace bool
	traceSize = 16*1024
)

func init() {
	if os.Getenv("AWESOMELIB_ENABLE_STACKTRACE") == "true" {
		stacktrace = true
	}
}

func wrapErr(err error) error {
	// If stacktraces are disabled, we return the error as is
	if !stacktrace {
		return err
	}

	// This is a convenience, so that we can just throw a wrapErr at every
	// point we return an error and don't get layered useless wrappers
	if Err, ok := err.(*tracedError); ok {
		return Err
	}

	buf := make([]byte, traceSize)
	n := runtime.Stack(buf, false)
	return &tracedError{ err: err, trace: string(buf[:n]) }
}

func (err *tracedError) Error() string {
	return fmt.Sprintf("%v\n%s", err.err, err.trace)
}

func DoFancyStuff(path string) error {
	file, err := os.Open(path)
	if err != nil {
		return wrapErr(err)
	}
	// fancy stuff
}
```
]]></content:encoded>
      <dc:date>2014-02-19T02:17:59+00:00</dc:date>
    </item>
    <item>
      <title>Signed blog posts</title>
      <link>https://blog.merovius.de//2014/01/23/signed-blog-posts.html</link>
      <description><![CDATA[tl;dr: I sign my blog posts. curl
http://blog.merovius.de/2014/01/23/signed-blog-posts.html | gpg
]]></description>
      <pubDate>Thu, 23 Jan 2014 04:04:25 +0000</pubDate>
      <guid>https://blog.merovius.de//2014/01/23/signed-blog-posts.html</guid>
      <content:encoded><![CDATA[**tl;dr: I sign my blog posts. curl
http://blog.merovius.de/2014/01/23/signed-blog-posts.html | gpg**

I might have to update my TLS server certificate soon, because the last change
seems to have broken the verification of https://merovius.de/. This is nothing
too exciting, but it occured to me that I should actually provide some warning
or notice in that case, so that people can be sure, that there is nothing
wrong. The easiest way to accomplish this would be a blogpost and the easiest
way to verify that the statements in that blogpost are correct would be, to
provide a signed version. So because of this (and, well, because I can) I
decided to sign all my blogposts with my gpg-key. People who know me should
have my gpg key so they can verify that I really have written everything I
claim.

I could have used
[jekyll-gpg_clearsign](https://github.com/kormoc/jekyll-gpg_clearsign), but it
does not really do the right thing in my opinion. It wraps all the HTML in a
GPG SIGNED MESSAGE block and attaches a signature. This has the advantage of
minimum overhead - you only add the signature itself plus some constant
comments of overhead. However, it makes really verifying the contents of a
blogpost pretty tedious: You would have to either manually parse the HTML in
your mind, or you would have to save it to disk and view it in your browser,
because you cannot be sure, that the HTML you get when verifying it via curl on
the commandline is the same you get in your browser. You could write a
browser-extension or something similar that looks for these blocks, but still,
the content could be tempered with (for example: Add the correctly signed page
as a comment in a tampered with page. Or try to somehow include some javascript
that changes the text after verifying…). Also, the generated HTML is not really
what I want to sign; after all I can not really attest that the HTML-generation
is really solid and trustworthy, I never read the jekyll source-code and I
don't want to, at every update. What I really want to sign is the stuff I wrote
myself, the markdown (or whatever) I put into the post. This has the additional
advantage, that most markdown is easily parseable by humans, so you can
actually have your gpg output the signed text and immediately read everything I
wrote.

So this is, what happens now. In every blogpost there is a HTML-comment
embedded, containing the original markdown I wrote for this post in compressed,
signed and ASCII-armored form. You can try it via

	curl http://blog.merovius.de/2014/01/23/signed-blog-posts.html | gpg

This should output some markdown to stdout and a synopsis of gpg about a valid
(possibly untrusted, if you don't have my gpg-key) signature on stderr. Neat!

The [changes](http://git.merovius.de/blog/commit/?id=dd005159f9fb25ebc8ef789608a609bcb65fc62c)
needed in the blog-code itself where pretty minimal. I had however (since I
don't want my gpg secret key to be on the server) to change the deployment a
little bit. Where before a git push would trigger a hook on the remote
repository on my server that ran jekyll, now I have a local script, that wraps
a jekyll build, an rsync to the webserver-directory and a git push. gpg-agent
ensures, that I am not asked for a passphrase too often.

So, yeah. Crypto is cool. And the procrastinator prevailed again!
]]></content:encoded>
      <dc:date>2014-01-23T04:04:25+00:00</dc:date>
    </item>
    <item>
      <title>Incentives in education</title>
      <link>https://blog.merovius.de//2013/12/16/incentives-in-education.html</link>
      <description><![CDATA[tl;dr: I hate software-engineering as it is teached in Heidelberg. Really
]]></description>
      <pubDate>Mon, 16 Dec 2013 01:32:47 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/12/16/incentives-in-education.html</guid>
      <content:encoded><![CDATA[**tl;dr: I hate software-engineering as it is teached in Heidelberg. Really**

I often recited the story of how I got to choose computer science over physics
as a minor in my mathematics bachelor:

After sitting through almost one semester of the introductory course to
theoretical physics in my 3rd semester — which is incredibly unsatisfactory
and boring, once you are past your first year of mathematics — I suddenly
realized that my reward for suffering through yet another problem sheet of
calculating yet another set of differential operators is, that I have to suffer
through four or five more of these type of courses. This really seemed like a
poor incentive, when I was just discovering hacking and that I was really good
at computer science. So I decided to pass on the opportunity, did not work all
night on that last sheet (and later found out that I would have gotten credit
for that course without even taking the written exam if I just handed in this
additional problem sheet) and instead decided to minor in computer science.

Three years after that I decided to get a second bachelor degree in computer
science (I finished my bachelor of mathematics earlier that year and was
pursuing my master degree at that point), because it seemed a really easy thing
to do at that point: I only needed two semesters more of studies and a bachelor
thesis. That is not a lot of work for a degree. We are now one year and some
change after that point, and there really is not a lot I need anymore.
Basically I only need to finish the introduction to software engineering and
then write my thesis. Yay for me.

The reason I write this (and the reason I started with the anecdote of
physics) is that once again I am questioning the incentives versus the cost.
Since I am pretty sure that it would actually be fun to write my thesis, it all
boils down to the question, whether I want to finish this course (which again,
I'm more than halfway done with, it is not a lot work to go) to get a bachelor
degree in computer science. And don't get me wrong — I'm sure that software
engineering is a topic, that can be interesting and captivating, or at the
minimum bearable. But the way it is done here in Heidelberg is just hell. It is
incredibly boringly presented and consists of a flood of uninteresting
repetitive tasks and the project-work, to show how important teamwork and
quality-assurance and drawing a **lot** of diagrams is, is a catastrophically
bad, unusable and ugly piece of crapware, that can't even decently perform the
very simple task it was designed to (managing a private movie collection. I
mean, come on, it is not exactly rocket science to do this in at least a barely
usable way).

And even though it is a hell that I would only have to endure for about two or
three problem sheets and one written exam, I watch myself putting off the work
on it (for example by writing this stupid blogpost) and I seriously question
whether this second bachelor is really incentive enough to suffer through it.

If it was my first degree, that would of course be a clear ”yes“. But a second
one? Not sure. Ironically the main way I'm putting of work on this problem
sheet — I got up today at 10am, to immediately and energetically start to work
on it — is watching I lot of TED talks on youtube. That's right, I practically
spent 14 hours more or less non-stop watching TED talks. This one applies to
some extend — extrinsic incentives can only go this far in making us do some
work; at some point, without at least some intrinsic motivation, I at least
will not perform very well (or at all):

<div class="video-container">
  <iframe width="560" height="315" src="//www.youtube-nocookie.com/embed/rrkrvAUbU9Y" frameborder="0" allowfullscreen></iframe>
</div>
]]></content:encoded>
      <dc:date>2013-12-16T01:32:47+00:00</dc:date>
    </item>
    <item>
      <title>ext4: Mysterious “No space left on device”-errors</title>
      <link>https://blog.merovius.de//2013/10/20/ext4-mysterious-no-space-left-on.html</link>
      <description><![CDATA[tl;dr: ext4 has a feature called dir_index enabled by default, which is
quite susceptible to hash-collisions
]]></description>
      <pubDate>Sun, 20 Oct 2013 21:13:07 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/20/ext4-mysterious-no-space-left-on.html</guid>
      <content:encoded><![CDATA[**tl;dr: ext4 has a feature called `dir_index` enabled by default, which is
quite susceptible to hash-collisions**

I am currently restructuring my mail-setup. Currently, I use offlineimap to
sync my separate accounts to a series of maildirs on my server. I then use sup
on the server as a MUA. I want to switch to a local setup with notmuch, so I
set up an dovecot imapd on my server and have all my accounts forward to my
primary address. I then want to use offlineimap to have my mails in a local
maildir, which I browse with notmuch.

I then stumbled about a curious problem: When trying to copy my mails from my
server to my local harddisk, it would fail after about 50K E-mails with the
message “could not create xyz: no space left on device” (actually, offlineimap
would just hog all my CPUs and freeze my whole machine in the process, but
that's a different story). But there actually was plenty of space left.

It took me and a few friends a whole while to discover the problem. So if you
ever get this error message (using ext4) you should probably check these four
things (my issue was the last one):

#### Do you *actually* have enough space?

Use `df -h`. There is actually a very common pitfall with ext4. Let's have a look:

```
mero@rincewind ~$ df -h
Filesystem              Size  Used Avail Use% Mounted on
/dev/mapper/sda2_crypt  235G  164G   69G  71% /
...
```

If you add 164G and 69G, you get 233G, which is 2G short of the actual size.
This is about 1%, but on your system it will likely be more of 5% difference.
The reason is the distinction between "free" and "available" space. Per default
on ext4, there are about 5% of "reserved" blocks. This has two reasons: First
ext4's performance seems to take a small hit, when almost full. Secondly, it
leaves a little space for root to login and troubleshoot problems or delete
some files, when users filled their home-directory. If there was *no* space
left, it might well be, that no login is possible anymore (because of the
creation of temporary files, logfiles, history-files…). So use `tune2fs
<path_to_your_disk>` to see, if you have reserved blocks, and how many of them:

```
mero@rincewind ~$ sudo tune2fs -l /dev/mapper/sda2_crypt | grep "Reserved block"
Reserved block count:     2499541
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
```

#### Do you have too many files?

Even though you might have enough space left, it might well be, that you have
too many files. ext4 allows an enormous amount of files on any file system, but
it is limited. Checking this is easy: Just use `df -i`:

```
Filesystem               Inodes  IUsed    IFree IUse% Mounted on
/dev/mapper/sda2_crypt 15622144 925993 14696151    6% /
...
```

So as you see, that wasn't the problem with me. But if you ever have the `IUse%`
column near 100, you probably want to delete some old files (and you should
*definitely* question, how so many files could be created to begin with).

#### Do a file system check

At least some people on the internet say, that something like this has
happened to them after a crash (coincidentally my system crashed before the
problem arose. See above comments about offlineimap) and that a file system
check got rid of it. So you probably want to run `fsck -f <path_to_your_disk>`
to run such a check. You probably also want to do that from a live-system, if
you cannot unmount it (for example if it's mounted at the root-dir).

#### Do you have `dir_index` enabled?

So this is the punch line: ext4 has the possibility to hash the filenames of
its contents. This enhances performance, but has a “small” problem: ext4 does
not grow its hashtable, when it starts to fill up. Instead it returns -ENOSPC
or “no space left on device”.

ext4 uses `half_md4` as a default hashing-mechanism. If I interpret my
google-results correctly, this uses the md4-hash algorithm, but strips it to 32
bits. This is a classical example of the
[birthday-paradox](http://en.wikipedia.org/wiki/Birthday_problem): A 32 bit
hash means, that there are 4294967296 different hash values available, so if we
are fair and assume a uniform distribution of hash values, that makes it highly
unlikely to hit one specific hash. But the probability of hitting two identical
hashes, given enough filenames, is much much higher. Using the
[formula](http://en.wikipedia.org/wiki/Birthday_problem#Cast_as_a_collision_problem)
from Wikipedia we get (with about 50K files) a probability of about 25% that a
newly added file has the same hash. This is a huge probability of failure. If
on the other hand we take a 64bit hash-function the probability becomes much
smaller, about 0.00000000007%.

So if you have a lot of files in the same directory, you probably want to switch
off `dir_index`, or at least change to a different hash-algorithm. You can
check if you have `dir_index` enabled and change the hash, like this:

```
mero@rincewind ~$ sudo tune2fs -l /dev/mapper/sda2_crypt | grep -o dir_index
dir_index

#### Change the hash-algo to a bigger one
mero@rincewind ~$ sudo tune2fs -E "hash_alg=tea" /dev/mapper/sda2_crypt
#### Disable it completely
mero@rincewind ~$ sudo tune2fs -O "^dir_index"
```

Note however, that `dir_index` and `half_md4` where choices made for
performance reasons. So you might experience a performance-hit after this.

**UPDATE:** After trying it out, I realized, that the problem actually also
persists with the tea-hash. I then had a look at [the
ext4-documentation](https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#Hash_Tree_Directories)
about the topic and it seems, that the hash is only stored as 32 bits, so it
actually does not matter what hash we choose, regarding this particular
problem. So if `half_md4` is chosen [because of its better performance and
collision-resistance](http://git.whamcloud.com/?p=tools/e2fsprogs.git;a=commitdiff_plain;h=d1070d91b4de8438dc78c034283baaa19b31d25e)
it actually makes sense to leave it as the default. You can by the way easily
test and reproduce the issue by using the following on an ext4 file system:

```
for a in `seq 100000`
do
        file=`head -c 51 /dev/urandom | base64 | tr '/' '_'`
        touch $file
done
```

Curiously, this only gives me about 160 collisions on 100K files (instead of
about 10K collisions on 60K files), which would suggest, that my original
sample (meaning my mailbox) exhibits some properties that make collisions more
likely both on `half_md4` *and* `tea`.
]]></content:encoded>
      <dc:date>2013-10-20T21:13:07+00:00</dc:date>
    </item>
    <item>
      <title>Using i3 and vim to keep a set of notes at hand</title>
      <link>https://blog.merovius.de//2013/10/20/using-i3-and-vim-to-keep-a-set-o.html</link>
      <description><![CDATA[tl;dr: Put a terminal with a vim-instance in an i3-scratchpad, combine it
with autosave-when-idle and you got the perfect note keeping workflow
]]></description>
      <pubDate>Sun, 20 Oct 2013 03:45:52 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/20/using-i3-and-vim-to-keep-a-set-o.html</guid>
      <content:encoded><![CDATA[**tl;dr: Put a terminal with a vim-instance in an i3-scratchpad, combine it
with autosave-when-idle and you got the perfect note keeping workflow**

There are often occasions where I want to write something down, while not
wanting to disturb my thought-process too much or taking too much of an effort.
An example for the former would be a short TODO I suddenly remember while doing
something more important. As an example for the latter, I keep an "account" for
drinks at our local computer club, so that I don't always have to put single
coins into the register, but can just put 20€ or something in and don't have to
worry about it for a while. Combining the
[scratchpad-window](http://i3wm.org/docs/userguide.html#_scratchpad) feature of
i3 with a little vim-magic makes this effortless enough to be actually
preferable to just paying.

First of, you should map a key to `scratchpad show` in i3, for example I have
the following in my config:

```
bind Mod4+Shift+21 move scratchpad
bind Mod4+21 scratchpad show
```

I can then just use `Mod4+<backtic>` to access the scratchpad.

Now, just put a terminal in scratchpad-mode and open .notes in vim in this
terminal. By pressing the `scratchpad show` binding repeatedly, you can send it
to the background and bring it to the foreground again.

I have my current "balance" in this notes-file and during the meetings of the
computer club leave the cursor on this balance. If I take a drink, I press `^X`
decreasing my balance by one (every drink is one Euro). If I pay, say 10 Euros
into the register, I press `10^A` increasing my balance by 10.

This is already much better, but it still has one problem: I better save that
file every time I change my balance, else a crash would screw up my accounting.
Luckily, vim provides autocommands and has an event for "the user did not type
for a while". This means, that we can automatically save the file if we idled
for a few seconds, for example if we send the scratchpad window away. For this,
we put the following in our `.vimrc`:

```
" Automatically save the file notes when idle
autocmd CursorHold .notes :write
```

Now adjusting my balance is just a matter of a very short key sequence:
``<mod4>`<c-x><mod4>` ``
]]></content:encoded>
      <dc:date>2013-10-20T03:45:52+00:00</dc:date>
    </item>
    <item>
      <title>Tic Tac Toe AI</title>
      <link>https://blog.merovius.de//2013/10/19/tic-tac-toe-ai.html</link>
      <description><![CDATA[tl;dr: I wrote a simple Tic Tac Toe AI as an exercise. You can get it on
github
]]></description>
      <pubDate>Sat, 19 Oct 2013 01:58:04 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/19/tic-tac-toe-ai.html</guid>
      <content:encoded><![CDATA[**tl;dr: I wrote a simple Tic Tac Toe AI as an exercise. You can get it on
[github](https://github.com/Merovius/tictactoe)**

I am currently considering writing a basic chess AI as an exercise in AI
development and to help me analyze my own games (and hopefully get a better
chess-player just by thinking about how a machine would do it). As a small
exercise and to get some familiarity with the algorithms involved, I started
with [Tic Tac Toe](https://en.wikipedia.org/wiki/Tic_tac_toe). Because of the
limited number of games (only [255168](http://www.se16.info/hgb/tictactoe.htm))
all positions can be bruteforced very fast, which makes it an excellent
exercise, because even with a very simple
[Minimax-Algorithm](https://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves)
perfect play is possible.

[My AI](https://github.com/Merovius/tictactoe) uses exactly this algorithm (if
coded a little crude). It comes with a little TUI and a small testsuite, you
can try it like this:

```sh
$ git clone git://github.com/Merovius/tictactoe.git
$ cd tictactoe
$ make
$ make test
$ ./tictactoe
```

You will notice, that there already is no noticable delay (at least not on a
relatively modern machine), even though the AI is unoptimized and bruteforces
the whole tree of possible moves on every move.

Next I will first refactor the basic algorithm in use now, then I will probably
implement better techniques, such as limited search-depth,
[αβ-Pruning](https://en.wikipedia.org/wiki/Alpha-beta_pruning) or machine
learning. I will then think about moving on to a little more complex games (for
example Connect 4, Mill or Hex seem good choices). Then I will decide how big
the effort would be for chess and if it's worth a try.
]]></content:encoded>
      <dc:date>2013-10-19T01:58:04+00:00</dc:date>
    </item>
    <item>
      <title>Inject Environment variables into running processes</title>
      <link>https://blog.merovius.de//2013/10/11/inject-environment-variables-int.html</link>
      <description><![CDATA[tl;dr: Using gdb to manipulate a running process is fun and just the right
amount of danger to be exiting
]]></description>
      <pubDate>Fri, 11 Oct 2013 03:25:09 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/11/inject-environment-variables-int.html</guid>
      <content:encoded><![CDATA[**tl;dr: Using gdb to manipulate a running process is fun and just the right
amount of danger to be exiting**

Just to document this (a friend asked me): If you ever wanted for example to
globally change your `$PATH`, or add a global `$LD_PRELOAD` (for example to use
[insulterr](https://github.com/Merovius/insulterr) ;) ), without restarting
your session, gdb is your friend.

You can call arbitrary functions in the context of any process (that you are
priviledged to attach a debugger, it has to run under your uid or you have to
be root, see `ptrace(2)` for specifics), as long as they are linked. Almost
everything is linked to `libld`, so with enough effort this actually means
*every* function.

For example, suppose you are running [i3wm](http://i3wm.org) and want to add
`/home/user/insulterr/insulterr.so` to your `$LD_PRELOAD` in every process
started by i3:

```
$ gdb -p `pidof i3` `which i3`
<lots of output of gdb>
gdb $ call setenv("LD_PRELOAD", "/home/user/insulterr/insulterr.so")
gdb $ quit
A debugging session is active.

	Inferior 1 [process 2] will be detached.

Quit anyway? (y or n) y
Detaching from program: /usr/bin/i3, process 2
```

This is of course a terrible hack, by high standards. Things to look out for
are (off the top of my head):

* You call a function that manipulates `errno` or does some other non-reentrent
  things. If you are attaching the debugger right in the middle of a library
  call (or immediately after) this *might* make the program unhappy because it
  does not detect an error (or falsely thinks there is an error).
* You call a function that does not work in a multithreaded context and another
  thread modifies it at the same time. Bad.
* You interrupt a badly checked `read(2)`/`write(2)`/`whatever(…)` call and a
  badly written program doesn't realize it got less data then expected (and/or
  crashes).  Shouldn't happen in practice, if it does, file a bug.
* You try to use symbols that are not available. This is actually not very bad
  and can be worked around (a friend of mine had the problem of needing `false`
  and just substituted 0).
* You use a daemon (like `urxvtd(1)`) for your terminals and the environment
  does not get passed correctly. This is also not very bad, just confusing.
  Attach your debugger to the daemon and change the environment there too.
* You attach the debugger to some process vital to the interaction with your
  debugger. Your window manager is a mild example. The terminal daemon is
  slightly worse (because, well, you can't actually type in the terminal window
  that your debugger is running in, ergo you can't stop it…), but you can
  change to a virtual terminal. Something like getty or init might be killing
  it.

Have fun!
]]></content:encoded>
      <dc:date>2013-10-11T03:25:09+00:00</dc:date>
    </item>
    <item>
      <title>How to C-Golf</title>
      <link>https://blog.merovius.de//2013/10/11/how-to-cgolf.html</link>
      <description><![CDATA[tl;dr: We had a codegolf challenge recently. My C-solution was 246 byte, the
perl-winner was 191. I decided to give notes for C-golf beginners
]]></description>
      <pubDate>Fri, 11 Oct 2013 02:09:38 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/11/how-to-cgolf.html</guid>
      <content:encoded><![CDATA[**tl;dr: We had a codegolf challenge recently. My C-solution was 246 byte, the
perl-winner was 191. I decided to give notes for C-golf beginners**

**Note:** Most of this blog post is incredibly boring. A better way than to
read through it is to just skip through the git-repository and refer to the
explanations here everytime you don't know why a change works or what it
does. To make this easier, I added ankers to every paragraph, named by the
commit of the change it explains. So if you want to know, how a specific change
works, you can add `#commitid` to the url of this post, with `commitid` being
the full id of that change. If you want to read the more interesting
stuff, [from about here](#4272ca2a181e8f50c1645b793c7a1338f9ff1502) it starts
to get non-obvious I think.

At the [rgb2rv10](http://rgb2r.noname-ev.de/) we again had a
[codegolf](https://en.wikipedia.org/wiki/Code_golf) event. C is generally not a
preferred language for golf, but I use it, because I know it best and my
experiences with it are not that bad. My C solutions are most of the times
longer then the shortest solutions in perl or similar languages, but they are
competitive. That's why I decided to make a blogpost explaining this years C
solution as an example to show some basic C-golfing techniques.

This years [challenge](https://www.noname-ev.de/w/Codegolf/RGB2Rv10) was to
implement an interpreter for esocalc, a two-dimensional language for arithmetic
expressions. Follow the link for a more detailed specification. This challenge
is primed to be solved in C. This is also reflected by the length of the
different solutions: The shortest C solution is 246 bytes, the shortest Python
solution is 227 and the shortest Perl solution is 191 bytes. For a
codegolf-challenge this is an impressively small gap between C and scripting
languages.

You can follow this post by checking out [the code](https://github.com/Merovius/cgolf)
on github. The oldest commit is the one we are starting with and we will refine
it until we reach the 246 byte solution in `master`. You can test it by
compiling it (`gcc -o golf golf.c` should suffice in most cases, the shortest
needs a longer commandline, which is put in the Makefile, so you should `make`
it). You can run it through the testsuite used in the contest by running
`GOLF_BIN="./golf prove -l"`.

<a name="e3dc46c7c88f740c6b4eb671cd3b987061797529"></a>
The first step is to implement an easily readable, working version. This is
done in the
[first commit](https://github.com/Merovius/cgolf/blob/e3dc46c7c88f740c6b4eb671cd3b987061797529/golf.c).
Though you yourself might have come up with a different implementation, this is
pretty straightforward I think. We just read the whole esocalc-sourcecode and
walk through it, executing every instruction as we go. The stack is statically
allocated and of a fixed size, but that's no problem because we only have a
limited testsuite anyway.

<a name="38e6ffceb633615f48d0a9d25a391abf5228c35c"></a>
The [next step](https://github.com/Merovius/cgolf/blob/38e6ffceb633615f48d0a9d25a391abf5228c35c/golf.c)
is obvious: We remove comments and move to one-letter variable names, thus
reducing readability, but also size considerably. We will leave most of the
whitespace for now, because else it is hard to follow the changes.

<a name="004b45da976b3d1aab23e1b5ed3b9ff87b002895"></a>
An important lesson for C-golfers is the following: *for is never longer then
while and most of the times shorter*. An endless loop with `while` takes one
character more then a `for`-loop. We will later see more instances when `for` will
be considerably shorter. Also, we see the `if`/`else`-constructs in the
control-flow instructions. It is considerably shorter to use a ternary operator
in most cases, because in C, most statements are also expressions, so we can
write them as cases in `?:` - or use the short-circuiting `&&` if there is no
`else`-part. We will see more of that later. Lastly we collapse multiple
variable declarations into one to save `int`-keywords. These three changes are
what happend in [the next version](https://github.com/Merovius/cgolf/blob/004b45da976b3d1aab23e1b5ed3b9ff87b002895/golf.c).

<a name="eb5227716869399d62f12dcfc07c7e42094782b7"></a>
We continue in our path and notice, that we every `char`-literal takes three
bytes, while the number it represents often only takes two in decimal.
[Let's fix that](https://github.com/Merovius/cgolf/blob/eb5227716869399d62f12dcfc07c7e42094782b7/golf.c).

<a name="75625a730875ded009a216887db5455b5105e7e6"></a>
We also have two temporary variables `a` and `b`, that we shouldn't need.
[We can get rid of them](https://github.com/Merovius/cgolf/blob/75625a730875ded009a216887db5455b5105e7e6/golf.c),
by thinking up a single statement for arithmetic operations.

<a name="f0af3799d6c5ee3c30a1f43dd5c89523f2619759"></a>
[The next step](https://github.com/Merovius/cgolf/blob/f0af3799d6c5ee3c30a1f43dd5c89523f2619759/golf.c)
uses a real detail of C: If you don't give a type for a global variable, a
parameter or the return type of a function, `int` is assumed. If a function is
not defined, a prototype of `int foo()` is assumed, meaning we can pass
arbitrary arguments and get an `int`. The libc is linked in by default. All
these facts means, we can drop all `include`s and put our variables in the
global scope to remove all `int`-keywords. This is a very basic, but very
usefull technique. It has one important caveat, you should look out for: If you
need the return value of a libc-function and it is *not* `int`, you should
think about wether it can be safely converted. For example on amd64 an `int`
has 32 bits, but a pointer has 64 bits, therefore pointers as return values get
truncated (even if you assign them to a pointer).

<a name="17f305a0091651c03bb9e86e6ee9332f72138c04"></a>
[We can save more](https://github.com/Merovius/cgolf/blob/17f305a0091651c03bb9e86e6ee9332f72138c04/golf.c)
by using a parameter to `main`. This is also a very basic and often seen trick
in C-golfing. You get up to 3 local variables for free this way. In our case
there is an additional benefit: The first parameter to `main` is the number of
arguments, which is 1 for a normal call (the first argument is the name with
which the programm was called). This means, we get the initialization to 1 for
free.

<a name="f3957253031431ec25f8d4f68c10ca1b4dcfd4ed"></a>
[A trivial optimization](https://github.com/Merovius/cgolf/blob/f3957253031431ec25f8d4f68c10ca1b4dcfd4ed/golf.c)
is using `gets` instead of `read`. `gets` always adds a terminating zero-byte,
so we need to grow our buffer a little bit.

<a name="https://github.com/Merovius/cgolf/blob/fed1a817b88072dc5d27d8ae4dc772da8518ee5d"></a>
If we now look at our code, all the `case`-keywords might annoy us. If we see
a lot of repititions in our code, the obvious tool to use in C are `define`s. So
[lets define](https://github.com/Merovius/cgolf/blob/fed1a817b88072dc5d27d8ae4dc772da8518ee5d/golf.c)
the structure of the cases and replace every case by a short 1-letter identifier.

<a name="9de0b6f05fc52e5c08829bcf6d60a83c6756fba2"></a>
The same goes for the arithmetic operations: Four times the same long code cries
for a [define](https://github.com/Merovius/cgolf/blob/9de0b6f05fc52e5c08829bcf6d60a83c6756fba2/golf.c).
A `define` is not always a good solution. You have to weigh the additional
overhead of the keyword and the needed newline against the savings and number
of repititions.

<a name="ec654b1a11012a7820807cd29fe65a6427f300d4"></a>
[Next](https://github.com/Merovius/cgolf/blob/ec654b1a11012a7820807cd29fe65a6427f300d4/golf.c)
we eliminate the variable `i`. Skilled C-coders use pointer-arithmetic quite
often (no matter how bad the reputation is). In this case it would be a bad
idea, if we were not explicitely allowed to assume that all programs are
correct and stay in the bounds given (because bound-checks are a lot harder
without indexing).

<a name="6a10cb1480e1ca6cdc61bd628d8cb2f4d365a699"></a>
Another example of savings by `for`-loops is
[the next change](https://github.com/Merovius/cgolf/blob/6a10cb1480e1ca6cdc61bd628d8cb2f4d365a699/golf.c).
Here we moved two statements into the `for`-loop, thus using the semicolons we
need there anyway and saving two bytes.

<a name="7d506e18324daf3d6d98e25682321c19c7bef781"></a>
So the next big thing that catches our eyes are the `switch`, `case` and
`break`-keywords. Everytime you see long identifiers or keywords you should
think about wether a different program-structure or a different libc-builtin
may help you save it. `switch`-construct can almost always be replaced by an
`if`-`else if` construct (which is why we learned to use `switch` anyway). This
is often shorter, but as we learned, the ternary operator is even shorter. So in
[the next step](https://github.com/Merovius/cgolf/blob/7d506e18324daf3d6d98e25682321c19c7bef781/golf.c)
we use a giant ternary expression instead of a `switch`-structure. This brings
one major problem: `return` is one of the few things that's a statement, but
not an expression. So we can't use it in `?:`-expressions (because the branches
have to be expressions). We use `exit()` instead, which is an expression, but a
`void`-expression, so again we run into problems using it in `?:`. We work
around that for now by using `(exit(0),1)` instead. If you connect expressions
by `,` they are evaluated in succession (contrary to using boolean operators
for example) and the value of the last one is becoming the value of the whole
expression - so our `exit`-expression evaluates to 1 in this case.

<a name="4272ca2a181e8f50c1645b793c7a1338f9ff1502"></a>
`exit` is still pretty long (especially with the added parens and
comma-expression), so we want to avoid it too. Here comes a notable quote of
the organisator of the competition into action: “The return value isn't
important, as long as the output is correct. So it doesn't matter if you
segfault or anything”. This is the key to
[the next change](https://github.com/Merovius/cgolf/blob/4272ca2a181e8f50c1645b793c7a1338f9ff1502/golf.c):
Instead of exiting orderly we just create the conditions for a segfault by
assigning zero to `p`, which is dereferenced shortly thereafter, thus creating
a segfault when we want to exit. This is one of my favourite optimizations.

<a name="bb1b73fdfd4be6a75ebc47046af7b9af06ff80fe"></a>
There still is some repitition in our code. We still assign to `d` more often
then not. But our big nested ternary operator doesn't return anything yet. So our
[next step](https://github.com/Merovius/cgolf/blob/bb1b73fdfd4be6a75ebc47046af7b9af06ff80fe/golf.c)
is to return the new value for `d` in all subexpressions (if need be by using a
comma). This does not save a lot, but still a few bytes.

<a name="309465a985f67a8326ab10347b568ef467362b1c"></a>
Now the sources of bytes to save are getting scarcer. What still is a pain is
the explicit stack of a fixed size. Here another deep mistery of C (or more
specifically the way modern computers work)  comes into play:
[The call stack](https://en.wikipedia.org/wiki/Call_stack). We can actually
[use this as our stack](https://github.com/Merovius/cgolf/blob/309465a985f67a8326ab10347b568ef467362b1c/golf.c).
The way this works is, that we use a pointer to an address in the memory area,
the operating system reserved for our call stack and grow down (contrary to the
illustration on wikipedia, the stack grows downwards. But this is a minor
detail). By writing to this pointer and decrementing, we can push to the stack.
By incrementing it and reading we can pop something from the top of the stack.
To get a valid stack-address we could use the address of a local variable (for
example `s` itself). Local variables are at the bottom of the stackframe, so we
do not overwrite anything important if growing down. There is however a
problem: We call `gets` and `printf` which push a few stackframes to the
callstack. Our stack would get smashed by these calls. Therefore we just
subtract a sufficiently high number from it to reserve space for the
stackframes of the function calls. 760 is the minimum amount needed in my
setup, everything up to 99999 should save at least one byte.

<a name="00afa97fb52ba275f638092118b49b4027261928"></a>
This still is unsatisfactory, so we will hack a little more and use the fact,
that the testsuite only uses quite small programms and a quite small stack is
needed. So we just
[use `s` unitialized](https://github.com/Merovius/cgolf/blob/00afa97fb52ba275f638092118b49b4027261928/golf.c),
which is absolutely crazy. I discovered (by accident), that you will always end
up with a pointer to your program-array, using around 200 bytes of the end
(most probably some earlier deeply nested call in the startup of the binary
will write an appropriate address here by accident). This of course is
borderline cheating, but it saves 6 bytes, so who cares. From now on it's
absolutely forbidden to compile with optimizations, because this will destroy
this coincidence. Oh well.

<a name="e2aafeb23a88abb731d0341610bc84acd285424d"></a>
So, if we are already doing unreliable horrific voodoo which will curl up the
fingernails of every honest C developer, we can also
[save two bytes](https://github.com/Merovius/cgolf/blob/e2aafeb23a88abb731d0341610bc84acd285424d/golf.c)
by not setting `p` to zero, but instead just doubling it. You will then end up
with *some* address, that is hard to predict, but in all cases I tried leads to
crashing just as reliable. This means, we exit our program in just one byte. Neato!

<a name="7b1803ce9fe52c0f57fb804067493bc975dfb3be"></a>
There is not a lot we can save left now. What might still annoy us and is a
very good tip in general are all this numbers. Even if most characters have
only 2 bytes as a decimal, they still only have one byte as a character (not a
`char`-literal!). We can
[fix this](https://github.com/Merovius/cgolf/blob/7b1803ce9fe52c0f57fb804067493bc975dfb3be/golf.c)
by passing a verbatim character as the first argument to the `c`-makro. To
interpret it as a `char`, we stringify it (with `#a`) and dereference it (with
`*#a`), getting the first `char`. This opens a problem: A space is a
significant character in the interpreted source code, so we need to use it as
an argument. But a space is not significant at that point in the C source code,
so we simply can not pass it to our makro. The solution to this is to move the
whole ASCII-table. So instead of comparing `*p` we compare `*p+n` with `n` to
be choosen. Thus we don't need to pass a space, but some other char, that is
`n` positions away and everyone is happy. Kind of. We also need to avoid single
quotes, double quotes (though we can avoid this by using emtpy string (think
about why this works), but too many bytes!!!), parenthesis and chars outside of
ASCII (because this will break our C-file). These constrictions make `n=3`
pretty much the only choice. This means, we have to include a DEL-character in
our source-code, but the compiler is quite happy about that (the wiki isn't,
github isn't, the editor isn't, but who cares). This is my second most favourite hack.

<a name="60a5912baccb94e3e31cc57fe09712b1e7cb0280"></a><a name="70da40d21ca8ff3a58e5d2a3a890ff0f44d2ee0c"></a>
Now there is not much left to do. We
[remove the last char-literal left](https://github.com/Merovius/cgolf/blob/60a5912baccb94e3e31cc57fe09712b1e7cb0280/golf.c) and
[remove all non-essential whitespace](https://github.com/Merovius/cgolf/blob/70da40d21ca8ff3a58e5d2a3a890ff0f44d2ee0c/golf.c).

<a name="09ff6c236827639aad31edec198e97748241c3ea"></a>
This leaves us with 253 bytes. To get below 250, we
[use buildflags](https://github.com/Merovius/cgolf/blob/09ff6c236827639aad31edec198e97748241c3ea/Makefile)
instead of
[defines](https://github.com/Merovius/cgolf/blob/09ff6c236827639aad31edec198e97748241c3ea/golf.c).
Usually such flags are counted by the difference they add to a minimal compiler
call needed. In this case, we have a 186 byte C-file (after removing the
trailing newline added by vim) and 60 bytes of compiler-flags, totalling 246
bytes.

I think there still is potential to remove some more characters. Other tools
not used here include
[dispatch tables](https://en.wikipedia.org/wiki/Dispatch_table)
(which are kind of hard in C, because it lacks an eval, but some variations of
the concept still apply) and magic formulas. If the testcases are very limited,
some people resort to hardcoding the wanted results and just golf a minimal way
to differentiate between what output is wanted. This might be surprising, but
in many cases (this included) this will end up being shorter (though I consider
it cheating and try to avoid it). We also didn't do a lot of
[bit banging](https://en.wikipedia.org/wiki/Bit_banging). For example using `^`
instead of `==` reverses the check but saves a byte. But I think it is a
usefull intro for people who are just learning C and want to dive deeper into
the language by golfing.
]]></content:encoded>
      <dc:date>2013-10-11T02:09:38+00:00</dc:date>
    </item>
    <item>
      <title>New PGP Key</title>
      <link>https://blog.merovius.de//2013/10/03/new-pgp-key.html</link>
      <description><![CDATA[Because I recently applied for the position of a Debian maintainer, I
finally had to upgrade my PGP Key to a more secure 4096 bit RSA. The
new fingerprint is
]]></description>
      <pubDate>Thu, 03 Oct 2013 00:44:00 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/10/03/new-pgp-key.html</guid>
      <content:encoded><![CDATA[Because I recently applied for the position of a Debian maintainer, I
finally had to upgrade my PGP Key to a more secure 4096 bit RSA. The
new fingerprint is

    AF03 1CB8 DFFB 7DC5 E1EE  EB04 A7C9 FF06 3F3D 2E03

I signed the new key with my old key and [uploaded it to the
keyservers](http://pgp.mit.edu/pks/lookup?op=vindex&search=0xA7C9FF063F3D2E03).
My old key will be valid for a little longer, so you can still send me
encrypted Mails using my old key, but I would ask you to transition as
fast as possible to the new one. If the signature with my old key is
enough to earn your trust, you can delete the old key and set the new
one to trusted (and maybe even sign it again and mail me the signed
key or upload it), else you can check the fingerprint in person when
we meet next time.
]]></content:encoded>
      <dc:date>2013-10-03T00:44:00+00:00</dc:date>
    </item>
    <item>
      <title>Lazy blogging with jekyll</title>
      <link>https://blog.merovius.de//2013/09/28/lazy-blogging-with-jekyll.html</link>
      <description><![CDATA[tl;dr: I put up a small script to
automate creating blog-posts in jekyll
]]></description>
      <pubDate>Sat, 28 Sep 2013 15:11:10 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/09/28/lazy-blogging-with-jekyll.html</guid>
      <content:encoded><![CDATA[**tl;dr: I put up a [small script](https://gist.github.com/Merovius/6736709) to
automate creating blog-posts in jekyll**

If you think about setting up your own blog, jekyll seems to be an appropriate
choice. This short guide should put you through the process of having an easy
setup for writing and deploying your blog via your favourite editor (vim) and
your favourite version control system (git) to a publicly available server via
ssh.

First thing you will need, is to have jekyll installed on both machines (the
ones where you will write your posts and the one where you will deploy them to).
Because the debian-version appears to be horribly outdated, I installed it via
gem. As far as I understood, this has the advantage of making an installation
without root-privileges possible. You should also have git available on both
machines.

Initializing your blog is pretty easy, `jekyll new mynewblog` (on your local
machine) should suffice.  You still want to do some configuration and
customization, most of which should be straight-forward. Edit the `index.html`,
the `_config.yml` and the `_layouts/default.html`. You might also want to have
an Atom-template, so people can subscribe to your blog in their favourite
RSS-reader. My good friend Stefan [helped with
that](https://git.yrden.de/?p=blog.git/.git;a=blob;f=atom.xml;hb=HEAD) just put
that file into the root of your blog-directory, edit your blogtitle and
everything into it and add the line
{% highlight html %}
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">
{% endhighlight %}
in the `<head>` section of `_layouts/default.html`.

Next thing is setting up deployment. Just `git init` a blog, `git add` every
configuration file, page, template and whatnot and `git commit` it. ssh onto
your deployment-machine and do a `git init --bare blog.git`. Save the following
file to `blog.git/hooks/post-update` and change the path to point to a
directory, that is served by your http-server:
{% gist 6736709 post-update %}
Everytime you push into `blog.git` you will then have jekyll automatically
rebuild your blog. You now only have to do the following on your local machine
to deploy your blog:

```sh
git remote add origin username@example.com:blog.git
git push --set-upstream origin master
```

Now to the really fancy stuff. Jekyll expects your blogposts to live under the
`_posts`-directory under a special filename-format and to have a YAML-preamble,
containing some configuration. It can be quite cumbersome to manage this
yourself, so I wrote a [shellscript](https://gist.github.com/Merovius/6736709)
to ease the process. Put it anywhere in your path (i chose the name `newpost`)
and make it executable.

When you run the script, it will look into the current directory for a
jekyll-blog and create a draft from a small template given in the script. It
will then optionally run a jekyll-development server, so that you can preview
your blog-post in your browser (by saving the draft) and open the draft in your
favourite editor. After you close your editor, the jekyll server will be
stopped and the draft will be saved under `_posts/YYYY-MM-DD-abbrev-title.fmt`,
where `YYYY-MM-DD` is the current date (date and time will also be automatically
added to the YAML-preamble), `fmt` is a configurable format (markdown is default)
and `abbrev-title` is a short string derived from the title you put in.

There will also (optionally) be a git-commit created with a default
commit-message. You can edit the message in an editor and abort the commit, by
deleting everything and saving an empty commit-message. If you really want
(though I would not advise it) you can also automatically push it, after you're
done.

After this setup, to create a new blogpost, you just have to `cd` to your
blog-repository, run `newpost`, type your blogpost (and add a title), preview it
in your browsers, exit your editor and you have everything ready to push. It
can't get much easier.
]]></content:encoded>
      <dc:date>2013-09-28T15:11:10+00:00</dc:date>
    </item>
    <item>
      <title>First-year introductory course for programming</title>
      <link>https://blog.merovius.de//2013/09/28/firstyear-introductory-course-fo.html</link>
      <description><![CDATA[tl;dr: I gave a very introductory programming course and saw once again how
the basic ideas underlying the modernization of teaching just work when
implemented right.
]]></description>
      <pubDate>Sat, 28 Sep 2013 03:16:52 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/09/28/firstyear-introductory-course-fo.html</guid>
      <content:encoded><![CDATA[**tl;dr: I gave a very introductory programming course and saw once again how
the basic ideas underlying the modernization of teaching *just work* when
implemented right.**

This last week I organized a (very basic) introductory course on programming for
our first-year students. I was set on C++ because it is the language used in the
introductory lecture and we wanted to give people with absolutely no background
in programming or proper use of a computer the necessary tools to start in this
lecture on mostly equal grounds to people who already took a basic computer
science course in school. We had five days, with 3 hours a day to try to reach
that goal, which is a very limited amount of time for such a course and we had
50 participants.

The whole concept of the course was very modern (at least for our universities
standards) - instead of just giving lectures, telling people about syntax and
stuff we divided up the whole course into 19 lessons, each of which was worked
at mostly independent. That had two big advantages (and was very positively
perceived): First, the amount of time, we needed to spend lecturing and doing
frontal presentations was minimized to about half an hour over all the course.
The saved time could be invested in individual tutoring. This enabled us to
react to every student needing help in a few seconds, using only about 3-4
senior students (with mostly pretty minimal background themselves actually) to
teach.

Second the students where able to just work in their own speed without external
pressure or a limit on the time spent on any lesson. Missing deadlines for
lessons meant more experimentation, less competition amongst the students, less
stress and less pressure to finish with all lessons in time. The course was not
designed to be finished, so even though many students didn't reach the last
lesson, I think the additional experimentation (combined with a less
content-driven curriculum) added much more value for the students.

The content also was rather different from what you usually read in tutorials or
get in lectures at the university. Instead of systematically developing syntax
and different language constructs, we used the language less then the object to
learn, but the mean to learn basic skills needed, when tackling a programming
lecture (basically: „How do I start“ and „what can I do, if it doesn't work?“).
We introduced every lesson with about a page of text, describing the key
constructs underlying the object of that lesson, gave some basic code-examples
and (without explaining the details of the syntax) then presented some basic
exercises, which could be mastered without much understanding of what was
happening, but which ensured the reproduction needed, to properly learn the
syntactic device or the idea. We then added some playfull, very open exercises,
where through experimentation and through their own mistakes the students where
supposed to discover themselves the more intricate details of the subject
matter. Thematically we restricted the syntax to the absolute minimum to get
some basic, but fun and usefull programms to work (for example, we introduced
only one kind of loop, and we introduced only the datatypes int, bool,
std::string and double, as well as arrays thereof)

Though this all might sound fairly „new-agey“, it worked remarkably well. We saw
a fair amount of experimentation, we saw very creative solutions to seemingly
easy and straightforward, we got very positive feedback and though we introduced
many special subjects (for example debuggers, online references and detailed
lectures and exercises on how to read manpages or error output of the compiler),
I think it is fair to say, that we reached at least the level of proficiency and
confidence as the more traditional courses we held the last years had.

So, the bottomline is: We took a very huge bite out of the ideas and thoughts
underlying the ongoing effort in europe to modernize teaching at universities
(The „Bologna Process“, as it's known at least here in germany) and though I
totally agree, that the implementation of these guidelines at the universities
is currently pretty misguided and plain *bad*, I once again feel confirmed in my
view, that if you put some effort into it and really use what the underlying
ideas of bologna are (instead of just picking up, what you hear from the media
about it), you can create a really kick-ass curriculum, that is both more fun
*and* more informative at the same time.

All used content is on
[github](https://github.com/FachschaftMathPhys/Infovorkurs), if you are
interested in what exactly we used in the course.
]]></content:encoded>
      <dc:date>2013-09-28T03:16:52+00:00</dc:date>
    </item>
    <item>
      <title>Relaunch</title>
      <link>https://blog.merovius.de//2013/09/28/relaunch.html</link>
      <description><![CDATA[Years ago I took down my blog because I was so unsatisfied with
wordpress. In these years I started about 5 times to write my own
git-based blog-engine and about 5 times I stopped after making
considerable progress (mostly because it was too hard to integrate
comments). This time the urge to restart my blog is finally
overpowering my chronic nii-syndrome and I decided to use
jekyll as a blog-engine and add some git-hook-magic myself.
]]></description>
      <pubDate>Sat, 28 Sep 2013 02:08:05 +0000</pubDate>
      <guid>https://blog.merovius.de//2013/09/28/relaunch.html</guid>
      <content:encoded><![CDATA[Years ago I took down my blog because I was so unsatisfied with
wordpress. In these years I started about 5 times to write my own
git-based blog-engine and about 5 times I stopped after making
considerable progress (mostly because it was too hard to integrate
comments). This time the urge to restart my blog is finally
overpowering my chronic nii-syndrome and I decided to use
[jekyll](http://jekyllrb.com/) as a blog-engine and add some git-hook-magic myself.

The layout is still pretty shitty, it's the default of jekyll, but I
wanted to put it online before seing my efforts die again on such
minor details.

So this is finally the relaunch of my blog. Yay for me.
]]></content:encoded>
      <dc:date>2013-09-28T02:08:05+00:00</dc:date>
    </item>
    <dc:date>2018-09-05T04:00:00+00:00</dc:date>
  </channel>
</rss>